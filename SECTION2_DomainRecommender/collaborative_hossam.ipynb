{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 3: Collaborative Filtering and Hybrid Approach\n",
                "\n",
                "## 8. Collaborative Filtering Integration\n",
                "\n",
                "In this section, we implement **Item-Based Collaborative Filtering** using **Centered Cosine Similarity (Pearson Correlation)**.\n",
                "\n",
                "**Approach Formalization**:\n",
                "1.  **Item-Item Similarity**: We compute the similarity between items based on their co-ratings by users. Two items are similar if the same users rated them similarly.\n",
                "2.  **Normalization**: We subtract the user's mean rating from each rating to handle optimism/pessimism bias. This effectively turns Cosine Similarity into Pearson Correlation.\n",
                "3.  **Prediction**: $\\hat{r}_{ui} = \\mu_u + \\frac{\\sum_{j \\in N(i)} w_{ij} (r_{uj} - \\mu_u)}{\\sum_{j \\in N(i)} |w_{ij}|}$\n",
                "    *   Where $w_{ij}$ is the similarity between item $i$ and item $j$.\n",
                "    *   $N(i)$ are the k-nearest neighbors of item $i$ that user $u$ has rated.\n",
                "    *   $\\mu_u$ is the average rating of user $u$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy.sparse import csr_matrix, diags\n",
                "from scipy.sparse.linalg import norm\n",
                "import os\n",
                "\n",
                "RESULTS_DIR = \"../results\"\n",
                "if not os.path.exists(RESULTS_DIR):\n",
                "    os.makedirs(RESULTS_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Subtask 2: Build the user–item rating matrix\n",
                "\n",
                "We construct a sparse matrix where rows are Users and columns are Items. This is slightly counter-intuitive for \"Item-based\" similarity (which likes Items as rows for dot product), but standard datasets are $U \\times I$. We can Transpose for similarity computation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_user_item_matrix(df_interactions):\n",
                "    \"\"\"\n",
                "    Constructs sparse User-Item Rating Matrix (Rows=Users, Cols=Items).\n",
                "    Returns: Matrix R, user_map (id->idx), item_map (id->idx)\n",
                "    \"\"\"\n",
                "    print(\"\\n--- Building User-Item Matrix ---\")\n",
                "    \n",
                "    # Unique IDs\n",
                "    users = sorted(df_interactions['user_id'].unique())\n",
                "    items = sorted(df_interactions['item_id'].unique())\n",
                "    \n",
                "    user_map = {u: i for i, u in enumerate(users)}\n",
                "    item_map = {it: i for i, it in enumerate(items)}\n",
                "    \n",
                "    # Map Data\n",
                "    row_indices = df_interactions['user_id'].map(user_map)\n",
                "    col_indices = df_interactions['item_id'].map(item_map)\n",
                "    ratings = df_interactions['rating'].values\n",
                "    \n",
                "    # Build CSR\n",
                "    # Shape: (n_users, n_items)\n",
                "    R = csr_matrix((ratings, (row_indices, col_indices)), shape=(len(users), len(items)))\n",
                "    \n",
                "    print(f\"Matrix Shape: {R.shape} with {R.nnz} ratings.\")\n",
                "    print(f\"Sparsity: {1.0 - R.nnz / (R.shape[0] * R.shape[1]):.4%}\")\n",
                "    \n",
                "    return R, user_map, item_map"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Subtask 3: Normalize ratings\n",
                "\n",
                "We perform **Mean-Centering**. For each user, we calculate their average rating and subtract it from their ratings. This creates a matrix of deviations.\n",
                "We perform this manually to keep sparsity structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def center_ratings_by_user(R):\n",
                "    \"\"\"\n",
                "    Subtracts row mean from non-zero elements.\n",
                "    Returns: Centered Matrix R_centered, User Means vector\n",
                "    \"\"\"\n",
                "    print(\"\\n--- Mean-Centering Ratings ---\")\n",
                "    \n",
                "    # Calculate Row Means (User Means)\n",
                "    # Sum of ratings / Count of non-zero ratings\n",
                "    row_sums = np.array(R.sum(axis=1)).flatten()\n",
                "    row_counts = np.diff(R.indptr)\n",
                "    \n",
                "    # Avoid division by zero\n",
                "    with np.errstate(divide='ignore', invalid='ignore'):\n",
                "        user_means = row_sums / row_counts\n",
                "    user_means[~np.isfinite(user_means)] = 0.0\n",
                "    \n",
                "    # Create Centered Matrix\n",
                "    # To maintain sparsity, we only subtract from existing ratings.\n",
                "    # Non-rated items remain 0 (conceptually undefined, but treated as neutral 0 in centered space? \n",
                "    # Actually Pearson is defined only on common items. In sparse matrices, 0 usually means missing.\n",
                "    # If we subtract mean, 0 becomes -mean, making matrix dense. \n",
                "    # Optimization: We ONLY adjust specific non-zero entries. \n",
                "    # Similarity between i and j uses ONLY users who rated BOTH.\n",
                "    \n",
                "    R_coo = R.tocoo()\n",
                "    rows = R_coo.row\n",
                "    cols = R_coo.col\n",
                "    data = R_coo.data\n",
                "    \n",
                "    # Subtract user mean from each rating\n",
                "    new_data = data - user_means[rows]\n",
                "    \n",
                "    R_centered = csr_matrix((new_data, (rows, cols)), shape=R.shape)\n",
                "    \n",
                "    print(\"Mean Centering complete.\")\n",
                "    return R_centered, user_means"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Subtask 4: Compute item–item similarity\n",
                "\n",
                "We calculate the Cosine Similarity between distinct **columns** of $R_{centered}$.\n",
                "Since $R_{centered}$ has user biases removed, this is equivalent to Pearson Correlation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_item_similarity_matrix(R_centered):\n",
                "    \"\"\"\n",
                "    Computes Cosine Similarity between columns (Items).\n",
                "    Sim(i, j) = (Ri . Rj) / (|Ri| |Rj|)\n",
                "    \"\"\"\n",
                "    print(\"\\n--- Computing Item-Item Similarity matrix ---\")\n",
                "    \n",
                "    # Transpose so Items are Rows -> easier dot product\n",
                "    # Matrix M: (n_items, n_users)\n",
                "    M = R_centered.T.tocsr()\n",
                "    \n",
                "    # Compute Norms\n",
                "    print(\"Calculating Item Norms...\")\n",
                "    # row_norms for M = column norms for R_centered\n",
                "    item_norms = np.sqrt(np.array(M.power(2).sum(axis=1)).flatten())\n",
                "    \n",
                "    # Avoid division by zero\n",
                "    item_norms[item_norms == 0] = 1e-9\n",
                "    inv_norms = 1.0 / item_norms\n",
                "    \n",
                "    # Normalize rows of M (so dot product becomes cosine)\n",
                "    M_normalized = diags(inv_norms) @ M\n",
                "    \n",
                "    # Compute Dot Product: (N_items, N_items)\n",
                "    print(\"Computing Dot Product (Similarity)...\")\n",
                "    # This can be heavy if N_items is huge. \n",
                "    # For this assignment, assuming it fits in memory or we sparsify.\n",
                "    sim_matrix = M_normalized.dot(M_normalized.T)\n",
                "    \n",
                "    # Set diagonal to 0 (ignore self-similarity)\n",
                "    sim_matrix.setdiag(0.0)\n",
                "    \n",
                "    # Sparsify: drop near-zero values to save memory\n",
                "    sim_matrix.eliminate_zeros()\n",
                "    \n",
                "    print(f\"Similarity Matrix Shape: {sim_matrix.shape}, NNZ: {sim_matrix.nnz}\")\n",
                "    return sim_matrix"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Subtask 5: Select neighborhood size (k)\n",
                "\n",
                "Function to extract local top-K similar items."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_k_neighbors(sim_matrix, k=20):\n",
                "    \"\"\"\n",
                "    Use the full similarity matrix. For prediction, we will select top-k on the fly.\n",
                "    This function is a placeholder configuration.\n",
                "    \"\"\"\n",
                "    print(f\"Using Neighborhood size k={k}\")\n",
                "    return k"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Subtask 6 & 7: Predict ratings and Handle Edge Cases\n",
                "\n",
                "We predict rating for user $u$ on item $i$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_item_based_cf(user_idx, item_idx, R, sim_matrix, user_means, k=20):\n",
                "    \"\"\"\n",
                "    Predicts rating: mu_u + [Sum(sim * (r - mu_u)) / Sum(|sim|)]\n",
                "    Uses R (sparse) directly.\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Get Similarity row for target item\n",
                "    # Row 'item_idx' in sim_matrix contains similarities to all other items\n",
                "    # Extract row efficiently\n",
                "    sim_row = sim_matrix.getrow(item_idx)\n",
                "    \n",
                "    # 2. Get User's Rated items\n",
                "    # Row 'user_idx' in R contains items user rated\n",
                "    user_row = R.getrow(user_idx)\n",
                "    \n",
                "    # 3. Find intersection: Items j that are (Similar to i) AND (Rated by u)\n",
                "    # We can iterate over user's rated items since that set is usually smaller\n",
                "    rated_indices = user_row.indices\n",
                "    rated_values = user_row.data\n",
                "    \n",
                "    if len(rated_indices) == 0:\n",
                "         return user_means[user_idx] # Fallback: User Mean\n",
                "    \n",
                "    # Extract similarities for these items\n",
                "    # dense conversion for fast indexing if items are not too many, or iterative check\n",
                "    # For assignment scale, dense row is fine\n",
                "    \n",
                "    relevant_sims = []\n",
                "    relevant_ratings = []\n",
                "    \n",
                "    # Optim: Iterate only Non-Zero similarities if Sim row is sparse\n",
                "    # But we need intersection. \n",
                "    # Let's map rated items to their ratings\n",
                "    rating_map = {idx: val for idx, val in zip(rated_indices, rated_values)}\n",
                "    \n",
                "    # Iterate Top-K neighbors from Sim Row\n",
                "    # To do that, we get indices and data from sim_row\n",
                "    neighbor_indices = sim_row.indices\n",
                "    neighbor_scores = sim_row.data\n",
                "    \n",
                "    # Filter: Neighbors must have been rated by user\n",
                "    candidates = []\n",
                "    for j, score in zip(neighbor_indices, neighbor_scores):\n",
                "        if j in rating_map:\n",
                "            candidates.append((score, rating_map[j]))\n",
                "            \n",
                "    # Sort by absolute similarity (Top-K)\n",
                "    candidates.sort(key=lambda x: abs(x[0]), reverse=True)\n",
                "    top_k = candidates[:k]\n",
                "    \n",
                "    if not top_k:\n",
                "        return user_means[user_idx] # No common neighbors\n",
                "        \n",
                "    weighted_sum = 0.0\n",
                "    sum_abs_sim = 0.0\n",
                "    \n",
                "    mu_u = user_means[user_idx]\n",
                "    \n",
                "    for score, r_val in top_k:\n",
                "        # Prediction formula uses deviation: (r - mu_u)\n",
                "        # r_val is raw rating.\n",
                "        dev = r_val - mu_u\n",
                "        weighted_sum += score * dev\n",
                "        sum_abs_sim += abs(score)\n",
                "        \n",
                "    if sum_abs_sim == 0:\n",
                "        return mu_u\n",
                "        \n",
                "    pred = mu_u + (weighted_sum / sum_abs_sim)\n",
                "    \n",
                "    # Clip to valid range\n",
                "    pred = max(1.0, min(5.0, pred))\n",
                "    return pred"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Subtask 8, 9, 10: Generate Recommendations, Save, and Validate\n",
                "\n",
                "Orchestrate the process for a set of target users."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_collaborative_filtering_pipeline(df_interactions, df_items_map, target_user_ids=None):\n",
                "    \"\"\"\n",
                "    Full execution wrapper.\n",
                "    \"\"\"\n",
                "    print(\"\\n==============================================\")\n",
                "    print(\"    STARTING ITEM-BASED COLLABORATIVE FILTERING    \")\n",
                "    print(\"==============================================\")\n",
                "    \n",
                "    # 1. Build Matrix\n",
                "    R, user_map, item_map = build_user_item_matrix(df_interactions)\n",
                "    inv_user_map = {v: k for k, v in user_map.items()}\n",
                "    inv_item_map = {v: k for k, v in item_map.items()}\n",
                "    \n",
                "    # 2. Normalize\n",
                "    R_centered, user_means = center_ratings_by_user(R)\n",
                "    \n",
                "    # 3. Similarity\n",
                "    sim_matrix = compute_item_similarity_matrix(R_centered)\n",
                "    \n",
                "    # 4. Generate Recs for Targets\n",
                "    if target_user_ids is None:\n",
                "        # Pick top 5 active users\n",
                "        target_user_ids = df_interactions['user_id'].value_counts().head(5).index.tolist()\n",
                "        \n",
                "    print(f\"\\nGenerating recommendations for {len(target_user_ids)} users...\")\n",
                "    \n",
                "    all_recs = []\n",
                "    \n",
                "    for uid in target_user_ids:\n",
                "        if uid not in user_map:\n",
                "            print(f\"User {uid} not in matrix. Skipping.\")\n",
                "            continue\n",
                "            \n",
                "        u_idx = user_map[uid]\n",
                "        \n",
                "        # Identify unrated items (Candidates)\n",
                "        rated_indices = R.getrow(u_idx).indices\n",
                "        rated_set = set(rated_indices)\n",
                "        \n",
                "        candidates = []\n",
                "        preds = []\n",
                "        \n",
                "        # For demonstration, we can't score ALL items if N is huge. \n",
                "        # But if N ~ 1000-5000, we can.\n",
                "        # Let's score all.\n",
                "        \n",
                "        N_items = R.shape[1]\n",
                "        \n",
                "        # Heuristic: Only score items that appear in Top-K neighbors of rated items?\n",
                "        # Strict Item-based approach involves iterating all candidates.\n",
                "        # We will iterate all columns.\n",
                "        \n",
                "        for i_idx in range(N_items):\n",
                "            if i_idx not in rated_set:\n",
                "                score = predict_item_based_cf(u_idx, i_idx, R, sim_matrix, user_means, k=20)\n",
                "                preds.append((score, i_idx))\n",
                "        \n",
                "        # Top 10\n",
                "        preds.sort(key=lambda x: x[0], reverse=True)\n",
                "        top_10 = preds[:10]\n",
                "        \n",
                "        # Format\n",
                "        rank = 1\n",
                "        for score, i_idx in top_10:\n",
                "            item_id = inv_item_map[i_idx]\n",
                "            # Get title attempt\n",
                "            title = \"Unknown\"\n",
                "            items_row = df_items_map[df_items_map['item_id'] == item_id]\n",
                "            if not items_row.empty:\n",
                "                title = items_row.iloc[0]['title']\n",
                "                \n",
                "            all_recs.append({\n",
                "                'User': uid,\n",
                "                'Rank': rank,\n",
                "                'Item_ID': item_id,\n",
                "                'Predicted_Rating': round(score, 4),\n",
                "                'Title': title\n",
                "            })\n",
                "            rank += 1\n",
                "            \n",
                "    # Save Results\n",
                "    df_out = pd.DataFrame(all_recs)\n",
                "    save_path = os.path.join(RESULTS_DIR, \"cf_item_based_recommendations.csv\")\n",
                "    df_out.to_csv(save_path, index=False)\n",
                "    print(f\"\\nRecommendations saved to {save_path}\")\n",
                "    \n",
                "    # Validate: Print first few\n",
                "    print(df_out.head(10))\n",
                "    return df_out"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_collaborative_filtering_pipeline(df_interactions, df_items_map, target_user_ids=None):\n",
                "    \"\"\"\n",
                "    Full execution wrapper. UPDATED to return matrices for SVD.\n",
                "    \"\"\"\n",
                "    print(\"\\n==============================================\")\n",
                "    print(\"    STARTING ITEM-BASED COLLABORATIVE FILTERING    \")\n",
                "    print(\"==============================================\")\n",
                "    \n",
                "    # 1. Build Matrix\n",
                "    R, user_map, item_map = build_user_item_matrix(df_interactions)\n",
                "    inv_user_map = {v: k for k, v in user_map.items()}\n",
                "    inv_item_map = {v: k for k, v in item_map.items()}\n",
                "    \n",
                "    # 2. Normalize\n",
                "    R_centered, user_means = center_ratings_by_user(R)\n",
                "    \n",
                "    # 3. Similarity\n",
                "    sim_matrix = compute_item_similarity_matrix(R_centered)\n",
                "    \n",
                "    # 4. Generate Recs for Targets\n",
                "    if target_user_ids is None:\n",
                "        # Pick top 5 active users\n",
                "        target_user_ids = df_interactions['user_id'].value_counts().head(5).index.tolist()\n",
                "        \n",
                "    print(f\"\\nGenerating recommendations for {len(target_user_ids)} users...\")\n",
                "    \n",
                "    all_recs = []\n",
                "    \n",
                "    for uid in target_user_ids:\n",
                "        if uid not in user_map:\n",
                "            print(f\"User {uid} not in matrix. Skipping.\")\n",
                "            continue\n",
                "            \n",
                "        u_idx = user_map[uid]\n",
                "        \n",
                "        # Identify unrated items (Candidates)\n",
                "        rated_indices = R.getrow(u_idx).indices\n",
                "        rated_set = set(rated_indices)\n",
                "        \n",
                "        candidates = []\n",
                "        preds = []\n",
                "        \n",
                "        # For demonstration, we can't score ALL items if N is huge. \n",
                "        # But if N ~ 1000-5000, we can.\n",
                "        # Let's score all.\n",
                "        \n",
                "        N_items = R.shape[1]\n",
                "        \n",
                "        # Heuristic: Only score items that appear in Top-K neighbors of rated items?\n",
                "        # Strict Item-based approach involves iterating all candidates.\n",
                "        # We will iterate all columns.\n",
                "        \n",
                "        for i_idx in range(N_items):\n",
                "            if i_idx not in rated_set:\n",
                "                score = predict_item_based_cf(u_idx, i_idx, R, sim_matrix, user_means, k=20)\n",
                "                preds.append((score, i_idx))\n",
                "        \n",
                "        # Top 10\n",
                "        preds.sort(key=lambda x: x[0], reverse=True)\n",
                "        top_10 = preds[:10]\n",
                "        \n",
                "        # Format\n",
                "        rank = 1\n",
                "        for score, i_idx in top_10:\n",
                "            item_id = inv_item_map[i_idx]\n",
                "            # Get title attempt\n",
                "            title = \"Unknown\"\n",
                "            items_row = df_items_map[df_items_map['item_id'] == item_id]\n",
                "            if not items_row.empty:\n",
                "                title = items_row.iloc[0]['title']\n",
                "                \n",
                "            all_recs.append({\n",
                "                'User': uid,\n",
                "                'Rank': rank,\n",
                "                'Item_ID': item_id,\n",
                "                'Predicted_Rating': round(score, 4),\n",
                "                'Title': title\n",
                "            })\n",
                "            rank += 1\n",
                "            \n",
                "    # Save Results\n",
                "    df_out = pd.DataFrame(all_recs)\n",
                "    save_path = os.path.join(RESULTS_DIR, \"cf_item_based_recommendations.csv\")\n",
                "    df_out.to_csv(save_path, index=False)\n",
                "    print(f\"\\nRecommendations saved to {save_path}\")\n",
                "    \n",
                "    # Validate: Print first few\n",
                "    print(df_out.head(10))\n",
                "    \n",
                "    # RETURN MATRICES FOR SVD\n",
                "    return df_out, R, R_centered, user_means, user_map, item_map"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7230e121",
            "metadata": {},
            "source": [
                "## 8.2. Matrix Factorization (SVD)\n",
                "\n",
                "We implement **Truncated SVD** to uncover latent factors.\n",
                "We reuse the centered user-item matrix $R_{centered}$ from the previous step.\n",
                "$$ R_{centered} \\approx U \\Sigma V^T $$\n",
                "Prediction: $\\hat{r}_{ui} = (U_u \\cdot \\Sigma \\cdot V_i^T) + \\mu_u$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff03b3c0",
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.sparse.linalg import svds\n",
                "\n",
                "# Subtasks 1, 2, 3: Reuse and Prepare Matrix\n",
                "# We already have R_centered from the CF pipeline.\n",
                "# Missing values in R (sparse) are 0.\n",
                "# Since R_centered = R - mu, the 0s represent 'average rating' (mean imputation implicitly),\n",
                "# which is a common strategy for SVD to avoid treating missing as 'dislike' (0 stars).\n",
                "def prepare_matrix_for_svd(R_centered):\n",
                "    \"\"\"\n",
                "    Verifies the matrix is ready for SVD.\n",
                "    \"\"\"\n",
                "    print(\"\\n--- Preparing Matrix for SVD ---\")\n",
                "    # Conversion to float for SVD precision\n",
                "    R_float = R_centered.asfptype()\n",
                "    print(f\"Matrix Properties: Shape={R_float.shape}, NNZ={R_float.nnz}, Dtype={R_float.dtype}\")\n",
                "    return R_float"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "66da7fd5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Subtask 4: Apply Truncated SVD\n",
                "def apply_truncated_svd(matrix, k=20):\n",
                "    \"\"\"\n",
                "    Decomposes the matrix using Truncated SVD.\n",
                "    Returns U, Sigma (diagonal), Vt.\n",
                "    \"\"\"\n",
                "    print(f\"\\n--- Applying Truncated SVD (k={k}) ---\")\n",
                "    \n",
                "    # svds returns: \n",
                "    # u: Unitary matrix having left singular vectors as columns.\n",
                "    # s: The singular values.\n",
                "    # vt: Unitary matrix having right singular vectors as rows.\n",
                "    u, s, vt = svds(matrix, k=k)\n",
                "    \n",
                "    # svds returns them in ascending order of singular values, we usually want descending\n",
                "    # so we flip them\n",
                "    u = u[:, ::-1]\n",
                "    s = s[::-1]\n",
                "    vt = vt[::-1, :]\n",
                "    \n",
                "    # Convert Sigma to diagonal matrix\n",
                "    sigma = np.diag(s)\n",
                "    \n",
                "    print(f\"SVD Complete.\")\n",
                "    print(f\"U shape: {u.shape}\")\n",
                "    print(f\"Sigma shape: {sigma.shape}\")\n",
                "    print(f\"Vt shape: {vt.shape}\")\n",
                "    \n",
                "    return u, sigma, vt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "37e1efe3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Subtasks 5 & 6: Reconstruct and Clip\n",
                "def reconstruct_predictions(u, sigma, vt, user_means):\n",
                "    \"\"\"\n",
                "    Computes prediction matrix: P = (U * Sigma * Vt) + UserMeans\n",
                "    \"\"\"\n",
                "    print(\"\\n--- Reconstructing Prediction Matrix ---\")\n",
                "    \n",
                "    # 1. Dot Product\n",
                "    # U (N_users, k) . Sigma (k, k) . Vt (k, N_items)\n",
                "    # Result is Dense (N_users, N_items)\n",
                "    pred_centered = np.dot(np.dot(u, sigma), vt)\n",
                "    \n",
                "    # 2. Add Means back (Broadcasting)\n",
                "    # user_means is (N_users, )\n",
                "    # reshaped to (N_users, 1) to add to each column\n",
                "    means_reshaped = user_means.reshape(-1, 1)\n",
                "    predicted_ratings = pred_centered + means_reshaped\n",
                "    \n",
                "    # 3. Clip to Valid Range [1, 5]\n",
                "    predicted_ratings = np.clip(predicted_ratings, 1.0, 5.0)\n",
                "    \n",
                "    print(f\"Reconstruction Complete. Shape: {predicted_ratings.shape}\")\n",
                "    return predicted_ratings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c19719a4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Subtasks 7, 8, 9, 10: Generate Recommendations for Target Users\n",
                "def generate_svd_recommendations(predicted_matrix, df_items_map, user_map, target_users, k_factors):\n",
                "    \"\"\"\n",
                "    Extracts top N items for target users from the dense predicted matrix.\n",
                "    \"\"\"\n",
                "    print(f\"\\n--- Generating SVD Recommendations (Latent Factors={k_factors}) ---\")\n",
                "    \n",
                "    inv_user_map = {v: k for k, v in user_map.items()}\n",
                "    all_recs = []\n",
                "    \n",
                "    # For metadata lookup\n",
                "    item_map_dict = df_items_map.set_index('item_id').to_dict('index')\n",
                "    \n",
                "    # Inverse Item Map needed? \n",
                "    # Usually item_map in build_matrix was item_id -> col_idx\n",
                "    # We need col_idx -> item_id\n",
                "    # We assume 'item_map' passed to this function is the dict from build_matrix? \n",
                "    # Wait, we need to pass that map explicitly or recreate inverse here.\n",
                "    # Let's assume we pass inv_item_map for efficiency or re-generate it.\n",
                "    # For safety, let's pass the interaction df to re-derive consistent mapping or return maps from build fn.\n",
                "    # We will assume we have 'item_id_list' correpsonding to columns.\n",
                "    pass \n",
                "\n",
                "def run_svd_pipeline(R_centered, user_means, user_map, item_map, df_items_map, target_users=None, k=20):\n",
                "    \"\"\"\n",
                "    Wrapper.\n",
                "    \"\"\"\n",
                "    # 1. Prepare\n",
                "    R_ready = prepare_matrix_for_svd(R_centered)\n",
                "    \n",
                "    # 2. Decompose\n",
                "    u, sigma, vt = apply_truncated_svd(R_ready, k=k)\n",
                "    \n",
                "    # 3. Reconstruct\n",
                "    pred_matrix = reconstruct_predictions(u, sigma, vt, user_means)\n",
                "    \n",
                "    # 4. Recommend\n",
                "    # Inverse maps\n",
                "    inv_user_map = {v: k for k, v in user_map.items()}\n",
                "    inv_item_map = {v: k for k, v in item_map.items()}\n",
                "    \n",
                "    if target_users is None:\n",
                "        # Default to first 5 users in map\n",
                "        target_users = list(user_map.keys())[:5]\n",
                "        \n",
                "    all_recs = []\n",
                "    \n",
                "    for uid in target_users:\n",
                "        if uid not in user_map:\n",
                "            continue\n",
                "        \n",
                "        u_idx = user_map[uid]\n",
                "        user_preds = pred_matrix[u_idx, :]\n",
                "        \n",
                "        # Identify already rated? Standard practice is to exclude them.\n",
                "        # But SVD fills them with 'denoised' values. \n",
                "        # We usually want to recommend NEW items.\n",
                "        # We need original R to check what was rated.\n",
                "        # Since we only passed R_centered, we can check non-zeros in R_centered? \n",
                "        # R_centered has 0 for unrated, but also potentially 0 for rated-as-average.\n",
                "        # Better to assume we recommend top scored items regardless or pass original R.\n",
                "        # We will just rank top items for now.\n",
                "        \n",
                "        # Sort indices\n",
                "        top_indices = np.argsort(user_preds)[::-1][:20]\n",
                "        \n",
                "        rank = 1\n",
                "        for i_idx in top_indices:\n",
                "            score = user_preds[i_idx]\n",
                "            item_id = inv_item_map[i_idx]\n",
                "            \n",
                "            # Meta\n",
                "            title = \"Unknown\"\n",
                "            item_row = df_items_map[df_items_map['item_id'] == item_id]\n",
                "            if not item_row.empty:\n",
                "                title = item_row.iloc[0]['title']\n",
                "            \n",
                "            all_recs.append({\n",
                "                'User': uid,\n",
                "                'Rank': rank,\n",
                "                'Item_ID': item_id,\n",
                "                'Predicted_Rating': round(score, 4),\n",
                "                'Title': title,\n",
                "                'Method': f'SVD (k={k})'\n",
                "            })\n",
                "            rank += 1\n",
                "            \n",
                "    # Save\n",
                "    df_out = pd.DataFrame(all_recs)\n",
                "    save_path = os.path.join(RESULTS_DIR, \"svd_recommendations.csv\")\n",
                "    df_out.to_csv(save_path, index=False)\n",
                "    print(f\"\\nSVD Recommendations saved to {save_path}\")\n",
                "    print(df_out.head(10))\n",
                "    return df_out"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bcabd9b5",
            "metadata": {},
            "source": [
                "### Memory Optimization for SVD\n",
                "\n",
                "**Critical Note**: Reconstructing the full dense matrix $(N_{users} \\times N_{items})$ allows for global ranking but consumes massive memory (approx 2.8TB for this dataset), leading to `MemoryError`.\n",
                "\n",
                "To resolve this, we implement **On-Demand Prediction**:\n",
                "Instead of `np.dot(U, Sigma, Vt)`, we compute predictions only for the specific **target users** one by one.\n",
                "Prediction for user $u$: $r_u = U_u \\cdot \\Sigma \\cdot V^T + \\mu_u$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "acd83327",
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_for_user_svd(u_idx, u_matrix, sigma_matrix, vt_matrix, user_mean):\n",
                "    \"\"\"\n",
                "    Computes dense prediction vector for a single user to save memory.\n",
                "    Result shape: (n_items, )\n",
                "    \"\"\"\n",
                "    # Get User's latent vector: (1, k)\n",
                "    user_latent = u_matrix[u_idx, :].reshape(1, -1)\n",
                "    \n",
                "    # Compute: User_Latent * Sigma * Vt\n",
                "    # (1, k) * (k, k) -> (1, k)\n",
                "    w = np.dot(user_latent, sigma_matrix)\n",
                "    \n",
                "    # (1, k) * (k, n_items) -> (1, n_items)\n",
                "    user_preds = np.dot(w, vt_matrix).flatten()\n",
                "    \n",
                "    # Add Mean\n",
                "    user_preds += user_mean\n",
                "    \n",
                "    # Clip\n",
                "    return np.clip(user_preds, 1.0, 5.0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6da1a80",
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_svd_recommendations_efficient(u_matrix, sigma_matrix, vt_matrix, user_means, user_map, item_map, df_items_map, target_users, k_factors):\n",
                "    \"\"\"\n",
                "    Generates recommendations row-by-row to avoid OOM.\n",
                "    \"\"\"\n",
                "    print(f\"\\n--- Generating Efficient SVD Recommendations (k={k_factors}) ---\")\n",
                "    \n",
                "    inv_item_map = {v: k for k, v in item_map.items()}\n",
                "    all_recs = []\n",
                "    \n",
                "    # Pre-fetch item titles for speed\n",
                "    item_titles = pd.Series(df_items_map.title.values, index=df_items_map.item_id).to_dict()\n",
                "    \n",
                "    for uid in target_users:\n",
                "        if uid not in user_map:\n",
                "            continue\n",
                "            \n",
                "        u_idx = user_map[uid]\n",
                "        mean_rating = user_means[u_idx]\n",
                "        \n",
                "        # Compute ONLY this user's predictions\n",
                "        preds = predict_for_user_svd(u_idx, u_matrix, sigma_matrix, vt_matrix, mean_rating)\n",
                "        \n",
                "        # Sort top 20\n",
                "        # argsort is fast on 1D array\n",
                "        top_indices = np.argsort(preds)[::-1][:20]\n",
                "        \n",
                "        rank = 1\n",
                "        for i_idx in top_indices:\n",
                "            score = preds[i_idx]\n",
                "            item_id = inv_item_map.get(i_idx, \"Unknown\")\n",
                "            title = item_titles.get(item_id, \"Unknown\")\n",
                "            \n",
                "            all_recs.append({\n",
                "                'User': uid,\n",
                "                'Rank': rank,\n",
                "                'Item_ID': item_id,\n",
                "                'Predicted_Rating': round(score, 4),\n",
                "                'Title': title,\n",
                "                'Method': f'SVD (k={k_factors})'\n",
                "            })\n",
                "            rank += 1\n",
                "            \n",
                "    # Save\n",
                "    df_out = pd.DataFrame(all_recs)\n",
                "    save_path = os.path.join(RESULTS_DIR, \"svd_recommendations.csv\")\n",
                "    df_out.to_csv(save_path, index=False)\n",
                "    print(f\"Recommendations saved to {save_path}\")\n",
                "    print(df_out.head(10))\n",
                "    return df_out"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "27754bf4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_svd_pipeline_efficient(R_centered, user_means, user_map, item_map, df_items_map, target_users=None, k=20):\n",
                "    \"\"\"\n",
                "    Memory-Efficient Wrapper.\n",
                "    \"\"\"\n",
                "    # 1. Prepare\n",
                "    R_ready = prepare_matrix_for_svd(R_centered)\n",
                "    \n",
                "    # 2. Decompose\n",
                "    u, sigma, vt = apply_truncated_svd(R_ready, k=k)\n",
                "    \n",
                "    # 3. Recommend (Skip full reconstruction)\n",
                "    if target_users is None:\n",
                "        target_users = list(user_map.keys())[:5]\n",
                "        \n",
                "    svd_recs = generate_svd_recommendations_efficient(\n",
                "        u, sigma, vt, user_means, user_map, item_map, df_items_map, target_users, k\n",
                "    )\n",
                "    \n",
                "    return svd_recs"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
