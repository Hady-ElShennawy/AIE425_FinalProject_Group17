{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Group 17: \n",
                "### Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3. Item-Based Collaborative Filtering System\n",
                "\n",
                "###  Calculating Item Similarity\n",
                "\n",
                "We calculate the pairwise similarity between items using Cosine Similarity on the $R_{centered}$ matrix. Because the data is centered, this results in the Pearson Correlation Coefficient, which is robust to differences in rating scales.\n",
                "\n",
                "###  Prediction Mechanism\n",
                "\n",
                "For a given user $u$ and target item $i$, we predict the rating by aggregating the deviations of similar items the user has rated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Results directory exists: ../results\n",
                        "Libraries imported successfully.\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy.sparse import csr_matrix, diags\n",
                "from scipy.sparse.linalg import svds\n",
                "import os\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import mean_squared_error\n",
                "import time\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "np.random.seed(42)\n",
                "\n",
                "RESULTS_DIR = \"../results\"\n",
                "if not os.path.exists(RESULTS_DIR):\n",
                "    os.makedirs(RESULTS_DIR)\n",
                "    print(f\"Created results directory: {RESULTS_DIR}\")\n",
                "else:\n",
                "    print(f\"Results directory exists: {RESULTS_DIR}\")\n",
                "    \n",
                "print(\"Libraries imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 1] Loading data from: ../data/Amazon_health&household_label_encoded.csv...\n",
                        "Data loaded in 0.0728 seconds.\n",
                        "Data Shape: (14554, 7)\n",
                        "First 5 rows:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>user_id</th>\n",
                            "      <th>item</th>\n",
                            "      <th>item_id_encoded</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>price</th>\n",
                            "      <th>text</th>\n",
                            "      <th>is_green</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>AEV4DP5E3FJH6FHDLXUYQTDEQYCQ</td>\n",
                            "      <td>Sonic Handheld Percussion Massage Gun - Deep T...</td>\n",
                            "      <td>827</td>\n",
                            "      <td>2</td>\n",
                            "      <td>79.99</td>\n",
                            "      <td>This product worked great when it worked, but ...</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
                            "      <td>DUDE Wipes On-The-Go Flushable Wet Wipes - 1 P...</td>\n",
                            "      <td>255</td>\n",
                            "      <td>5</td>\n",
                            "      <td>6.48</td>\n",
                            "      <td>These are amazing for travel or for keeping in...</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
                            "      <td>Sleep Mask for Side Sleeper, 100% Blackout 3D ...</td>\n",
                            "      <td>814</td>\n",
                            "      <td>5</td>\n",
                            "      <td>12.69</td>\n",
                            "      <td>These are great! My other sleep mask pressed o...</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
                            "      <td>Cottonelle Freshfeel Flushable Wet Wipes, Adul...</td>\n",
                            "      <td>226</td>\n",
                            "      <td>5</td>\n",
                            "      <td>15.79</td>\n",
                            "      <td>These are really good quality. Do not tear lik...</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
                            "      <td>Silk Sleep Eye Mask for Men Women, Comfortable...</td>\n",
                            "      <td>808</td>\n",
                            "      <td>5</td>\n",
                            "      <td>8.88</td>\n",
                            "      <td>Great for travel. Super soft and silky. Has ad...</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                        user_id  \\\n",
                            "0  AEV4DP5E3FJH6FHDLXUYQTDEQYCQ   \n",
                            "1  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
                            "2  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
                            "3  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
                            "4  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
                            "\n",
                            "                                                item  item_id_encoded  rating  \\\n",
                            "0  Sonic Handheld Percussion Massage Gun - Deep T...              827       2   \n",
                            "1  DUDE Wipes On-The-Go Flushable Wet Wipes - 1 P...              255       5   \n",
                            "2  Sleep Mask for Side Sleeper, 100% Blackout 3D ...              814       5   \n",
                            "3  Cottonelle Freshfeel Flushable Wet Wipes, Adul...              226       5   \n",
                            "4  Silk Sleep Eye Mask for Men Women, Comfortable...              808       5   \n",
                            "\n",
                            "   price                                               text  is_green  \n",
                            "0  79.99  This product worked great when it worked, but ...         1  \n",
                            "1   6.48  These are amazing for travel or for keeping in...         1  \n",
                            "2  12.69  These are great! My other sleep mask pressed o...         0  \n",
                            "3  15.79  These are really good quality. Do not tear lik...         1  \n",
                            "4   8.88  Great for travel. Super soft and silky. Has ad...         0  "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "file_path = '../data/Amazon_health&household_label_encoded.csv'\n",
                "print(f\"\\n[Step 1] Loading data from: {file_path}...\")\n",
                "start_time = time.time()\n",
                "df = pd.read_csv(file_path)\n",
                "elapsed = time.time() - start_time\n",
                "print(f\"Data loaded in {elapsed:.4f} seconds.\")\n",
                "\n",
                "# Display first few rows\n",
                "print(f\"Data Shape: {df.shape}\")\n",
                "print(\"First 5 rows:\")\n",
                "display(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing: Matrix Construction & Normalization\n",
                "\n",
                "### 2.1 Interaction Matrix Construction\n",
                "\n",
                "We transform the raw interaction logs into a sparse User-Item Matrix ($R$), where rows correspond to unique users and columns to unique items.\n",
                "\n",
                "### 2.2 User Bias Correction\n",
                "\n",
                "We apply **Mean-Centering** to the matrix. By computing and subtracting the average rating ($\\mu_u$) for every user, we isolate the user's preference relative to their own baseline. This step is crucial for accurate similarity computation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 2a] Building User-Item Matrix...\n",
                        "Found 10000 unique users and 1000 unique items.\n",
                        "Matrix Built in 0.0106s. Shape: (10000, 1000) with 14346 ratings.\n",
                        "Sparsity: 99.856540%\n",
                        "\n",
                        "[Step 2b] Mean-Centering Ratings (Bias Removal)...\n",
                        "User Means Computed. Example means: [5. 5. 5. 5. 5.]\n",
                        "Mean Centering complete in 0.0020s.\n"
                    ]
                }
            ],
            "source": [
                "def build_user_item_matrix(df_interactions):\n",
                "    \"\"\"\n",
                "    Constructs sparse User-Item Rating Matrix (Rows=Users, Cols=Items).\n",
                "    Returns: Matrix R, user_map (id->idx), item_map (id->idx)\n",
                "    \"\"\"\n",
                "    print(\"\\n[Step 2a] Building User-Item Matrix...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    user_col = 'user_id'\n",
                "    item_col = 'item_id_encoded'\n",
                "    rating_col = 'rating'\n",
                "    \n",
                "    # Unique IDs\n",
                "    users = sorted(df_interactions[user_col].unique())\n",
                "    items = sorted(df_interactions[item_col].unique())\n",
                "    print(f\"Found {len(users)} unique users and {len(items)} unique items.\")\n",
                "    \n",
                "    user_map = {u: i for i, u in enumerate(users)}\n",
                "    item_map = {it: i for i, it in enumerate(items)}\n",
                "    \n",
                "    # Map Data\n",
                "    row_indices = df_interactions[user_col].map(user_map)\n",
                "    col_indices = df_interactions[item_col].map(item_map)\n",
                "    ratings = df_interactions[rating_col].values\n",
                "    \n",
                "    # Build CSR\n",
                "    R = csr_matrix((ratings, (row_indices, col_indices)), shape=(len(users), len(items)))\n",
                "    elapsed = time.time() - start_time\n",
                "    \n",
                "    print(f\"Matrix Built in {elapsed:.4f}s. Shape: {R.shape} with {R.nnz} ratings.\")\n",
                "    print(f\"Sparsity: {1.0 - R.nnz / (R.shape[0] * R.shape[1]):.6%}\")\n",
                "    \n",
                "    return R, user_map, item_map\n",
                "\n",
                "def center_ratings_by_user(R):\n",
                "    \"\"\"\n",
                "    Subtracts row mean from non-zero elements.\n",
                "    Returns: Centered Matrix R_centered, User Means vector\n",
                "    \"\"\"\n",
                "    print(\"\\n[Step 2b] Mean-Centering Ratings (Bias Removal)...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    row_sums = np.array(R.sum(axis=1)).flatten()\n",
                "    row_counts = np.diff(R.indptr)\n",
                "    \n",
                "    with np.errstate(divide='ignore', invalid='ignore'):\n",
                "        user_means = row_sums / row_counts\n",
                "    user_means[~np.isfinite(user_means)] = 0.0\n",
                "    \n",
                "    print(f\"User Means Computed. Example means: {user_means[:5]}\")\n",
                "    \n",
                "    # Create Centered Matrix\n",
                "    R_coo = R.tocoo()\n",
                "    rows = R_coo.row\n",
                "    cols = R_coo.col\n",
                "    data = R_coo.data\n",
                "    \n",
                "    new_data = data - user_means[rows]\n",
                "    \n",
                "    R_centered = csr_matrix((new_data, (rows, cols)), shape=R.shape)\n",
                "    elapsed = time.time() - start_time\n",
                "    \n",
                "    print(f\"Mean Centering complete in {elapsed:.4f}s.\")\n",
                "    return R_centered, user_means\n",
                "\n",
                "# Execute\n",
                "R, user_map, item_map = build_user_item_matrix(df)\n",
                "R_centered, user_means = center_ratings_by_user(R)\n",
                "\n",
                "# [Added] Maps for Lookup\n",
                "inv_user_map = {v: k for k, v in user_map.items()}\n",
                "inv_item_map = {v: k for k, v in item_map.items()}\n",
                "# title map from df\n",
                "unique_items = df[['item_id_encoded', 'item']].drop_duplicates()\n",
                "item_title_map = dict(zip(unique_items.item_id_encoded, unique_items.item))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Item-Based Collaborative Filtering\n",
                "\n",
                "### Subtask 4: Compute itemâ€“item similarity\n",
                "\n",
                "We calculate the Cosine Similarity between distinct **columns** of $R_{centered}$.\n",
                "Since $R_{centered}$ has user biases removed, this is equivalent to Pearson Correlation.\n",
                "\n",
                "### Subtask 6 & 7: Predict ratings and Handle Edge Cases\n",
                "\n",
                "We predict rating for user $u$ on item $i$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 3] Computing Item-Item Similarity matrix...\n",
                        "Transposed Matrix Shape: (1000, 10000)\n",
                        "Calculating Item Norms...\n",
                        "Computing Dot Product (Similarity)...\n",
                        "Similarity Matrix Computed in 0.0025s. Shape: (1000, 1000)\n"
                    ]
                }
            ],
            "source": [
                "def compute_item_similarity_matrix(R_centered):\n",
                "    \"\"\"\n",
                "    Computes Cosine Similarity between columns (Items).\n",
                "    Since data is centered, this is Pearson Correlation.\n",
                "    \"\"\"\n",
                "    print(\"\\n[Step 3] Computing Item-Item Similarity matrix...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Transpose so Items are Rows\n",
                "    M = R_centered.T.tocsr()\n",
                "    print(f\"Transposed Matrix Shape: {M.shape}\")\n",
                "    \n",
                "    # Compute Norms\n",
                "    print(\"Calculating Item Norms...\")\n",
                "    item_norms = np.sqrt(np.array(M.power(2).sum(axis=1)).flatten())\n",
                "    item_norms[item_norms == 0] = 1e-9\n",
                "    inv_norms = 1.0 / item_norms\n",
                "    \n",
                "    # Normalize\n",
                "    M_normalized = diags(inv_norms) @ M\n",
                "    \n",
                "    # Dot Product\n",
                "    print(\"Computing Dot Product (Similarity)...\")\n",
                "    sim_matrix = M_normalized.dot(M_normalized.T)\n",
                "    \n",
                "    # Set diagonal\n",
                "    sim_matrix.setdiag(0.0)\n",
                "    sim_matrix.eliminate_zeros()\n",
                "    \n",
                "    elapsed = time.time() - start_time\n",
                "    print(f\"Similarity Matrix Computed in {elapsed:.4f}s. Shape: {sim_matrix.shape}\")\n",
                "    return sim_matrix\n",
                "\n",
                "sim_matrix = compute_item_similarity_matrix(R_centered)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_item_based_cf(user_idx, item_idx, R, sim_matrix, user_means, k=20):\n",
                "    \"\"\"\n",
                "    Predicts rating: mu_u + [Sum(sim * (r - mu_u)) / Sum(|sim|)]\n",
                "    \"\"\"\n",
                "    # This function is called many times, so we minimize print here to avoid flooding\n",
                "    sim_row = sim_matrix.getrow(item_idx)\n",
                "    user_row = R.getrow(user_idx)\n",
                "    \n",
                "    rated_indices = user_row.indices\n",
                "    rated_values = user_row.data\n",
                "    \n",
                "    if len(rated_indices) == 0:\n",
                "         return user_means[user_idx]\n",
                "    \n",
                "    rating_map = {idx: val for idx, val in zip(rated_indices, rated_values)}\n",
                "    neighbor_indices = sim_row.indices\n",
                "    neighbor_scores = sim_row.data\n",
                "    \n",
                "    candidates = []\n",
                "    for j, score in zip(neighbor_indices, neighbor_scores):\n",
                "        if j in rating_map:\n",
                "            candidates.append((score, rating_map[j]))\n",
                "            \n",
                "    candidates.sort(key=lambda x: abs(x[0]), reverse=True)\n",
                "    top_k = candidates[:k]\n",
                "    \n",
                "    if not top_k:\n",
                "        return user_means[user_idx]\n",
                "        \n",
                "    weighted_sum = 0.0\n",
                "    sum_abs_sim = 0.0\n",
                "    mu_u = user_means[user_idx]\n",
                "    \n",
                "    for score, r_val in top_k:\n",
                "        dev = r_val - mu_u\n",
                "        weighted_sum += score * dev\n",
                "        sum_abs_sim += abs(score)\n",
                "        \n",
                "    if sum_abs_sim == 0:\n",
                "        return mu_u\n",
                "        \n",
                "    pred = mu_u + (weighted_sum / sum_abs_sim)\n",
                "    return max(1.0, min(5.0, pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation (Item-Based CF)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 4] Starting Evaluation (Train/Test Split)...\n",
                        "Train Samples: 11643\n",
                        "Test Samples: 2911\n",
                        "Rebuilding matrices on TRAIN data to avoid leakage...\n",
                        "\n",
                        "[Step 2a] Building User-Item Matrix...\n",
                        "Found 8485 unique users and 997 unique items.\n",
                        "Matrix Built in 0.0080s. Shape: (8485, 997) with 11517 ratings.\n",
                        "Sparsity: 99.863858%\n",
                        "\n",
                        "[Step 2b] Mean-Centering Ratings (Bias Removal)...\n",
                        "User Means Computed. Example means: [5.         5.         5.         5.         4.33333333]\n",
                        "Mean Centering complete in 0.0010s.\n",
                        "\n",
                        "[Step 3] Computing Item-Item Similarity matrix...\n",
                        "Transposed Matrix Shape: (997, 8485)\n",
                        "Calculating Item Norms...\n",
                        "Computing Dot Product (Similarity)...\n",
                        "Similarity Matrix Computed in 0.0010s. Shape: (997, 997)\n",
                        "Matrices rebuilt in 0.0120s.\n",
                        "Evaluating predictions on 2911 test cases...\n",
                        "  Processed 500/2911...\n",
                        "  Processed 1000/2911...\n",
                        "  Processed 1500/2911...\n",
                        "  Processed 2000/2911...\n",
                        "  Processed 2500/2911...\n",
                        "Evaluation done in 0.1576s.\n",
                        "\n",
                        ">>> Item-Based CF (Centered Cosine) RMSE: 1.4285 <<<\n",
                        "\n",
                        "[Step 4.1] Generating Item-Based CF Recommendations for Top 5 Users...\n",
                        "Saved CF Recommendations to ../results/cf_item_based_recommendations.csv\n",
                        "                           User  Rank  Item_ID  \\\n",
                        "0  AEBOWVW4PMZZICMOGENXNVFTJUCQ     1       18   \n",
                        "1  AEBOWVW4PMZZICMOGENXNVFTJUCQ     2       20   \n",
                        "2  AEBOWVW4PMZZICMOGENXNVFTJUCQ     3       23   \n",
                        "3  AEBOWVW4PMZZICMOGENXNVFTJUCQ     4       35   \n",
                        "4  AEBOWVW4PMZZICMOGENXNVFTJUCQ     5       36   \n",
                        "\n",
                        "                                               Title  Predicted_Rating  \n",
                        "0  6 Pack Mop Replacement Heads for Spin Mop, Mic...               5.0  \n",
                        "1  ABUSA Sheepskin Insoles Women's Premium Think ...               5.0  \n",
                        "2  ACDelco AA and AAA 200-Count Combo Pack Super ...               5.0  \n",
                        "3  AUVON Rechargeable TENS Unit Muscle Stimulator...               5.0  \n",
                        "4  Active Wow Activated Coconut Charcoal Powder -...               5.0  \n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n[Step 4] Starting Evaluation (Train/Test Split)...\")\n",
                "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
                "print(f\"Train Samples: {len(train_df)}\")\n",
                "print(f\"Test Samples: {len(test_df)}\")\n",
                "\n",
                "print(\"Rebuilding matrices on TRAIN data to avoid leakage...\")\n",
                "start_time = time.time()\n",
                "R_train, user_map_train, item_map_train = build_user_item_matrix(train_df)\n",
                "R_train_centered, user_means_train = center_ratings_by_user(R_train)\n",
                "sim_matrix_train = compute_item_similarity_matrix(R_train_centered)\n",
                "elapsed = time.time() - start_time\n",
                "print(f\"Matrices rebuilt in {elapsed:.4f}s.\")\n",
                "\n",
                "def evaluate_cf(test_df, R, sim, user_means, user_map, item_map):\n",
                "    errors = []\n",
                "    print(f\"Evaluating predictions on {len(test_df)} test cases...\")\n",
                "    start_eval = time.time()\n",
                "    count = 0\n",
                "    \n",
                "    for _, row in test_df.iterrows():\n",
                "        uid = row['user_id']\n",
                "        iid = row['item_id_encoded']\n",
                "        true_r = row['rating']\n",
                "        \n",
                "        if uid in user_map and iid in item_map:\n",
                "            u_idx = user_map[uid]\n",
                "            i_idx = item_map[iid]\n",
                "            pred = predict_item_based_cf(u_idx, i_idx, R, sim, user_means, k=20)\n",
                "            errors.append((true_r - pred) ** 2)\n",
                "        else:\n",
                "            # Cold start\n",
                "            errors.append((true_r - 4.4) ** 2)\n",
                "        \n",
                "        count += 1\n",
                "        if count % 500 == 0:\n",
                "             print(f\"  Processed {count}/{len(test_df)}...\")\n",
                "            \n",
                "    rmse = np.sqrt(np.mean(errors))\n",
                "    print(f\"Evaluation done in {time.time() - start_eval:.4f}s.\")\n",
                "    return rmse\n",
                "\n",
                "rmse_cf = evaluate_cf(test_df, R_train, sim_matrix_train, user_means_train, user_map_train, item_map_train)\n",
                "print(f\"\\n>>> Item-Based CF (Centered Cosine) RMSE: {rmse_cf:.4f} <<<\")\n",
                "\n",
                "# --- [Added] Generate Top-10 CF Recommendations for Top 5 Active Users ---\n",
                "print(\"\\n[Step 4.1] Generating Item-Based CF Recommendations for Top 5 Users...\")\n",
                "# Identify Top 5 active users\n",
                "top_5_users = df['user_id'].value_counts().head(5).index.tolist()\n",
                "\n",
                "cf_recs = []\n",
                "\n",
                "for uid in top_5_users:\n",
                "    if uid in user_map:\n",
                "        u_idx = user_map[uid]\n",
                "        # Get rated items to exclude\n",
                "        rated_indices = R.getrow(u_idx).indices\n",
                "        rated_set = set(rated_indices)\n",
                "        \n",
                "        # Predict for all items\n",
                "        user_predictions = []\n",
                "        for i_idx in range(R.shape[1]):\n",
                "            if i_idx not in rated_set:\n",
                "                 # Use existing function using sim_matrix\n",
                "                 score = predict_item_based_cf(u_idx, i_idx, R, sim_matrix, user_means, k=20)\n",
                "                 user_predictions.append((score, i_idx))\n",
                "        \n",
                "        # Sort and pick Top 10\n",
                "        user_predictions.sort(key=lambda x: x[0], reverse=True)\n",
                "        top_10 = user_predictions[:10]\n",
                "        \n",
                "        for rank, (score, i_idx) in enumerate(top_10, 1):\n",
                "            item_code = inv_item_map.get(i_idx, \"Unknown\")\n",
                "            title = item_title_map.get(item_code, \"Unknown Title\")\n",
                "            \n",
                "            cf_recs.append({\n",
                "                'User': uid,\n",
                "                'Rank': rank,\n",
                "                'Item_ID': item_code,\n",
                "                'Title': title,\n",
                "                'Predicted_Rating': round(score, 4)\n",
                "            })\n",
                "\n",
                "df_cf_recs = pd.DataFrame(cf_recs)\n",
                "save_path_cf = \"../results/cf_item_based_recommendations.csv\"\n",
                "df_cf_recs.to_csv(save_path_cf, index=False)\n",
                "print(f\"Saved CF Recommendations to {save_path_cf}\")\n",
                "print(df_cf_recs.head())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Latent Factor Model (SVD)\n",
                "\n",
                "## 8.2. Dimensionality Reduction via SVD\n",
                "\n",
                "We employ **Truncated SVD** to decompose the centered rating matrix into latent factors, effectively reducing noise and sparsity.\n",
                "$$ R_{centered} \\approx U \\cdot \\Sigma \\cdot V^T $$\n",
                "The reconstructed rating is estimated as: $\\hat{r}_{ui} = (U_u \\cdot \\Sigma \\cdot V_i^T) + \\mu_u$\n",
                "\n",
                "### Scalable Prediction Strategy (Memory Efficiency)\n",
                "\n",
                "**Optimization**: Reconstructing the full dense matrix for prediction requires significant memory (proportional to $Users \\times Items$), which often leads to overflows.\n",
                "\n",
                "**Solution**: We implement a **Row-by-Row Prediction** pattern. We compute the dense prediction vector for one target user at a time, extract the top items, and then discard the vector. This keeps memory usage low and constant regardless of the total number of users."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "k     RMSE (Reconstruction)    \n",
                        "------------------------------\n",
                        "\n",
                        "[Step 5] Calculating SVD with k=10...\n",
                        "SVD Done in 0.0097s.\n",
                        "U: (10000, 10), Sigma: (10, 10), Vt: (10, 1000)\n",
                        "Calculating Reconstruction Error...\n",
                        "10    0.0235                   \n",
                        "\n",
                        "[Step 5] Calculating SVD with k=20...\n",
                        "SVD Done in 0.0202s.\n",
                        "U: (10000, 20), Sigma: (20, 20), Vt: (20, 1000)\n",
                        "Calculating Reconstruction Error...\n",
                        "20    0.0225                   \n"
                    ]
                }
            ],
            "source": [
                "def perform_svd(matrix, k):\n",
                "    print(f\"\\n[Step 5] Calculating SVD with k={k}...\")\n",
                "    start_time = time.time()\n",
                "    # Helper to convert to float for SVD\n",
                "    matrix_f = matrix.asfptype()\n",
                "    \n",
                "    U, sigma, Vt = svds(matrix_f, k=k)\n",
                "    \n",
                "    # Sort descending\n",
                "    U = U[:, ::-1]\n",
                "    sigma = sigma[::-1]\n",
                "    Vt = Vt[::-1, :]\n",
                "    \n",
                "    sigma_diag = np.diag(sigma)\n",
                "    print(f\"SVD Done in {time.time() - start_time:.4f}s.\")\n",
                "    print(f\"U: {U.shape}, Sigma: {sigma_diag.shape}, Vt: {Vt.shape}\")\n",
                "    return U, sigma_diag, Vt\n",
                "\n",
                "def calculate_rmse_reconstruction(original_centered, U, sigma_diag, Vt):\n",
                "    # Approximation\n",
                "    print(\"Calculating Reconstruction Error...\")\n",
                "    approx = np.dot(np.dot(U, sigma_diag), Vt)\n",
                "    \n",
                "    diff = original_centered - approx\n",
                "    rmse = np.sqrt(np.mean(np.square(diff)))\n",
                "    return rmse\n",
                "\n",
                "# Evaluate both k=10 and k=20\n",
                "print(\"\\n{:<5} {:<25}\".format(\"k\", \"RMSE (Reconstruction)\"))\n",
                "print(\"-\"*30)\n",
                "\n",
                "for k in [10, 20]:\n",
                "    U, S, Vt = perform_svd(R_centered, k)\n",
                "    rmse_svd = calculate_rmse_reconstruction(R_centered, U, S, Vt)\n",
                "    print(\"{:<5} {:<25.4f}\".format(k, rmse_svd))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 6] Generating efficient SVD Recommendations (k=20)...\n",
                        "\n",
                        "[Step 5] Calculating SVD with k=20...\n",
                        "SVD Done in 0.0229s.\n",
                        "U: (10000, 20), Sigma: (20, 20), Vt: (20, 1000)\n",
                        "Generating SVD recommendations for 5 users...\n",
                        "Saved SVD Recommendations to ../results/svd_recommendations.csv\n",
                        "                           User  Rank  Item_ID  \\\n",
                        "0  AEBOWVW4PMZZICMOGENXNVFTJUCQ     1      191   \n",
                        "1  AEBOWVW4PMZZICMOGENXNVFTJUCQ     2      410   \n",
                        "2  AEBOWVW4PMZZICMOGENXNVFTJUCQ     3      295   \n",
                        "3  AEBOWVW4PMZZICMOGENXNVFTJUCQ     4      972   \n",
                        "4  AEBOWVW4PMZZICMOGENXNVFTJUCQ     5       77   \n",
                        "\n",
                        "                                               Title  Predicted_Rating  \\\n",
                        "0  Charmin Ultra Gentle Toilet Paper, 18 Mega Rol...            4.3757   \n",
                        "1  Glade Automatic Spray Refill, Air Freshener fo...            4.3102   \n",
                        "2  Drive Medical RTL12004KD Handicap Bathroom Sto...            4.2759   \n",
                        "3  Yogasleep Rohm Portable White Noise Machine fo...            4.2705   \n",
                        "4  Amazon Brand - Solimo Sandwich Storage Bags, 3...            4.2666   \n",
                        "\n",
                        "       Method  \n",
                        "0  SVD (k=20)  \n",
                        "1  SVD (k=20)  \n",
                        "2  SVD (k=20)  \n",
                        "3  SVD (k=20)  \n",
                        "4  SVD (k=20)  \n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n[Step 6] Generating efficient SVD Recommendations (k=20)...\")\n",
                "\n",
                "# Efficient SVD Prediction Function (Row-by-Row)\n",
                "def predict_for_user_svd(u_idx, U, S, Vt, user_mean):\n",
                "    \"\"\"\n",
                "    Computes dense prediction vector for a single user to save memory.\n",
                "    Result shape: (n_items, )\n",
                "    P = (User_Latent . Sigma . Vt) + Mean\n",
                "    \"\"\"\n",
                "    # User latent vector: U[u_idx] -> shape (1, k)\n",
                "    user_vec = U[u_idx, :].reshape(1, -1)\n",
                "    \n",
                "    # w = User_Latent . S\n",
                "    w = np.dot(user_vec, S)\n",
                "    \n",
                "    # Preds = w . Vt\n",
                "    preds = np.dot(w, Vt).flatten() + user_mean\n",
                "    return np.clip(preds, 1.0, 5.0)\n",
                "\n",
                "# Use the best model k=20\n",
                "k_best = 20\n",
                "U, S, Vt = perform_svd(R_centered, k_best)\n",
                "\n",
                "svd_recs = []\n",
                "\n",
                "# Reuse Top 5 users\n",
                "top_5_users = df['user_id'].value_counts().head(5).index.tolist()\n",
                "\n",
                "print(f\"Generating SVD recommendations for {len(top_5_users)} users...\")\n",
                "\n",
                "for uid in top_5_users:\n",
                "    if uid in user_map:\n",
                "        u_idx = user_map[uid]\n",
                "        user_mean = user_means[u_idx]\n",
                "        \n",
                "        # Predict all items for this user efficiently\n",
                "        preds = predict_for_user_svd(u_idx, U, S, Vt, user_mean)\n",
                "        \n",
                "        # Accessing R sparse row again to filter out already rated items\n",
                "        rated_indices = set(R.getrow(u_idx).indices)\n",
                "        \n",
                "        # Create list of (score, idx) for unrated items only\n",
                "        candidates = []\n",
                "        for i_idx in range(len(preds)):\n",
                "            if i_idx not in rated_indices:\n",
                "                candidates.append((preds[i_idx], i_idx))\n",
                "        \n",
                "        # Sort by score descending\n",
                "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
                "        top_20 = candidates[:20]\n",
                "        \n",
                "        for rank, (score, i_idx) in enumerate(top_20, 1):\n",
                "            item_code = inv_item_map.get(i_idx, \"Unknown\")\n",
                "            title = item_title_map.get(item_code, \"Unknown Title\")\n",
                "            \n",
                "            svd_recs.append({\n",
                "                'User': uid,\n",
                "                'Rank': rank,\n",
                "                'Item_ID': item_code,\n",
                "                'Title': title,\n",
                "                'Predicted_Rating': round(score, 4),\n",
                "                'Method': f'SVD (k={k_best})'\n",
                "            })\n",
                "\n",
                "df_svd_recs = pd.DataFrame(svd_recs)\n",
                "save_path_svd = \"../results/svd_recommendations.csv\"\n",
                "df_svd_recs.to_csv(save_path_svd, index=False)\n",
                "print(f\"Saved SVD Recommendations to {save_path_svd}\")\n",
                "print(df_svd_recs.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
