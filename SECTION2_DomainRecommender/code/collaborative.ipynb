{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 3: Collaborative Filtering and Hybrid Approach\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.sparse.linalg import svds\n",
                "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from utils import *\n",
                "\n",
                "ensure_results_folders()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Loading and Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "filename = 'Amazon_health&household_label_encoded.csv'\n",
                "print(f\"Loading {filename}...\")\n",
                "df = load_data(filename)\n",
                "\n",
                "if df is not None:\n",
                "    print(\"Dataset/Sample loaded successfully.\")\n",
                "    print(df.head())\n",
                "    print(f\"Shape: {df.shape}\")\n",
                "else:\n",
                "    print(\"Failed to load dataset.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter/Rename columns if necessary to match standard user-item format (userId, itemId, rating)\n",
                "# Assuming the encoded csv has columns that map to user, item, rating. \n",
                "# Let's check columns and map them. \n",
                "# Usually: 'user_id', 'item_id', 'rating' or similar.\n",
                "if df is not None:\n",
                "    print(\"Columns:\", df.columns)\n",
                "    # Heuristic column mapping if needed, or assume standard names from inspection if possible\n",
                "    # Adjusted based on typical label encoded files: usually 'user_encoded', 'item_encoded', 'rating'\n",
                "    # We will assume first 3 columns are user, item, rating if strict names aren't found, \n",
                "    # or check specifically for 'user', 'item', 'rating' variations.\n",
                "    \n",
                "    # Pivot to create User-Item Matrix\n",
                "    # Using mean filling as per Section 1\n",
                "    print(\"Creating User-Item Matrix...\")\n",
                "    # Identify user and item columns\n",
                "    user_col = [c for c in df.columns if 'user' in c.lower()][0]\n",
                "    item_col = [c for c in df.columns if 'item' in c.lower() or 'movie' in c.lower() or 'product' in c.lower()][0]\n",
                "    rating_col = [c for c in df.columns if 'rating' in c.lower()][0]\n",
                "    \n",
                "    print(f\"Using columns: User='{user_col}', Item='{item_col}', Rating='{rating_col}'\")\n",
                "    \n",
                "    # Handle duplicates if any (User-Item pairs should be unique)\n",
                "    if df.duplicated(subset=[user_col, item_col]).any():\n",
                "        print(f\"Duplicate entries found: {df.duplicated(subset=[user_col, item_col]).sum()}\")\n",
                "        print(\"Aggregating duplicates by taking the mean rating...\")\n",
                "        df = df.groupby([user_col, item_col], as_index=False)[rating_col].mean()\n",
                "    \n",
                "    # Pivot to create User-Item Matrix\n",
                "    R_df = df.pivot(index=user_col, columns=item_col, values=rating_col)\n",
                "    \n",
                "    # Normalization: Subtract User Means\n",
                "    print(\"Normalizing by subtracting User Means...\")\n",
                "    user_means = R_df.mean(axis=1)\n",
                "    R_centered = R_df.sub(user_means, axis=0)\n",
                "    \n",
                "    # Fill NaN with 0 (neutral relative to user's average)\n",
                "    R_filled = R_centered.fillna(0)\n",
                "    R = R_filled.values\n",
                "    \n",
                "    print(f\"Matrix R Shape: {R.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Collaborative Filtering Integration"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8.1: Item-Based Collaborative Filtering (Cosine Similarity)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def perform_item_based_cf(matrix_centered, top_n=5):\n",
                "    print(\"\\nComputing Item-Item Cosine Similarity...\")\n",
                "    # Calculate Cosine Similarity between Items\n",
                "    # matrix_centered is (Users x Items), so we transpose to get (Items x Users)\n",
                "    # Samples=Items, Features=Users\n",
                "    item_similarity = cosine_similarity(matrix_centered.T)\n",
                "    \n",
                "    print(f\"Item Similarity Matrix Shape: {item_similarity.shape}\")\n",
                "    \n",
                "    # Predict: Weighted Sum of ratings\n",
                "    print(\"Generating predictions (Item-Based)...\")\n",
                "    pred_weighted_sum = np.dot(matrix_centered, item_similarity)\n",
                "    \n",
                "    # Normalize by sum of absolute similarities\n",
                "    sum_abs_sim = np.array([np.abs(item_similarity).sum(axis=1)])\n",
                "    \n",
                "    # Avoid division by zero\n",
                "    sum_abs_sim[sum_abs_sim == 0] = 1e-9\n",
                "    \n",
                "    R_pred_centered = pred_weighted_sum / sum_abs_sim\n",
                "    \n",
                "    return R_pred_centered, item_similarity\n",
                "\n",
                "def recommend_items(user_idx, R_pred, original_df_pivot, top_n=5):\n",
                "    # Helper reused for both Item-Based and SVD\n",
                "    # User ID\n",
                "    user_id_label = original_df_pivot.index[user_idx]\n",
                "    \n",
                "    # Get user's predicted ratings (Add mean back!)\n",
                "    # R_pred passed here is the CENTERED prediction row\n",
                "    user_mean = original_df_pivot.iloc[user_idx].mean(skipna=True)\n",
                "    user_predicted_ratings = R_pred + user_mean\n",
                "    \n",
                "    # Get observed items to filter them out\n",
                "    observed_items = original_df_pivot.iloc[user_idx].dropna().index\n",
                "    \n",
                "    # Create a series for easier sorting/filtering\n",
                "    predictions = pd.Series(user_predicted_ratings, index=original_df_pivot.columns)\n",
                "    \n",
                "    # Filter out already rated items\n",
                "    predictions = predictions.drop(labels=observed_items, errors='ignore')\n",
                "    \n",
                "    # Sort descending\n",
                "    top_recommendations = predictions.sort_values(ascending=False).head(top_n)\n",
                "    \n",
                "    return top_recommendations\n",
                "\n",
                "if 'R' in locals():\n",
                "    # Perform Item-Based CF\n",
                "    R_pred_item_centered, item_sim_matrix = perform_item_based_cf(R)\n",
                "    \n",
                "    # Select a few target users (e.g., first 5 in the matrix)\n",
                "    target_users = range(5)\n",
                "    \n",
                "    print(f\"\\nTop 5 Personalized Recommendations (Item-Based CF) for first 5 users:\")\n",
                "    for u_idx in target_users:\n",
                "        user_real_id = R_df.index[u_idx]\n",
                "        recs = recommend_items(u_idx, R_pred_item_centered[u_idx, :], R_df)\n",
                "        \n",
                "        print(f\"\\nUser {user_real_id}:\")\n",
                "        for i, (item_name, rating) in enumerate(recs.items()):\n",
                "            print(f\"  {i+1}. Item {item_name} (Pred: {rating:.2f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8.2: Matrix Factorization using SVD"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def perform_svd(matrix, k):\n",
                "    print(f\"\\nCalculating SVD with k={k}...\")\n",
                "    U, sigma, Vt = svds(matrix, k=k)\n",
                "    \n",
                "    # Reverse to have descending order\n",
                "    U = U[:, ::-1]\n",
                "    sigma = sigma[::-1]\n",
                "    Vt = Vt[::-1, :]\n",
                "    \n",
                "    sigma_diag = np.diag(sigma)\n",
                "    \n",
                "    # Reconstruct (This is the approximated CENTERED matrix)\n",
                "    R_k_centered = np.dot(np.dot(U, sigma_diag), Vt)\n",
                "    \n",
                "    return U, sigma, Vt, R_k_centered\n",
                "\n",
                "def calculate_metrics(original_centered, approximation_centered):\n",
                "    # We want to measure how well we approximate the original ratings (observed ones)\n",
                "    # original_centered has 0s where data was missing, we shouldn't really penalize matching the 0s\n",
                "    # but standard RMSE on the whole matrix is what was requested implicitly.\n",
                "    diff = original_centered - approximation_centered\n",
                "    mae = np.mean(np.abs(diff))\n",
                "    rmse = np.sqrt(np.mean(np.square(diff)))\n",
                "    return mae, rmse\n",
                "\n",
                "k_values = [10, 20]\n",
                "svd_results = {}\n",
                "\n",
                "if 'R' in locals():\n",
                "    print(\"{:<5} {:<10} {:<10}\".format(\"k\", \"MAE\", \"RMSE\"))\n",
                "    print(\"-\"*30)\n",
                "    \n",
                "    for k in k_values:\n",
                "        U, sigma, Vt, R_k_centered = perform_svd(R, k)\n",
                "        mae, rmse = calculate_metrics(R, R_k_centered)\n",
                "        \n",
                "        svd_results[k] = {\n",
                "            'R_k_centered': R_k_centered, \n",
                "            'MAE': mae, \n",
                "            'RMSE': rmse\n",
                "        }\n",
                "        print(\"{:<5} {:<10.4f} {:<10.4f}\".format(k, mae, rmse))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'R' in locals():\n",
                "    # Select predictions for k=20\n",
                "    best_k = 20\n",
                "    if best_k in svd_results:\n",
                "        R_final_centered = svd_results[best_k]['R_k_centered']\n",
                "        \n",
                "        # Select a few target users (e.g., first 5 in the matrix)\n",
                "        target_users = range(5)\n",
                "        \n",
                "        print(f\"\\nTop 5 Personalized Recommendations (SVD k={best_k}) for first 5 users:\")\n",
                "        for u_idx in target_users:\n",
                "            user_real_id = R_df.index[u_idx]\n",
                "            recs = recommend_items(u_idx, R_final_centered[u_idx, :], R_df)\n",
                "            \n",
                "            print(f\"\\nUser {user_real_id}:\")\n",
                "            for i, (item_name, rating) in enumerate(recs.items()):\n",
                "                print(f\"  {i+1}. Item {item_name} (Pred: {rating:.2f})\")\n",
                "    else:\n",
                "         print(f\"SVD results for k={best_k} not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Hybrid Recommendation Strategy    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Prepare Item Features\n",
                "print(\"Preparing Item Features (Robust: Text + Price + Is_Green)...\")\n",
                "\n",
                "# Reload dataset to ensure we have all columns\n",
                "df_meta = load_data('Amazon_health&household_label_encoded.csv')\n",
                "\n",
                "if df_meta is not None:\n",
                "    # Separate unique items\n",
                "    # We need 'item_id', 'item', 'price', 'text', 'is_green'\n",
                "    # Assuming 'item' is the unique name, but let's strictly use the index from R_df to align later\n",
                "    \n",
                "    # Let's map R_df columns (items) to their metadata\n",
                "    unique_items = R_df.columns.tolist()\n",
                "    item_metadata = df_meta[['item', 'price', 'text', 'is_green']].drop_duplicates(subset='item').set_index('item')\n",
                "    \n",
                "    # Reindex to match R_df columns structure exactly\n",
                "    item_metadata = item_metadata.reindex(unique_items)\n",
                "    \n",
                "    # Handle NaNs\n",
                "    item_metadata['text'] = item_metadata['text'].fillna('')\n",
                "    item_metadata['price'] = item_metadata['price'].fillna(item_metadata['price'].median())\n",
                "    item_metadata['is_green'] = item_metadata['is_green'].fillna(0).astype(int)\n",
                "    \n",
                "    print(f\"Unique Items aligned with Matrix: {len(item_metadata)}\")\n",
                "    \n",
                "    # A. Text (TF-IDF)\n",
                "    print(\"  Computing TF-IDF...\")\n",
                "    tfidf = TfidfVectorizer(stop_words='english', max_features=100)\n",
                "    text_matrix = tfidf.fit_transform(item_metadata['text']).toarray()\n",
                "    \n",
                "    # B. Price (Normalized)\n",
                "    print(\"  Normalizing Price...\")\n",
                "    scaler = MinMaxScaler()\n",
                "    price_vec = scaler.fit_transform(item_metadata[['price']])\n",
                "    \n",
                "    # C. Is_Green\n",
                "    green_vec = item_metadata[['is_green']].values\n",
                "    \n",
                "    # D. Combine\n",
                "    item_features = np.hstack([text_matrix, price_vec, green_vec])\n",
                "    print(f\"  Item-Feature Matrix Ready: {item_features.shape}\")\n",
                "\n",
                "# 2. Build User Profiles\n",
                "def build_user_profiles(R_df, item_features_matrix):\n",
                "    print(\"\\nBuilding User Profiles...\")\n",
                "    user_profiles = {}\n",
                "    \n",
                "    # cold_start_vector = Global Average of all items\n",
                "    cold_start_vector = np.mean(item_features_matrix, axis=0)\n",
                "    \n",
                "    for user_id in R_df.index:\n",
                "        # Get user's ratings\n",
                "        user_ratings = R_df.loc[user_id]\n",
                "        \n",
                "        # Filter for rated items only (non-NaN)\n",
                "        rated_items = user_ratings[user_ratings.notna()]\n",
                "        \n",
                "        if rated_items.empty:\n",
                "            user_profiles[user_id] = cold_start_vector\n",
                "            continue\n",
                "            \n",
                "        # Get indices of rated items\n",
                "        # R_df.columns are items, which align with item_features_matrix rows 0..N\n",
                "        # We need integer indices of these rated items\n",
                "        # Since item_metadata was reindexed by R_df.columns, index i corresponds to column i\n",
                "        \n",
                "        # Get boolean mask or integer indices?\n",
                "        # Let's get integer indices of the rated items in the columns list\n",
                "        item_indices = [R_df.columns.get_loc(item) for item in rated_items.index]\n",
                "        \n",
                "        # Get vectors\n",
                "        rated_item_vectors = item_features_matrix[item_indices]\n",
                "        \n",
                "        # Get ratings weights\n",
                "        weights = rated_items.values.reshape(-1, 1)\n",
                "        \n",
                "        # Weighted Average\n",
                "        weighted_sum = np.sum(rated_item_vectors * weights, axis=0)\n",
                "        total_weight = np.sum(weights)\n",
                "        \n",
                "        if total_weight == 0:\n",
                "             user_profiles[user_id] = cold_start_vector\n",
                "        else:\n",
                "             user_profiles[user_id] = weighted_sum / total_weight\n",
                "             \n",
                "    print(f\"  Built profiles for {len(user_profiles)} users.\")\n",
                "    return user_profiles\n",
                "\n",
                "if 'item_features' in locals():\n",
                "    user_profiles = build_user_profiles(R_df, item_features)\n",
                "\n",
                "def get_hybrid_recommendations(user_idx, alpha, R_df, item_based_pred_centered, user_profiles, item_features, top_n=5):\n",
                "    # alpha: Weight for Content-Based\n",
                "    # (1-alpha): Weight for CF (Item-Based)\n",
                "    \n",
                "    user_id = R_df.index[user_idx]\n",
                "    \n",
                "    # --- 1. CF Component (Item-Based) ---\n",
                "    # We already have predictions from Section 2: item_based_pred_centered\n",
                "    # This is (Users x Items). Get row.\n",
                "    cf_scores_centered = item_based_pred_centered[user_idx, :]\n",
                "    \n",
                "    # Add User Mean to get absolute ratings scale (1-5 approx)\n",
                "    user_mean = R_df.iloc[user_idx].mean(skipna=True)\n",
                "    if pd.isna(user_mean): user_mean = 0 # Handle cold start user mean\n",
                "    cf_scores = cf_scores_centered + user_mean\n",
                "    \n",
                "    # Normalize CF Scores (0-1)\n",
                "    cf_min, cf_max = cf_scores.min(), cf_scores.max()\n",
                "    if cf_max - cf_min != 0:\n",
                "        cf_norm = (cf_scores - cf_min) / (cf_max - cf_min)\n",
                "    else:\n",
                "        cf_norm = np.zeros_like(cf_scores)\n",
                "        \n",
                "    # --- 2. CB Component (User Profile Similarity) ---\n",
                "    if user_id in user_profiles:\n",
                "        user_prof_vec = user_profiles[user_id].reshape(1, -1)\n",
                "        # Cosine Similarity with ALL items\n",
                "        cb_scores = cosine_similarity(user_prof_vec, item_features).flatten()\n",
                "    else:\n",
                "        # Fallback if missing profile\n",
                "        cb_scores = np.zeros(len(item_features))\n",
                "        \n",
                "    # Normalize CB Scores (cosine is -1 to 1, usually 0 to 1 for non-negative vectors)\n",
                "    # Let's MinMax it too to be safe and consistent\n",
                "    cb_min, cb_max = cb_scores.min(), cb_scores.max()\n",
                "    if cb_max - cb_min != 0:\n",
                "        cb_norm = (cb_scores - cb_min) / (cb_max - cb_min)\n",
                "    else:\n",
                "        cb_norm = np.zeros_like(cb_scores)\n",
                "        \n",
                "    # --- 3. Combine ---\n",
                "    hybrid_scores = (alpha * cb_norm) + ((1 - alpha) * cf_norm)\n",
                "    \n",
                "    # --- 4. Recommend ---\n",
                "    observed_items = R_df.iloc[user_idx].dropna().index\n",
                "    predictions = pd.Series(hybrid_scores, index=R_df.columns)\n",
                "    predictions = predictions.drop(labels=observed_items, errors='ignore')\n",
                "    \n",
                "    return predictions.sort_values(ascending=False).head(top_n)\n",
                "\n",
                "# Note: We need 'R_pred_item_centered' from Section 2 for the Item-Based Scores\n",
                "if 'R_pred_item_centered' in locals() and 'user_profiles' in locals():\n",
                "    alphas = [0.3, 0.5, 0.7]\n",
                "    target_users = range(5)\n",
                "    \n",
                "    for alpha in alphas:\n",
                "        print(f\"\\n{'='*50}\")\n",
                "        print(f\"Hybrid Recommendations (alpha={alpha} [CB={alpha}, CF={1-alpha}])\")\n",
                "        print(f\"{'='*50}\")\n",
                "        \n",
                "        for u_idx in target_users:\n",
                "            user_real_id = R_df.index[u_idx]\n",
                "            # Pass R_pred_item_centered to Hybrid\n",
                "            recs = get_hybrid_recommendations(u_idx, alpha, R_df, R_pred_item_centered, user_profiles, item_features)\n",
                "            \n",
                "            print(f\"\\nUser {user_real_id}:\")\n",
                "            for i, (item_name, score) in enumerate(recs.items()):\n",
                "                print(f\"  {i+1}. Item {item_name} (Score: {score:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Cold-Start Handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n10. Cold-Start Analysis\")\n",
                "print(\"-\"*30)\n",
                "\n",
                "# Find users with specific numbers of ratings\n",
                "user_counts = R_df.notna().sum(axis=1)\n",
                "\n",
                "targets = [3, 5, 10]\n",
                "cold_start_users = {}\n",
                "\n",
                "for t in targets:\n",
                "    # Find users with approx t ratings (t to t+2) to ensure we find someone\n",
                "    candidates = user_counts[(user_counts >= t) & (user_counts <= t+2)].index\n",
                "    if len(candidates) > 0:\n",
                "        cold_start_users[t] = R_df.index.get_loc(candidates[0])\n",
                "    else:\n",
                "        print(f\"No user found with ~{t} ratings.\")\n",
                "\n",
                "# Define Popularity Baseline Function\n",
                "def get_popular_recommendations(original_df_pivot, top_n=5):\n",
                "    # Mean rating per item (or count)\n",
                "    # Let's use count of ratings -> Popularity\n",
                "    item_counts = original_df_pivot.notna().sum(axis=0)\n",
                "    return item_counts.sort_values(ascending=False).head(top_n)\n",
                "\n",
                "popular_items = get_popular_recommendations(R_df)\n",
                "print(\"\\nMost Popular Items (Baseline):\")\n",
                "print(popular_items.index.tolist())\n",
                "\n",
                "# Run Comparison for Cold Start Users\n",
                "if 'R_pred_item_centered' in locals() and 'user_profiles' in locals():\n",
                "    for t, u_idx in cold_start_users.items():\n",
                "        user_real_id = R_df.index[u_idx]\n",
                "        print(f\"\\nUser with ~{t} ratings ({user_real_id}):\")\n",
                "        \n",
                "        # history\n",
                "        obs = R_df.iloc[u_idx].dropna().index.tolist()\n",
                "        print(f\"  History: {obs[:3]} ... ({len(obs)} total)\")\n",
                "        \n",
                "        # Hybrid Recs (alpha=0.5)\n",
                "        hybrid_recs = get_hybrid_recommendations(u_idx, 0.5, R_df, R_pred_item_centered, user_profiles, item_features)\n",
                "        print(\"  [Hybrid Recommendations]:\")\n",
                "        for i, (item, score) in enumerate(hybrid_recs.items()):\n",
                "            print(f\"    {i+1}. {item}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Baseline Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n11. Baseline Comparison\")\n",
                "print(\"-\"*30)\n",
                "\n",
                "# We will qualitatively compare recommendations for a single test user\n",
                "# to see the 'flavor' of recommendations from each model.\n",
                "\n",
                "test_user_idx = list(cold_start_users.values())[0] if cold_start_users else 0\n",
                "test_user_id = R_df.index[test_user_idx]\n",
                "\n",
                "# 1. Random\n",
                "all_items = R_df.columns.tolist()\n",
                "random_recs = np.random.choice(all_items, 5, replace=False)\n",
                "\n",
                "# 2. Popularity\n",
                "pop_recs = popular_items.index.tolist()\n",
                "\n",
                "# 3. Pure Content (Hybrid alpha=1.0)\n",
                "cb_recs = get_hybrid_recommendations(test_user_idx, 1.0, R_df, R_pred_item_centered, user_profiles, item_features).index.tolist()\n",
                "\n",
                "# 4. Pure CF (Hybrid alpha=0.0)\n",
                "# This is now ITEM-BASED CF\n",
                "cf_recs = get_hybrid_recommendations(test_user_idx, 0.0, R_df, R_pred_item_centered, user_profiles, item_features).index.tolist()\n",
                "\n",
                "# 5. Hybrid (alpha=0.5)\n",
                "hyb_recs = get_hybrid_recommendations(test_user_idx, 0.5, R_df, R_pred_item_centered, user_profiles, item_features).index.tolist()\n",
                "\n",
                "# Display\n",
                "comparison_df = pd.DataFrame({\n",
                "    'Metric/Model': ['Random', 'Popularity', 'Pure Content (CB)', 'Pure CF (SVD)', 'Hybrid (0.5)'],\n",
                "    'Top 1 Recommendation': [random_recs[0], pop_recs[0], cb_recs[0], cf_recs[0], hyb_recs[0]],\n",
                "    'Top 2 Recommendation': [random_recs[1], pop_recs[1], cb_recs[1], cf_recs[1], hyb_recs[1]]\n",
                "})\n",
                "\n",
                "print(f\"\\nComparison for User {test_user_id}:\")\n",
                "print(comparison_df.to_string(index=False))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}