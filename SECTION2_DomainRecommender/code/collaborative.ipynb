{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Group 17: \n",
                "### Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3. Item-Based Collaborative Filtering System\n",
                "\n",
                "###  Calculating Item Similarity\n",
                "\n",
                "We calculate the pairwise similarity between items using Cosine Similarity on the $R_{centered}$ matrix. Because the data is centered, this results in the Pearson Correlation Coefficient, which is robust to differences in rating scales.\n",
                "\n",
                "###  Prediction Mechanism\n",
                "\n",
                "For a given user $u$ and target item $i$, we predict the rating by aggregating the deviations of similar items the user has rated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Results directory exists: ../results\n",
                        "Libraries imported successfully.\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy.sparse import csr_matrix, diags\n",
                "from scipy.sparse.linalg import svds\n",
                "import os\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import mean_squared_error\n",
                "import time\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "np.random.seed(42)\n",
                "\n",
                "RESULTS_DIR = \"../results\"\n",
                "if not os.path.exists(RESULTS_DIR):\n",
                "    os.makedirs(RESULTS_DIR)\n",
                "    print(f\"Created results directory: {RESULTS_DIR}\")\n",
                "else:\n",
                "    print(f\"Results directory exists: {RESULTS_DIR}\")\n",
                "    \n",
                "print(\"Libraries imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 1] Loading data from: ../data/Amazon_health&household_label_encoded.csv...\n",
                        "Data loaded in 0.3870 seconds.\n",
                        "Data Shape: (83355, 7)\n",
                        "First 5 rows:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>user_id</th>\n",
                            "      <th>item</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>is_green</th>\n",
                            "      <th>price</th>\n",
                            "      <th>text</th>\n",
                            "      <th>item_id_encoded</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                            "      <td>Dawn Ultra Dishwashing Liquid, Original Scent ...</td>\n",
                            "      <td>4</td>\n",
                            "      <td>0</td>\n",
                            "      <td>22.77</td>\n",
                            "      <td>Too expensive</td>\n",
                            "      <td>107204</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                            "      <td>Charmin Ultra Soft Cushiony Touch Toilet Paper...</td>\n",
                            "      <td>4</td>\n",
                            "      <td>0</td>\n",
                            "      <td>28.82</td>\n",
                            "      <td>Very good paper but too expensiveoo</td>\n",
                            "      <td>85026</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                            "      <td>Viva Signature Cloth Choose-A-Sheet Paper Towe...</td>\n",
                            "      <td>4</td>\n",
                            "      <td>0</td>\n",
                            "      <td>23.65</td>\n",
                            "      <td>Expensive but great towels</td>\n",
                            "      <td>358978</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                            "      <td>Drano Max Gel Drain Clog Remover and Cleaner f...</td>\n",
                            "      <td>2</td>\n",
                            "      <td>0</td>\n",
                            "      <td>15.47</td>\n",
                            "      <td>The old Draino worked better.  This pours easi...</td>\n",
                            "      <td>117303</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                            "      <td>LiCB A23 23A 12V Alkaline Battery (5-Pack)</td>\n",
                            "      <td>4</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5.99</td>\n",
                            "      <td>I jut got them, don’t know yet if they last long</td>\n",
                            "      <td>204987</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                        user_id  \\\n",
                            "0  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                            "1  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                            "2  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                            "3  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                            "4  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                            "\n",
                            "                                                item  rating  is_green  price  \\\n",
                            "0  Dawn Ultra Dishwashing Liquid, Original Scent ...       4         0  22.77   \n",
                            "1  Charmin Ultra Soft Cushiony Touch Toilet Paper...       4         0  28.82   \n",
                            "2  Viva Signature Cloth Choose-A-Sheet Paper Towe...       4         0  23.65   \n",
                            "3  Drano Max Gel Drain Clog Remover and Cleaner f...       2         0  15.47   \n",
                            "4         LiCB A23 23A 12V Alkaline Battery (5-Pack)       4         0   5.99   \n",
                            "\n",
                            "                                                text  item_id_encoded  \n",
                            "0                                      Too expensive           107204  \n",
                            "1                Very good paper but too expensiveoo            85026  \n",
                            "2                         Expensive but great towels           358978  \n",
                            "3  The old Draino worked better.  This pours easi...           117303  \n",
                            "4   I jut got them, don’t know yet if they last long           204987  "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "file_path = '../data/Amazon_health&household_label_encoded.csv'\n",
                "print(f\"\\n[Step 1] Loading data from: {file_path}...\")\n",
                "start_time = time.time()\n",
                "df = pd.read_csv(file_path)\n",
                "elapsed = time.time() - start_time\n",
                "print(f\"Data loaded in {elapsed:.4f} seconds.\")\n",
                "\n",
                "# Display first few rows\n",
                "print(f\"Data Shape: {df.shape}\")\n",
                "print(\"First 5 rows:\")\n",
                "display(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing: Matrix Construction & Normalization\n",
                "\n",
                "### 2.1 Interaction Matrix Construction\n",
                "\n",
                "We transform the raw interaction logs into a sparse User-Item Matrix ($R$), where rows correspond to unique users and columns to unique items.\n",
                "\n",
                "### 2.2 User Bias Correction\n",
                "\n",
                "We apply **Mean-Centering** to the matrix. By computing and subtracting the average rating ($\\mu_u$) for every user, we isolate the user's preference relative to their own baseline. This step is crucial for accurate similarity computation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 2a] Building User-Item Matrix...\n",
                        "Found 9591 unique users and 1000 unique items.\n",
                        "Matrix Built in 0.0181s. Shape: (9591, 1000) with 79635 ratings.\n",
                        "Sparsity: 99.169690%\n",
                        "\n",
                        "[Step 2b] Mean-Centering Ratings (Bias Removal)...\n",
                        "User Means Computed. Example means: [3.85714286 5.09090909 4.6        3.74074074 4.66666667]\n",
                        "Mean Centering complete in 0.0020s.\n"
                    ]
                }
            ],
            "source": [
                "def build_user_item_matrix(df_interactions):\n",
                "    \"\"\"\n",
                "    Constructs sparse User-Item Rating Matrix (Rows=Users, Cols=Items).\n",
                "    Returns: Matrix R, user_map (id->idx), item_map (id->idx)\n",
                "    \"\"\"\n",
                "    print(\"\\n[Step 2a] Building User-Item Matrix...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    user_col = 'user_id'\n",
                "    item_col = 'item_id_encoded'\n",
                "    rating_col = 'rating'\n",
                "    \n",
                "    # Unique IDs\n",
                "    users = sorted(df_interactions[user_col].unique())\n",
                "    items = sorted(df_interactions[item_col].unique())\n",
                "    print(f\"Found {len(users)} unique users and {len(items)} unique items.\")\n",
                "    \n",
                "    user_map = {u: i for i, u in enumerate(users)}\n",
                "    item_map = {it: i for i, it in enumerate(items)}\n",
                "    \n",
                "    # Map Data\n",
                "    row_indices = df_interactions[user_col].map(user_map)\n",
                "    col_indices = df_interactions[item_col].map(item_map)\n",
                "    ratings = df_interactions[rating_col].values\n",
                "    \n",
                "    # Build CSR\n",
                "    R = csr_matrix((ratings, (row_indices, col_indices)), shape=(len(users), len(items)))\n",
                "    elapsed = time.time() - start_time\n",
                "    \n",
                "    print(f\"Matrix Built in {elapsed:.4f}s. Shape: {R.shape} with {R.nnz} ratings.\")\n",
                "    print(f\"Sparsity: {1.0 - R.nnz / (R.shape[0] * R.shape[1]):.6%}\")\n",
                "    \n",
                "    return R, user_map, item_map\n",
                "\n",
                "def center_ratings_by_user(R):\n",
                "    \"\"\"\n",
                "    Subtracts row mean from non-zero elements.\n",
                "    Returns: Centered Matrix R_centered, User Means vector\n",
                "    \"\"\"\n",
                "    print(\"\\n[Step 2b] Mean-Centering Ratings (Bias Removal)...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    row_sums = np.array(R.sum(axis=1)).flatten()\n",
                "    row_counts = np.diff(R.indptr)\n",
                "    \n",
                "    with np.errstate(divide='ignore', invalid='ignore'):\n",
                "        user_means = row_sums / row_counts\n",
                "    user_means[~np.isfinite(user_means)] = 0.0\n",
                "    \n",
                "    print(f\"User Means Computed. Example means: {user_means[:5]}\")\n",
                "    \n",
                "    # Create Centered Matrix\n",
                "    R_coo = R.tocoo()\n",
                "    rows = R_coo.row\n",
                "    cols = R_coo.col\n",
                "    data = R_coo.data\n",
                "    \n",
                "    new_data = data - user_means[rows]\n",
                "    \n",
                "    R_centered = csr_matrix((new_data, (rows, cols)), shape=R.shape)\n",
                "    elapsed = time.time() - start_time\n",
                "    \n",
                "    print(f\"Mean Centering complete in {elapsed:.4f}s.\")\n",
                "    return R_centered, user_means\n",
                "\n",
                "# Execute\n",
                "R, user_map, item_map = build_user_item_matrix(df)\n",
                "R_centered, user_means = center_ratings_by_user(R)\n",
                "\n",
                "# [Added] Maps for Lookup\n",
                "inv_user_map = {v: k for k, v in user_map.items()}\n",
                "inv_item_map = {v: k for k, v in item_map.items()}\n",
                "# title map from df\n",
                "unique_items = df[['item_id_encoded', 'item']].drop_duplicates()\n",
                "item_title_map = dict(zip(unique_items.item_id_encoded, unique_items.item))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Item-Based Collaborative Filtering\n",
                "\n",
                "### Subtask 4: Compute item–item similarity\n",
                "\n",
                "We calculate the Cosine Similarity between distinct **columns** of $R_{centered}$.\n",
                "Since $R_{centered}$ has user biases removed, this is equivalent to Pearson Correlation.\n",
                "\n",
                "### Subtask 6 & 7: Predict ratings and Handle Edge Cases\n",
                "\n",
                "We predict rating for user $u$ on item $i$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 3] Computing Item-Item Similarity matrix...\n",
                        "Transposed Matrix Shape: (1000, 9591)\n",
                        "Calculating Item Norms...\n",
                        "Computing Dot Product (Similarity)...\n",
                        "Similarity Matrix Computed in 0.0122s. Shape: (1000, 1000)\n"
                    ]
                }
            ],
            "source": [
                "def compute_item_similarity_matrix(R_centered):\n",
                "    \"\"\"\n",
                "    Computes Cosine Similarity between columns (Items).\n",
                "    Since data is centered, this is Pearson Correlation.\n",
                "    \"\"\"\n",
                "    print(\"\\n[Step 3] Computing Item-Item Similarity matrix...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Transpose so Items are Rows\n",
                "    M = R_centered.T.tocsr()\n",
                "    print(f\"Transposed Matrix Shape: {M.shape}\")\n",
                "    \n",
                "    # Compute Norms\n",
                "    print(\"Calculating Item Norms...\")\n",
                "    item_norms = np.sqrt(np.array(M.power(2).sum(axis=1)).flatten())\n",
                "    item_norms[item_norms == 0] = 1e-9\n",
                "    inv_norms = 1.0 / item_norms\n",
                "    \n",
                "    # Normalize\n",
                "    M_normalized = diags(inv_norms) @ M\n",
                "    \n",
                "    # Dot Product\n",
                "    print(\"Computing Dot Product (Similarity)...\")\n",
                "    sim_matrix = M_normalized.dot(M_normalized.T)\n",
                "    \n",
                "    # Set diagonal\n",
                "    sim_matrix.setdiag(0.0)\n",
                "    sim_matrix.eliminate_zeros()\n",
                "    \n",
                "    elapsed = time.time() - start_time\n",
                "    print(f\"Similarity Matrix Computed in {elapsed:.4f}s. Shape: {sim_matrix.shape}\")\n",
                "    return sim_matrix\n",
                "\n",
                "sim_matrix = compute_item_similarity_matrix(R_centered)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_item_based_cf(user_idx, item_idx, R, sim_matrix, user_means, k=20):\n",
                "    \"\"\"\n",
                "    Predicts rating: mu_u + [Sum(sim * (r - mu_u)) / Sum(|sim|)]\n",
                "    \"\"\"\n",
                "    # This function is called many times, so we minimize print here to avoid flooding\n",
                "    sim_row = sim_matrix.getrow(item_idx)\n",
                "    user_row = R.getrow(user_idx)\n",
                "    \n",
                "    rated_indices = user_row.indices\n",
                "    rated_values = user_row.data\n",
                "    \n",
                "    if len(rated_indices) == 0:\n",
                "         return user_means[user_idx]\n",
                "    \n",
                "    rating_map = {idx: val for idx, val in zip(rated_indices, rated_values)}\n",
                "    neighbor_indices = sim_row.indices\n",
                "    neighbor_scores = sim_row.data\n",
                "    \n",
                "    candidates = []\n",
                "    for j, score in zip(neighbor_indices, neighbor_scores):\n",
                "        if j in rating_map:\n",
                "            candidates.append((score, rating_map[j]))\n",
                "            \n",
                "    candidates.sort(key=lambda x: abs(x[0]), reverse=True)\n",
                "    top_k = candidates[:k]\n",
                "    \n",
                "    if not top_k:\n",
                "        return user_means[user_idx]\n",
                "        \n",
                "    weighted_sum = 0.0\n",
                "    sum_abs_sim = 0.0\n",
                "    mu_u = user_means[user_idx]\n",
                "    \n",
                "    for score, r_val in top_k:\n",
                "        dev = r_val - mu_u\n",
                "        weighted_sum += score * dev\n",
                "        sum_abs_sim += abs(score)\n",
                "        \n",
                "    if sum_abs_sim == 0:\n",
                "        return mu_u\n",
                "        \n",
                "    pred = mu_u + (weighted_sum / sum_abs_sim)\n",
                "    return max(1.0, min(5.0, pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation (Item-Based CF)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 4] Starting Evaluation (Train/Test Split)...\n",
                        "Train Samples: 66684\n",
                        "Test Samples: 16671\n",
                        "Rebuilding matrices on TRAIN data to avoid leakage...\n",
                        "\n",
                        "[Step 2a] Building User-Item Matrix...\n",
                        "Found 9450 unique users and 1000 unique items.\n",
                        "Matrix Built in 0.0179s. Shape: (9450, 1000) with 64165 ratings.\n",
                        "Sparsity: 99.321005%\n",
                        "\n",
                        "[Step 2b] Mean-Centering Ratings (Bias Removal)...\n",
                        "User Means Computed. Example means: [3.5  4.75 4.6  3.72 4.6 ]\n",
                        "Mean Centering complete in 0.0005s.\n",
                        "\n",
                        "[Step 3] Computing Item-Item Similarity matrix...\n",
                        "Transposed Matrix Shape: (1000, 9450)\n",
                        "Calculating Item Norms...\n",
                        "Computing Dot Product (Similarity)...\n",
                        "Similarity Matrix Computed in 0.0088s. Shape: (1000, 1000)\n",
                        "Matrices rebuilt in 0.0277s.\n",
                        "Evaluating predictions on 16671 test cases...\n",
                        "  Processed 500/16671...\n",
                        "  Processed 1000/16671...\n",
                        "  Processed 1500/16671...\n",
                        "  Processed 2000/16671...\n",
                        "  Processed 2500/16671...\n",
                        "  Processed 3000/16671...\n",
                        "  Processed 3500/16671...\n",
                        "  Processed 4000/16671...\n",
                        "  Processed 4500/16671...\n",
                        "  Processed 5000/16671...\n",
                        "  Processed 5500/16671...\n",
                        "  Processed 6000/16671...\n",
                        "  Processed 6500/16671...\n",
                        "  Processed 7000/16671...\n",
                        "  Processed 7500/16671...\n",
                        "  Processed 8000/16671...\n",
                        "  Processed 8500/16671...\n",
                        "  Processed 9000/16671...\n",
                        "  Processed 9500/16671...\n",
                        "  Processed 10000/16671...\n",
                        "  Processed 10500/16671...\n",
                        "  Processed 11000/16671...\n",
                        "  Processed 11500/16671...\n",
                        "  Processed 12000/16671...\n",
                        "  Processed 12500/16671...\n",
                        "  Processed 13000/16671...\n",
                        "  Processed 13500/16671...\n",
                        "  Processed 14000/16671...\n",
                        "  Processed 14500/16671...\n",
                        "  Processed 15000/16671...\n",
                        "  Processed 15500/16671...\n",
                        "  Processed 16000/16671...\n",
                        "  Processed 16500/16671...\n",
                        "Evaluation done in 2.5898s.\n",
                        "\n",
                        ">>> Item-Based CF (Centered Cosine) RMSE: 1.1146 <<<\n",
                        "\n",
                        "[Step 4.1] Generating Item-Based CF Recommendations for Top 5 Users...\n",
                        "Saved CF Recommendations to ../results/cf_item_based_recommendations.csv\n",
                        "                           User  Rank  Item_ID  \\\n",
                        "0  AG73BVBKUOH22USSFJA5ZWL7AKXA     1     2063   \n",
                        "1  AG73BVBKUOH22USSFJA5ZWL7AKXA     2    19220   \n",
                        "2  AG73BVBKUOH22USSFJA5ZWL7AKXA     3    24943   \n",
                        "3  AG73BVBKUOH22USSFJA5ZWL7AKXA     4    28053   \n",
                        "4  AG73BVBKUOH22USSFJA5ZWL7AKXA     5    29194   \n",
                        "\n",
                        "                                               Title  Predicted_Rating  \n",
                        "0  1 X 10 Energizer 377 376 Watch Battery SR626SW...               5.0  \n",
                        "1  3M Micropore Paper Tape - White, 1\" x 10yds (B...               5.0  \n",
                        "2  6 Pack Reading Glasses by BOOST EYEWEAR, Tradi...               5.0  \n",
                        "3  9 Elements Bathroom Cleaner, Eucalyptus Multi ...               5.0  \n",
                        "4  ACDelco 24-Count AA Batteries, Maximum Power S...               5.0  \n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n[Step 4] Starting Evaluation (Train/Test Split)...\")\n",
                "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
                "print(f\"Train Samples: {len(train_df)}\")\n",
                "print(f\"Test Samples: {len(test_df)}\")\n",
                "\n",
                "print(\"Rebuilding matrices on TRAIN data to avoid leakage...\")\n",
                "start_time = time.time()\n",
                "R_train, user_map_train, item_map_train = build_user_item_matrix(train_df)\n",
                "R_train_centered, user_means_train = center_ratings_by_user(R_train)\n",
                "sim_matrix_train = compute_item_similarity_matrix(R_train_centered)\n",
                "elapsed = time.time() - start_time\n",
                "print(f\"Matrices rebuilt in {elapsed:.4f}s.\")\n",
                "\n",
                "def evaluate_cf(test_df, R, sim, user_means, user_map, item_map):\n",
                "    errors = []\n",
                "    print(f\"Evaluating predictions on {len(test_df)} test cases...\")\n",
                "    start_eval = time.time()\n",
                "    count = 0\n",
                "    \n",
                "    for _, row in test_df.iterrows():\n",
                "        uid = row['user_id']\n",
                "        iid = row['item_id_encoded']\n",
                "        true_r = row['rating']\n",
                "        \n",
                "        if uid in user_map and iid in item_map:\n",
                "            u_idx = user_map[uid]\n",
                "            i_idx = item_map[iid]\n",
                "            pred = predict_item_based_cf(u_idx, i_idx, R, sim, user_means, k=20)\n",
                "            errors.append((true_r - pred) ** 2)\n",
                "        else:\n",
                "            # Cold start\n",
                "            errors.append((true_r - 4.4) ** 2)\n",
                "        \n",
                "        count += 1\n",
                "        if count % 500 == 0:\n",
                "             print(f\"  Processed {count}/{len(test_df)}...\")\n",
                "            \n",
                "    rmse = np.sqrt(np.mean(errors))\n",
                "    print(f\"Evaluation done in {time.time() - start_eval:.4f}s.\")\n",
                "    return rmse\n",
                "\n",
                "rmse_cf = evaluate_cf(test_df, R_train, sim_matrix_train, user_means_train, user_map_train, item_map_train)\n",
                "print(f\"\\n>>> Item-Based CF (Centered Cosine) RMSE: {rmse_cf:.4f} <<<\")\n",
                "\n",
                "# --- [Added] Generate Top-10 CF Recommendations for Top 5 Active Users ---\n",
                "print(\"\\n[Step 4.1] Generating Item-Based CF Recommendations for Top 5 Users...\")\n",
                "# Identify Top 5 active users\n",
                "top_5_users = df['user_id'].value_counts().head(5).index.tolist()\n",
                "\n",
                "cf_recs = []\n",
                "\n",
                "for uid in top_5_users:\n",
                "    if uid in user_map:\n",
                "        u_idx = user_map[uid]\n",
                "        # Get rated items to exclude\n",
                "        rated_indices = R.getrow(u_idx).indices\n",
                "        rated_set = set(rated_indices)\n",
                "        \n",
                "        # Predict for all items\n",
                "        user_predictions = []\n",
                "        for i_idx in range(R.shape[1]):\n",
                "            if i_idx not in rated_set:\n",
                "                 # Use existing function using sim_matrix\n",
                "                 score = predict_item_based_cf(u_idx, i_idx, R, sim_matrix, user_means, k=20)\n",
                "                 user_predictions.append((score, i_idx))\n",
                "        \n",
                "        # Sort and pick Top 10\n",
                "        user_predictions.sort(key=lambda x: x[0], reverse=True)\n",
                "        top_10 = user_predictions[:10]\n",
                "        \n",
                "        for rank, (score, i_idx) in enumerate(top_10, 1):\n",
                "            item_code = inv_item_map.get(i_idx, \"Unknown\")\n",
                "            title = item_title_map.get(item_code, \"Unknown Title\")\n",
                "            \n",
                "            cf_recs.append({\n",
                "                'User': uid,\n",
                "                'Rank': rank,\n",
                "                'Item_ID': item_code,\n",
                "                'Title': title,\n",
                "                'Predicted_Rating': round(score, 4)\n",
                "            })\n",
                "\n",
                "df_cf_recs = pd.DataFrame(cf_recs)\n",
                "save_path_cf = \"../results/cf_item_based_recommendations.csv\"\n",
                "df_cf_recs.to_csv(save_path_cf, index=False)\n",
                "print(f\"Saved CF Recommendations to {save_path_cf}\")\n",
                "print(df_cf_recs.head())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Latent Factor Model (SVD)\n",
                "\n",
                "## 8.2. Dimensionality Reduction via SVD\n",
                "\n",
                "We employ **Truncated SVD** to decompose the centered rating matrix into latent factors, effectively reducing noise and sparsity.\n",
                "$$ R_{centered} \\approx U \\cdot \\Sigma \\cdot V^T $$\n",
                "The reconstructed rating is estimated as: $\\hat{r}_{ui} = (U_u \\cdot \\Sigma \\cdot V_i^T) + \\mu_u$\n",
                "\n",
                "### Scalable Prediction Strategy (Memory Efficiency)\n",
                "\n",
                "**Optimization**: Reconstructing the full dense matrix for prediction requires significant memory (proportional to $Users \\times Items$), which often leads to overflows.\n",
                "\n",
                "**Solution**: We implement a **Row-by-Row Prediction** pattern. We compute the dense prediction vector for one target user at a time, extract the top items, and then discard the vector. This keeps memory usage low and constant regardless of the total number of users."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "k     RMSE (Reconstruction)    \n",
                        "------------------------------\n",
                        "\n",
                        "[Step 5] Calculating SVD with k=10...\n",
                        "SVD Done in 0.0318s.\n",
                        "U: (9591, 10), Sigma: (10, 10), Vt: (10, 1000)\n",
                        "Calculating Reconstruction Error...\n",
                        "10    0.1155                   \n",
                        "\n",
                        "[Step 5] Calculating SVD with k=20...\n",
                        "SVD Done in 0.0407s.\n",
                        "U: (9591, 20), Sigma: (20, 20), Vt: (20, 1000)\n",
                        "Calculating Reconstruction Error...\n",
                        "20    0.1107                   \n"
                    ]
                }
            ],
            "source": [
                "def perform_svd(matrix, k):\n",
                "    print(f\"\\n[Step 5] Calculating SVD with k={k}...\")\n",
                "    start_time = time.time()\n",
                "    # Helper to convert to float for SVD\n",
                "    matrix_f = matrix.asfptype()\n",
                "    \n",
                "    U, sigma, Vt = svds(matrix_f, k=k)\n",
                "    \n",
                "    # Sort descending\n",
                "    U = U[:, ::-1]\n",
                "    sigma = sigma[::-1]\n",
                "    Vt = Vt[::-1, :]\n",
                "    \n",
                "    sigma_diag = np.diag(sigma)\n",
                "    print(f\"SVD Done in {time.time() - start_time:.4f}s.\")\n",
                "    print(f\"U: {U.shape}, Sigma: {sigma_diag.shape}, Vt: {Vt.shape}\")\n",
                "    return U, sigma_diag, Vt\n",
                "\n",
                "def calculate_rmse_reconstruction(original_centered, U, sigma_diag, Vt):\n",
                "    # Approximation\n",
                "    print(\"Calculating Reconstruction Error...\")\n",
                "    approx = np.dot(np.dot(U, sigma_diag), Vt)\n",
                "    \n",
                "    diff = original_centered - approx\n",
                "    rmse = np.sqrt(np.mean(np.square(diff)))\n",
                "    return rmse\n",
                "\n",
                "# Evaluate both k=10 and k=20\n",
                "print(\"\\n{:<5} {:<25}\".format(\"k\", \"RMSE (Reconstruction)\"))\n",
                "print(\"-\"*30)\n",
                "\n",
                "for k in [10, 20]:\n",
                "    U, S, Vt = perform_svd(R_centered, k)\n",
                "    rmse_svd = calculate_rmse_reconstruction(R_centered, U, S, Vt)\n",
                "    print(\"{:<5} {:<25.4f}\".format(k, rmse_svd))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[Step 6] Generating efficient SVD Recommendations (k=20)...\n",
                        "\n",
                        "[Step 5] Calculating SVD with k=20...\n",
                        "SVD Done in 0.0452s.\n",
                        "U: (9591, 20), Sigma: (20, 20), Vt: (20, 1000)\n",
                        "Generating SVD recommendations for 5 users...\n",
                        "Saved SVD Recommendations to ../results/svd_recommendations.csv\n",
                        "                           User  Rank  Item_ID  \\\n",
                        "0  AG73BVBKUOH22USSFJA5ZWL7AKXA     1   130688   \n",
                        "1  AG73BVBKUOH22USSFJA5ZWL7AKXA     2   152134   \n",
                        "2  AG73BVBKUOH22USSFJA5ZWL7AKXA     3   159364   \n",
                        "3  AG73BVBKUOH22USSFJA5ZWL7AKXA     4   163117   \n",
                        "4  AG73BVBKUOH22USSFJA5ZWL7AKXA     5   172985   \n",
                        "\n",
                        "                                               Title  Predicted_Rating  \\\n",
                        "0  Energizer CR2032 Batteries, 3V Lithium Coin Ce...               5.0   \n",
                        "1         GLOVEWORKS Industrial Black Nitrile Gloves               5.0   \n",
                        "2  Glade Solid Air Freshener, Deodorizer for Home...               5.0   \n",
                        "3  Green Gobbler POWDER PLUNGER Toilet Bowl Clog ...               5.0   \n",
                        "4  Hefty Slider Jumbo Storage Bags, 2.5 Gallon Si...               5.0   \n",
                        "\n",
                        "       Method  \n",
                        "0  SVD (k=20)  \n",
                        "1  SVD (k=20)  \n",
                        "2  SVD (k=20)  \n",
                        "3  SVD (k=20)  \n",
                        "4  SVD (k=20)  \n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n[Step 6] Generating efficient SVD Recommendations (k=20)...\")\n",
                "\n",
                "# Efficient SVD Prediction Function (Row-by-Row)\n",
                "def predict_for_user_svd(u_idx, U, S, Vt, user_mean):\n",
                "    \"\"\"\n",
                "    Computes dense prediction vector for a single user to save memory.\n",
                "    Result shape: (n_items, )\n",
                "    P = (User_Latent . Sigma . Vt) + Mean\n",
                "    \"\"\"\n",
                "    # User latent vector: U[u_idx] -> shape (1, k)\n",
                "    user_vec = U[u_idx, :].reshape(1, -1)\n",
                "    \n",
                "    # w = User_Latent . S\n",
                "    w = np.dot(user_vec, S)\n",
                "    \n",
                "    # Preds = w . Vt\n",
                "    preds = np.dot(w, Vt).flatten() + user_mean\n",
                "    return np.clip(preds, 1.0, 5.0)\n",
                "\n",
                "# Use the best model k=20\n",
                "k_best = 20\n",
                "U, S, Vt = perform_svd(R_centered, k_best)\n",
                "\n",
                "svd_recs = []\n",
                "\n",
                "# Reuse Top 5 users\n",
                "top_5_users = df['user_id'].value_counts().head(5).index.tolist()\n",
                "\n",
                "print(f\"Generating SVD recommendations for {len(top_5_users)} users...\")\n",
                "\n",
                "for uid in top_5_users:\n",
                "    if uid in user_map:\n",
                "        u_idx = user_map[uid]\n",
                "        user_mean = user_means[u_idx]\n",
                "        \n",
                "        # Predict all items for this user efficiently\n",
                "        preds = predict_for_user_svd(u_idx, U, S, Vt, user_mean)\n",
                "        \n",
                "        # Accessing R sparse row again to filter out already rated items\n",
                "        rated_indices = set(R.getrow(u_idx).indices)\n",
                "        \n",
                "        # Create list of (score, idx) for unrated items only\n",
                "        candidates = []\n",
                "        for i_idx in range(len(preds)):\n",
                "            if i_idx not in rated_indices:\n",
                "                candidates.append((preds[i_idx], i_idx))\n",
                "        \n",
                "        # Sort by score descending\n",
                "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
                "        top_20 = candidates[:20]\n",
                "        \n",
                "        for rank, (score, i_idx) in enumerate(top_20, 1):\n",
                "            item_code = inv_item_map.get(i_idx, \"Unknown\")\n",
                "            title = item_title_map.get(item_code, \"Unknown Title\")\n",
                "            \n",
                "            svd_recs.append({\n",
                "                'User': uid,\n",
                "                'Rank': rank,\n",
                "                'Item_ID': item_code,\n",
                "                'Title': title,\n",
                "                'Predicted_Rating': round(score, 4),\n",
                "                'Method': f'SVD (k={k_best})'\n",
                "            })\n",
                "\n",
                "df_svd_recs = pd.DataFrame(svd_recs)\n",
                "save_path_svd = \"../results/svd_recommendations.csv\"\n",
                "df_svd_recs.to_csv(save_path_svd, index=False)\n",
                "print(f\"Saved SVD Recommendations to {save_path_svd}\")\n",
                "print(df_svd_recs.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
