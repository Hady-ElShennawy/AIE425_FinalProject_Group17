{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd470cd",
   "metadata": {},
   "source": [
    "# Group 17: \n",
    "### Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84d178",
   "metadata": {},
   "source": [
    "# Part 2: Content-Based Recommendation System\n",
    "\n",
    "This notebook implements a comprehensive content-based recommendation system as per the project requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2847a7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hadye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\hadye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\hadye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hadye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hadye\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hadye\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add87d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded. Shape: (14554, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hadye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item</th>\n",
       "      <th>item_id_encoded</th>\n",
       "      <th>rating</th>\n",
       "      <th>price</th>\n",
       "      <th>text</th>\n",
       "      <th>is_green</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEV4DP5E3FJH6FHDLXUYQTDEQYCQ</td>\n",
       "      <td>Sonic Handheld Percussion Massage Gun - Deep T...</td>\n",
       "      <td>827</td>\n",
       "      <td>2</td>\n",
       "      <td>79.99</td>\n",
       "      <td>This product worked great when it worked, but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
       "      <td>DUDE Wipes On-The-Go Flushable Wet Wipes - 1 P...</td>\n",
       "      <td>255</td>\n",
       "      <td>5</td>\n",
       "      <td>6.48</td>\n",
       "      <td>These are amazing for travel or for keeping in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
       "      <td>Sleep Mask for Side Sleeper, 100% Blackout 3D ...</td>\n",
       "      <td>814</td>\n",
       "      <td>5</td>\n",
       "      <td>12.69</td>\n",
       "      <td>These are great! My other sleep mask pressed o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
       "      <td>Cottonelle Freshfeel Flushable Wet Wipes, Adul...</td>\n",
       "      <td>226</td>\n",
       "      <td>5</td>\n",
       "      <td>15.79</td>\n",
       "      <td>These are really good quality. Do not tear lik...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
       "      <td>Silk Sleep Eye Mask for Men Women, Comfortable...</td>\n",
       "      <td>808</td>\n",
       "      <td>5</td>\n",
       "      <td>8.88</td>\n",
       "      <td>Great for travel. Super soft and silky. Has ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id  \\\n",
       "0  AEV4DP5E3FJH6FHDLXUYQTDEQYCQ   \n",
       "1  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
       "2  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
       "3  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
       "4  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
       "\n",
       "                                                item  item_id_encoded  rating  \\\n",
       "0  Sonic Handheld Percussion Massage Gun - Deep T...              827       2   \n",
       "1  DUDE Wipes On-The-Go Flushable Wet Wipes - 1 P...              255       5   \n",
       "2  Sleep Mask for Side Sleeper, 100% Blackout 3D ...              814       5   \n",
       "3  Cottonelle Freshfeel Flushable Wet Wipes, Adul...              226       5   \n",
       "4  Silk Sleep Eye Mask for Men Women, Comfortable...              808       5   \n",
       "\n",
       "   price                                               text  is_green  \n",
       "0  79.99  This product worked great when it worked, but ...         1  \n",
       "1   6.48  These are amazing for travel or for keeping in...         1  \n",
       "2  12.69  These are great! My other sleep mask pressed o...         0  \n",
       "3  15.79  These are really good quality. Do not tear lik...         1  \n",
       "4   8.88  Great for travel. Super soft and silky. Has ad...         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --- LOAD YOUR DATA HERE ---\n",
    "data_path = r'../data/'\n",
    "df = pd.read_csv(os.path.join(data_path, 'Amazon_health&household_label_encoded.csv'))\n",
    "\n",
    "print(\"Data Loaded. Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418b1230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaned. Generating Matrix...\n",
      "Item-Feature Matrix Ready: (1000, 102)\n"
     ]
    }
   ],
   "source": [
    "# 1. Separate Unique Items from Ratings\n",
    "# We need one row per item to build the feature matrix\n",
    "df_items = df[['item_id_encoded', 'item', 'price', 'text', 'is_green']].drop_duplicates(subset='item_id_encoded').sort_values('item_id_encoded').set_index('item_id_encoded')\n",
    "\n",
    "\n",
    "# --- FIX 1: Handle NaNs in Text ---\n",
    "# Fill missing text with empty string\n",
    "df_items['text'] = df_items['text'].fillna('')\n",
    "\n",
    "# --- FIX 2: Handle NaNs in Price ---\n",
    "# Fill missing prices with the Median price (better than 0)\n",
    "median_price = df_items['price'].median()\n",
    "df_items['price'] = df_items['price'].fillna(median_price)\n",
    "\n",
    "# --- FIX 3: Handle NaNs in Is_Green ---\n",
    "# Fill missing boolean with False (0)\n",
    "df_items['is_green'] = df_items['is_green'].fillna(False)\n",
    "\n",
    "print(\"Data Cleaned. Generating Matrix...\")\n",
    "\n",
    "\n",
    "# 2. Re-create the Item-Feature Matrix (from Phase 3)\n",
    "# A. Text (TF-IDF)\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=100) # Limited to 100 for speed on large data\n",
    "text_matrix = tfidf.fit_transform(df_items['text'].fillna('')).toarray()\n",
    "\n",
    "# B. Price (Normalized)\n",
    "scaler = MinMaxScaler()\n",
    "price_vec = scaler.fit_transform(df_items[['price']])\n",
    "\n",
    "# C. Is_Green (Binary)\n",
    "green_vec = df_items[['is_green']].astype(int).values\n",
    "\n",
    "# D. Combine into \"item_features\"\n",
    "item_features = np.hstack([text_matrix, price_vec, green_vec])\n",
    "\n",
    "print(f\"Item-Feature Matrix Ready: {item_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e428ee24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4. User Profile Construction ---\n",
      "   -> Cold-Start Strategy: Using Global Average Item Vector.\n",
      "   -> Built profiles for 10000 users.\n",
      "\n",
      "Sample Profile (User AEV4DP5E3FJH6FHDLXUYQTDEQYCQ):\n",
      "Vector Shape: (102,)\n",
      "First 5 Features: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 4. USER PROFILE CONSTRUCTION\n",
    "# ==========================================\n",
    "\n",
    "def build_user_profiles(df, item_features_matrix):\n",
    "    print(\"--- 4. User Profile Construction ---\")\n",
    "    \n",
    "    # 1. Get unique users\n",
    "    unique_users = df['user_id'].unique()\n",
    "    user_profiles = {}\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 4.2 STRATEGY: Handle Cold-Start Users (Popular/Average)\n",
    "    # ---------------------------------------------------------\n",
    "    # Since we lack demographics, we calculate the \"Global Average Item\"\n",
    "    # This represents the \"average taste\" of the entire catalog.\n",
    "    # Ideally, you could weigh this by popularity (rating count), \n",
    "    # but a simple mean is sufficient for this requirement.\n",
    "    cold_start_vector = np.mean(item_features_matrix, axis=0)\n",
    "    \n",
    "    print(f\"   -> Cold-Start Strategy: Using Global Average Item Vector.\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 4.1 STRATEGY: Build User Profiles (Weighted Average)\n",
    "    # ---------------------------------------------------------\n",
    "    for uid in unique_users:\n",
    "        # Get user history\n",
    "        user_history = df[df['user_id'] == uid]\n",
    "        \n",
    "        # If user has no ratings (or valid item_ids), treat as Cold Start\n",
    "        if user_history.empty:\n",
    "            user_profiles[uid] = cold_start_vector\n",
    "            continue\n",
    "            \n",
    "        # Get indices and ratings\n",
    "        # Ensure indices are integers for matrix lookup\n",
    "        item_indices = user_history['item_id_encoded'].values.astype(int)\n",
    "        ratings = user_history['rating'].values.reshape(-1, 1)\n",
    "        \n",
    "        # ERROR HANDLING: Check if any item_id is out of bounds\n",
    "        # (This happens if df contains items not in our feature matrix)\n",
    "        valid_mask = item_indices < item_features_matrix.shape[0]\n",
    "        item_indices = item_indices[valid_mask]\n",
    "        ratings = ratings[valid_mask]\n",
    "        \n",
    "        if len(item_indices) == 0:\n",
    "            user_profiles[uid] = cold_start_vector\n",
    "            continue\n",
    "\n",
    "        # Fetch vectors for items rated by this user\n",
    "        rated_item_vectors = item_features_matrix[item_indices]\n",
    "        \n",
    "        # CALCULATE WEIGHTED AVERAGE\n",
    "        # Formula: Sum(Item_Vector * Rating) / Sum(Ratings)\n",
    "        weighted_sum = np.sum(rated_item_vectors * ratings, axis=0)\n",
    "        total_rating_val = np.sum(ratings)\n",
    "        \n",
    "        if total_rating_val == 0:\n",
    "            user_profiles[uid] = cold_start_vector\n",
    "        else:\n",
    "            user_profiles[uid] = weighted_sum / total_rating_val\n",
    "            \n",
    "    print(f\"   -> Built profiles for {len(user_profiles)} users.\")\n",
    "    return user_profiles\n",
    "\n",
    "# --- EXECUTE PHASE 4 ---\n",
    "# Input: Your main dataframe (df) and the matrix from Phase 3 (item_features)\n",
    "user_profiles = build_user_profiles(df, item_features)\n",
    "\n",
    "# --- VERIFICATION ---\n",
    "# Let's look at the first user's profile\n",
    "sample_uid = list(user_profiles.keys())[0]\n",
    "print(f\"\\nSample Profile (User {sample_uid}):\")\n",
    "print(f\"Vector Shape: {user_profiles[sample_uid].shape}\")\n",
    "print(f\"First 5 Features: {user_profiles[sample_uid][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b84011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Recommendations for User AEV4DP5E3FJH6FHDLXUYQTDEQYCQ ---\n",
      "Item ID    | Score    | Item Name (Lookup)\n",
      "--------------------------------------------------\n",
      "428        | 0.8180   | Green Gobbler Septic Saver Bac..\n",
      "40         | 0.8137   | Affresh Washing Machine Cleane..\n",
      "425        | 0.7759   | Green Gobbler Liquid Hair Drai..\n",
      "420        | 0.7122   | Grandma's Secret Wrinkle Remov..\n",
      "692        | 0.7101   | Philips Sonicare Genuine E-Ser..\n",
      "645        | 0.7074   | Oral-B Dual Clean Replacement ..\n",
      "803        | 0.7061   | Seventh Generation Dish Soap L..\n",
      "388        | 0.7055   | Fractionated Coconut Oil Premi..\n",
      "855        | 0.7053   | Swedish Wholesale Swedish Dish..\n",
      "185        | 0.7051   | Cascade Complete Dishwasher Po..\n",
      "\n",
      "(Generated Top-20 list. Count: 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 5. SIMILARITY & RECOMMENDATION\n",
    "# ==========================================\n",
    "\n",
    "def get_recommendations(user_id, user_profiles, item_features_matrix, df_full, top_n=10):\n",
    "    \"\"\"\n",
    "    Generates content-based recommendations for a specific user.\n",
    "    \"\"\"\n",
    "    # 1. Get the User's Profile Vector\n",
    "    if user_id not in user_profiles:\n",
    "        print(f\"User {user_id} not found in profiles.\")\n",
    "        return []\n",
    "    \n",
    "    # Reshape is needed because cosine_similarity expects a 2D array (1 sample, n features)\n",
    "    user_vector = user_profiles[user_id].reshape(1, -1)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 5.1 COMPUTE SIMILARITY\n",
    "    # ---------------------------------------------------------\n",
    "    # Calculate similarity between this User Vector and ALL Item Vectors\n",
    "    # Result shape: (1, num_items) -> flatten to 1D array\n",
    "    similarity_scores = cosine_similarity(user_vector, item_features_matrix).flatten()\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 5.2 GENERATE TOP-N RECOMMENDATIONS\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    # Get the list of items the user has ALREADY rated\n",
    "    # We don't want to recommend things they already know about\n",
    "    rated_items = df_full[df_full['user_id'] == user_id]['item_id_encoded'].values\n",
    "    \n",
    "    # Create a list of tuples: (item_id_encoded, similarity_score)\n",
    "    # We enumerate to get the index (which corresponds to item_id_encoded)\n",
    "    all_scores = list(enumerate(similarity_scores))\n",
    "    \n",
    "    # Filter out already rated items\n",
    "    candidates = [\n",
    "        (item_id, score) \n",
    "        for item_id, score in all_scores \n",
    "        if item_id not in rated_items\n",
    "    ]\n",
    "    \n",
    "    # Sort by Score (Descending) -> Highest similarity first\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Slice the top N\n",
    "    top_recommendations = candidates[:top_n]\n",
    "    \n",
    "    return top_recommendations\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION EXAMPLE\n",
    "# ==========================================\n",
    "\n",
    "# Select a sample user to test\n",
    "sample_user = df['user_id'].iloc[0]\n",
    "\n",
    "# --- Get Top 10 Recommendations ---\n",
    "recs_10 = get_recommendations(sample_user, user_profiles, item_features, df, top_n=10)\n",
    "\n",
    "print(f\"\\n--- Top 10 Recommendations for User {sample_user} ---\")\n",
    "print(f\"{'Item ID':<10} | {'Score':<8} | {'Item Name (Lookup)'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# We need a helper to look up item names from IDs\n",
    "item_lookup = df[['item_id_encoded', 'item']].drop_duplicates().set_index('item_id_encoded')\n",
    "\n",
    "for item_id, score in recs_10:\n",
    "    # Handle case where item_id might be out of lookup range (sanity check)\n",
    "    try:\n",
    "        item_name = item_lookup.loc[item_id, 'item']\n",
    "        # Truncate long names for cleaner print\n",
    "        item_name = (item_name[:30] + '..') if len(item_name) > 30 else item_name\n",
    "    except KeyError:\n",
    "        item_name = \"Unknown Item\"\n",
    "        \n",
    "    print(f\"{item_id:<10} | {score:.4f}   | {item_name}\")\n",
    "\n",
    "\n",
    "# --- Get Top 20 Recommendations ---\n",
    "recs_20 = get_recommendations(sample_user, user_profiles, item_features, df, top_n=20)\n",
    "print(f\"\\n(Generated Top-20 list. Count: {len(recs_20)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54928ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. k-Nearest Neighbors (Item-Based) ---\n",
      "Test User: AEV4DP5E3FJH6FHDLXUYQTDEQYCQ\n",
      "Target Item ID: 0\n",
      "Predicted Rating (k=10): 2.0000\n",
      "Predicted Rating (k=20): 2.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 6. k-NEAREST NEIGHBORS (Item-Based)\n",
    "# ==========================================\n",
    "print(\"\\n--- 6. k-Nearest Neighbors (Item-Based) ---\")\n",
    "\n",
    "# A. FIT THE MODEL\n",
    "# We fit the model on the ITEM FEATURES matrix (from Phase 3)\n",
    "# metric='cosine' is crucial because we care about the angle (similarity), not distance\n",
    "knn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "knn_model.fit(item_features)\n",
    "\n",
    "def predict_rating_knn(user_id, target_item_id, df_full, knn_model, item_features, k=10):\n",
    "    \"\"\"\n",
    "    Predicts the rating a user would give to 'target_item_id' \n",
    "    based on their ratings of the k-nearest similar items.\n",
    "    \"\"\"\n",
    "    # 1. Find k nearest neighbors for the target item\n",
    "    # Input must be 2D array (1, n_features)\n",
    "    target_vec = item_features[target_item_id].reshape(1, -1)\n",
    "    \n",
    "    # We ask for k+1 neighbors because the closest neighbor is ALWAYS the item itself (distance=0)\n",
    "    # We will skip the first one later.\n",
    "    distances, indices = knn_model.kneighbors(target_vec, n_neighbors=k+1)\n",
    "    \n",
    "    neighbor_ids = indices.flatten()\n",
    "    neighbor_dists = distances.flatten()\n",
    "    \n",
    "    # 2. Get the user's rating history\n",
    "    user_history = df_full[df_full['user_id'] == user_id]\n",
    "    \n",
    "    weighted_sum = 0\n",
    "    similarity_sum = 0\n",
    "    count_matches = 0\n",
    "    \n",
    "    # 3. Loop through neighbors to calculate Weighted Average\n",
    "    # Start at index 1 to skip the target item itself\n",
    "    for i in range(1, len(neighbor_ids)):\n",
    "        n_id = neighbor_ids[i]\n",
    "        n_dist = neighbor_dists[i]\n",
    "        \n",
    "        # Convert distance to similarity (Cosine Similarity = 1 - Cosine Distance)\n",
    "        similarity = 1 - n_dist\n",
    "        \n",
    "        # Check if the user has actually rated this neighbor\n",
    "        if n_id in user_history['item_id_encoded'].values:\n",
    "            # Get the actual rating (1-5)\n",
    "            actual_rating = user_history[user_history['item_id_encoded'] == n_id]['rating'].values[0]\n",
    "            \n",
    "            # Accumulate Weighted Sum\n",
    "            weighted_sum += (similarity * actual_rating)\n",
    "            similarity_sum += similarity\n",
    "            count_matches += 1\n",
    "            \n",
    "    # 4. Final Prediction Logic\n",
    "    if count_matches == 0:\n",
    "        # Fallback: If user hasn't rated ANY similar items, return user's average rating\n",
    "        if not user_history.empty:\n",
    "            return user_history['rating'].mean()\n",
    "        return df_full['rating'].mean() # Global average fallback\n",
    "    \n",
    "    if similarity_sum == 0:\n",
    "        return 0\n",
    "\n",
    "    predicted_rating = weighted_sum / similarity_sum\n",
    "    return predicted_rating\n",
    "\n",
    "# --- EXECUTION: Test with k=10 and k=20 ---\n",
    "\n",
    "# Setup: Pick a user and an item they have NOT rated yet\n",
    "sample_user = df['user_id'].iloc[0]\n",
    "all_items = set(df['item_id_encoded'].unique())\n",
    "user_rated_items = set(df[df['user_id'] == sample_user]['item_id_encoded'])\n",
    "\n",
    "# Find a candidate item (just pick the first available one)\n",
    "candidate_item = list(all_items - user_rated_items)[0]\n",
    "\n",
    "print(f\"Test User: {sample_user}\")\n",
    "print(f\"Target Item ID: {candidate_item}\")\n",
    "\n",
    "# Test k=10\n",
    "pred_10 = predict_rating_knn(sample_user, candidate_item, df, knn_model, item_features, k=10)\n",
    "print(f\"Predicted Rating (k=10): {pred_10:.4f}\")\n",
    "\n",
    "# Test k=20\n",
    "pred_20 = predict_rating_knn(sample_user, candidate_item, df, knn_model, item_features, k=20)\n",
    "print(f\"Predicted Rating (k=20): {pred_20:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "339ba425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "       PART 7: COMPLETE NUMERICAL EXAMPLE\n",
      "============================================================\n",
      "\n",
      "[STEP 1] Sample Data Representation\n",
      "We select 5 items. The User has rated Item A and Item B.\n",
      "  item_id                text  price  is_green\n",
      "0       A    green eco cotton     20         1\n",
      "1       B    green eco bamboo     25         1\n",
      "2       C   red plastic cheap     10         0\n",
      "3       D    blue denim jeans     50         0\n",
      "4       E  green cotton shirt     22         1\n",
      "\n",
      "User Ratings: {'A': 5.0, 'B': 4.0}\n",
      "\n",
      "------------------------------\n",
      "[STEP 2] TF-IDF Calculation\n",
      "------------------------------\n",
      "Vocabulary (11 terms): ['bamboo', 'blue', 'cheap', 'cotton', 'denim', 'eco', 'green', 'jeans', 'plastic', 'red', 'shirt']\n",
      "\n",
      "TF-IDF Vectors (Sample):\n",
      "Item A: [0.00, 0.00, 0.00, 0.61, 0.00, 0.61, 0.51, 0.00, 0.00, 0.00, 0.00]\n",
      "Item B: [0.69, 0.00, 0.00, 0.00, 0.00, 0.56, 0.46, 0.00, 0.00, 0.00, 0.00]\n",
      "Item C: [0.00, 0.00, 0.58, 0.00, 0.00, 0.00, 0.00, 0.00, 0.58, 0.58, 0.00]\n",
      "\n",
      "------------------------------\n",
      "[STEP 3] Full Item-Feature Matrix\n",
      "------------------------------\n",
      "Combining: [TF-IDF Vectors] + [Norm_Price] + [Is_Green]\n",
      "Item A Full Vector (Size 13):\n",
      "[0.   0.   0.   0.61 0.   0.61 0.51 0.   0.   0.   0.   0.25 1.  ]\n",
      "\n",
      "------------------------------\n",
      "[STEP 4] User Profile Construction (Weighted Average)\n",
      "------------------------------\n",
      "Math: (Vec_A * 5.0 + Vec_B * 4.0) / 9.0\n",
      "User Profile Vector:\n",
      "[0.31 0.   0.   0.34 0.   0.59 0.49 0.   0.   0.   0.   0.31 1.  ]\n",
      "\n",
      "------------------------------\n",
      "[STEP 5] Similarity & Top Recommendations\n",
      "------------------------------\n",
      "\n",
      "Calculation for Item E ('green cotton shirt'):\n",
      "   Dot Product: 1.5052\n",
      "   Norm(User): 1.3721, Norm(Item): 1.4457\n",
      "   Score: 0.7588\n",
      "\n",
      "--- Final Recommendations ---\n",
      "Rank | Item | Score  | Description\n",
      "  1  |  E   | 0.7588 | green cotton shirt\n",
      "  2  |  D   | 0.1575 | blue denim jeans\n",
      "  3  |  C   | 0.0000 | red plastic cheap\n",
      "\n",
      "(Note: Item E is recommended #1 because it shares 'green' and 'cotton' with the User Profile.)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==========================================\n",
    "# 7.1 STEP-BY-STEP NUMERICAL EXAMPLE\n",
    "# ==========================================\n",
    "print(\"=\"*60)\n",
    "print(\"       PART 7: COMPLETE NUMERICAL EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- STEP 1: SAMPLE DATA ---\n",
    "print(\"\\n[STEP 1] Sample Data Representation\")\n",
    "print(\"We select 5 items. The User has rated Item A and Item B.\")\n",
    "\n",
    "data = {\n",
    "    'item_id': ['A', 'B', 'C', 'D', 'E'],\n",
    "    'text': [\n",
    "        'green eco cotton',   # Item A (Rated 5.0)\n",
    "        'green eco bamboo',   # Item B (Rated 4.0)\n",
    "        'red plastic cheap',  # Item C (Unrated - Dissimilar)\n",
    "        'blue denim jeans',   # Item D (Unrated - Dissimilar)\n",
    "        'green cotton shirt'  # Item E (Unrated - Target Recommendation)\n",
    "    ],\n",
    "    'price': [20, 25, 10, 50, 22],\n",
    "    'is_green': [1, 1, 0, 0, 1]\n",
    "}\n",
    "\n",
    "# User Ratings: Likes \"Green/Eco\" items\n",
    "ratings = {'A': 5.0, 'B': 4.0} \n",
    "\n",
    "df_sample = pd.DataFrame(data)\n",
    "print(df_sample)\n",
    "print(f\"\\nUser Ratings: {ratings}\")\n",
    "\n",
    "\n",
    "# --- STEP 2: TF-IDF CALCULATION ---\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"[STEP 2] TF-IDF Calculation\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# We use a simple tokenizer to keep vocabulary small\n",
    "tfidf = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "tfidf_matrix = tfidf.fit_transform(df_sample['text']).toarray()\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"Vocabulary ({len(vocab)} terms): {list(vocab)}\")\n",
    "\n",
    "# Show vectors for the first 3 items\n",
    "print(\"\\nTF-IDF Vectors (Sample):\")\n",
    "for i in range(3):\n",
    "    # Formatting to 2 decimal places\n",
    "    vec_str = \", \".join([f\"{x:.2f}\" for x in tfidf_matrix[i]])\n",
    "    print(f\"Item {df_sample.loc[i, 'item_id']}: [{vec_str}]\")\n",
    "\n",
    "\n",
    "# --- STEP 3: FULL FEATURE MATRIX ---\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"[STEP 3] Full Item-Feature Matrix\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Combining: [TF-IDF Vectors] + [Norm_Price] + [Is_Green]\")\n",
    "\n",
    "# Normalize Price (0-1)\n",
    "scaler = MinMaxScaler()\n",
    "price_norm = scaler.fit_transform(df_sample[['price']])\n",
    "\n",
    "# Is_Green (already 0/1)\n",
    "green_vec = df_sample[['is_green']].values\n",
    "\n",
    "# Combine\n",
    "item_features = np.hstack([tfidf_matrix, price_norm, green_vec])\n",
    "\n",
    "# Print Item A's full vector as an example\n",
    "vec_A = item_features[0]\n",
    "print(f\"Item A Full Vector (Size {len(vec_A)}):\")\n",
    "print(np.round(vec_A, 2))\n",
    "\n",
    "\n",
    "# --- STEP 4: USER PROFILE CONSTRUCTION ---\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"[STEP 4] User Profile Construction (Weighted Average)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Get vectors for rated items A (index 0) and B (index 1)\n",
    "vec_A = item_features[0]\n",
    "vec_B = item_features[1]\n",
    "rating_A = 5.0\n",
    "rating_B = 4.0\n",
    "\n",
    "# Formula: (VecA * 5 + VecB * 4) / (5 + 4)\n",
    "numerator = (vec_A * rating_A) + (vec_B * rating_B)\n",
    "denominator = rating_A + rating_B\n",
    "user_profile = numerator / denominator\n",
    "\n",
    "print(f\"Math: (Vec_A * {rating_A} + Vec_B * {rating_B}) / {denominator}\")\n",
    "print(f\"User Profile Vector:\\n{np.round(user_profile, 2)}\")\n",
    "\n",
    "\n",
    "# --- STEP 5: SIMILARITY & RECOMMENDATION ---\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"[STEP 5] Similarity & Top Recommendations\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "rec_scores = []\n",
    "\n",
    "# Calculate Cosine Similarity for unrated items (C, D, E)\n",
    "# Indices: C=2, D=3, E=4\n",
    "for i in [2, 3, 4]:\n",
    "    item_id = df_sample.loc[i, 'item_id']\n",
    "    vec_item = item_features[i]\n",
    "    \n",
    "    # Cosine Similarity Formula: dot(A, B) / (norm(A) * norm(B))\n",
    "    dot_product = np.dot(user_profile, vec_item)\n",
    "    norm_u = np.linalg.norm(user_profile)\n",
    "    norm_i = np.linalg.norm(vec_item)\n",
    "    \n",
    "    score = dot_product / (norm_u * norm_i)\n",
    "    rec_scores.append((item_id, score))\n",
    "    \n",
    "    # Print calculation for Item E (The expected winner)\n",
    "    if item_id == 'E':\n",
    "        print(f\"\\nCalculation for Item E ('green cotton shirt'):\")\n",
    "        print(f\"   Dot Product: {dot_product:.4f}\")\n",
    "        print(f\"   Norm(User): {norm_u:.4f}, Norm(Item): {norm_i:.4f}\")\n",
    "        print(f\"   Score: {score:.4f}\")\n",
    "\n",
    "# Sort and Recommend\n",
    "rec_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n--- Final Recommendations ---\")\n",
    "print(\"Rank | Item | Score  | Description\")\n",
    "for rank, (iid, score) in enumerate(rec_scores, 1):\n",
    "    desc = df_sample[df_sample['item_id'] == iid]['text'].values[0]\n",
    "    print(f\"  {rank}  |  {iid}   | {score:.4f} | {desc}\")\n",
    "\n",
    "print(\"\\n(Note: Item E is recommended #1 because it shares 'green' and 'cotton' with the User Profile.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde08d7",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction and Vector Space Model\n",
    "### 3.1. Text Feature Extraction (TF-IDF)\n",
    "We use **TF-IDF vectors** for the `title_y` column with basic preprocessing (tokenization and stop-word removal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0764dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Performing TF-IDF Vectorization...\")\n",
    "# tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "# tfidf_matrix = tfidf.fit_transform(df['text'].fillna(''))\n",
    "# print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8867fd",
   "metadata": {},
   "source": [
    "### 3.3. Create Item-Feature Matrix\n",
    "As specified, the item-feature matrix is constructed from the text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79de3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_feature_matrix = tfidf_matrix\n",
    "# print(f\"Final Item-Feature Matrix Shape: {item_feature_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae90782",
   "metadata": {},
   "source": [
    "## 4. User Profile Construction\n",
    "### 4.1. Build User Profiles\n",
    "We use a **Weighted average of rated item features**, where the weights are the rating values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb895ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_user_profiles(df, feature_matrix):\n",
    "#     user_profiles = {}\n",
    "#     user_groups = df.groupby('user_id')\n",
    "    \n",
    "#     for user_id, group in user_groups:\n",
    "#         indices = group.index\n",
    "#         ratings = group['rating'].values.reshape(-1, 1)\n",
    "        \n",
    "#         # Weighted sum of features\n",
    "#         user_features = feature_matrix[indices]\n",
    "#         weighted_features = user_features.multiply(ratings)\n",
    "#         user_profile = weighted_features.sum(axis=0) / ratings.sum()\n",
    "        \n",
    "#         user_profiles[user_id] = np.asarray(user_profile).flatten()\n",
    "        \n",
    "#     return user_profiles\n",
    "\n",
    "# print(\"Building user profiles...\")\n",
    "# user_profiles = build_user_profiles(df, item_feature_matrix)\n",
    "# print(f\"Generated profiles for {len(user_profiles)} users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f912c1c0",
   "metadata": {},
   "source": [
    "### 4.2. Handle Cold-Start Users\n",
    "**Strategy**: Use **popular item features**. Since demographic data is unavailable, we use the average features of the items most frequently rated in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae2d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_popular_item_profile(df, feature_matrix, top_n=100):\n",
    "#     # Identify popular items by count of appearances\n",
    "#     popular_titles = df['item'].value_counts().head(top_n).index\n",
    "#     popular_indices = df[df['item'].isin(popular_titles)].index\n",
    "    \n",
    "#     popular_profile = feature_matrix[popular_indices].mean(axis=0)\n",
    "#     return np.asarray(popular_profile).flatten()\n",
    "\n",
    "# cold_start_profile = get_popular_item_profile(df, item_feature_matrix)\n",
    "# print(\"Cold-start profile (popular) generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60104509",
   "metadata": {},
   "source": [
    "## 5. Similarity Computation and Recommendation\n",
    "### 5.1 & 5.2. Compute Similarity and Generate Top-N Recommendations\n",
    "We use **Cosine similarity** between user profiles and all items. We then rank items by score and remove items already rated by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43e5eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_recommendations(user_id, user_profiles, feature_matrix, df, top_n=10):\n",
    "#     if user_id in user_profiles:\n",
    "#         profile = user_profiles[user_id].reshape(1, -1)\n",
    "#     else:\n",
    "#         profile = cold_start_profile.reshape(1, -1)\n",
    "        \n",
    "#     # 5.1 Cosine Similarity Scores\n",
    "#     scores = cosine_similarity(profile, feature_matrix).flatten()\n",
    "    \n",
    "#     # 5.2 Ranking and Removing already-rated items\n",
    "#     rec_df = pd.DataFrame({'item_idx': range(len(df)), 'score': scores})\n",
    "    \n",
    "#     if user_id in user_profiles:\n",
    "#         rated_indices = df[df['user_id'] == user_id].index\n",
    "#         rec_df = rec_df[~rec_df['item_idx'].isin(rated_indices)]\n",
    "        \n",
    "#     top_indices = rec_df.sort_values(by='score', ascending=False).head(top_n)['item_idx'].values\n",
    "#     return df.iloc[top_indices][['item', 'rating']]\n",
    "\n",
    "# example_user = df['user_id'].iloc[0]\n",
    "# print(f\"--- Top-10 Recommendations for User {example_user} ---\")\n",
    "# print(get_recommendations(example_user, user_profiles, item_feature_matrix, df, top_n=10))\n",
    "\n",
    "# print(f\"\\n--- Top-20 Recommendations for User {example_user} ---\")\n",
    "# print(get_recommendations(example_user, user_profiles, item_feature_matrix, df, top_n=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94e0bc",
   "metadata": {},
   "source": [
    "## 6. k-Nearest Neighbors (k-NN)\n",
    "### 6.1. Implement Item-Based k-NN\n",
    "Predict ratings using a weighted average of the $k$ most similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f173f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_rating_knn(user_id, item_title, df, feature_matrix, k=10):\n",
    "#     target_row = df[df['item'] == item_title].head(1)\n",
    "#     if target_row.empty: return df['rating'].mean()\n",
    "#     target_idx = target_row.index[0]\n",
    "    \n",
    "#     knn = NearestNeighbors(n_neighbors=k+1, metric='cosine')\n",
    "#     knn.fit(feature_matrix)\n",
    "#     distances, indices = knn.kneighbors(feature_matrix[target_idx])\n",
    "    \n",
    "#     user_ratings = df[df['user_id'] == user_id]\n",
    "#     pred_numerator = 0\n",
    "#     pred_denominator = 0\n",
    "    \n",
    "#     for dist, idx in zip(distances.flatten()[1:], indices.flatten()[1:]):\n",
    "#         item_title_sim = df.iloc[idx]['item']\n",
    "#         user_record = user_ratings[user_ratings['item'] == item_title_sim]\n",
    "        \n",
    "#         if not user_record.empty:\n",
    "#             similarity = 1 - dist\n",
    "#             pred_numerator += similarity * user_record['rating'].values[0]\n",
    "#             pred_denominator += similarity\n",
    "            \n",
    "#     if pred_denominator == 0: return df['rating'].mean()\n",
    "#     return pred_numerator / pred_denominator\n",
    "\n",
    "# test_item = df.iloc[10]['item']\n",
    "# prediction = predict_rating_knn(example_user, test_item, df, item_feature_matrix, k=10)\n",
    "# print(f\"Predicted rating for '{test_item}': {prediction:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d43f47",
   "metadata": {},
   "source": [
    "### 6.2. Compare content-based and k-NN approaches\n",
    "\n",
    "| Feature | Content-Based (User Profiles) | k-NN (Item-Based) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Core Concept** | Matches user's overall keyword profile to items. | Matches a specific item to its closest 'neighbors'. |\n",
    "| **Pros** | Great for new items (no ratings needed), explains *why* based on content. | Better at capturing subtle niche similarities text can't describe. |\n",
    "| **Cons** | Can be 'boring' (always recommends similar keywords). | Subject to cold-start (needs ratings to predict well). |\n",
    "| **Use Case** | Discovery based on specific interests/topics. | 'Users who liked this also liked...' logic. |\n",
    "\n",
    "**Summary**: In this implementation, the **Content-Based** approach is more robust because it can recommend items based on text features alone, whereas the **k-NN rating prediction** relies heavily on the user having rated very similar items in the sparse high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025b3b5",
   "metadata": {},
   "source": [
    "## 7. Complete Numerical Example\n",
    "Step-by-step example using a tiny subset of 3 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c45c0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- Step 7.1: Sample Item Descriptions ---\")\n",
    "# mini_items = df['item'].unique()[:3]\n",
    "# print(mini_items)\n",
    "\n",
    "# print(\"\\n--- Step 7.2: TF-IDF Calculation (Sample 5 terms) ---\")\n",
    "# mini_tfidf = tfidf.transform(mini_items).toarray()\n",
    "# print(pd.DataFrame(mini_tfidf[:, :5], columns=tfidf.get_feature_names_out()[:5], index=mini_items))\n",
    "\n",
    "# print(\"\\n--- Step 7.3: User Profile (from 3 items with ratings 5, 4, 3) ---\")\n",
    "# user_ratings_val = np.array([5, 4, 3])\n",
    "# user_mini_profile = np.average(mini_tfidf, axis=0, weights=user_ratings_val)\n",
    "# print(f\"User Profile Vector (first 5 terms): {user_mini_profile[:5]}\")\n",
    "\n",
    "# print(\"\\n--- Step 7.4: Similarity Scores ---\")\n",
    "# mini_scores = cosine_similarity(user_mini_profile.reshape(1, -1), mini_tfidf).flatten()\n",
    "# print(pd.Series(mini_scores, index=mini_items))\n",
    "\n",
    "# print(\"\\n--- Step 7.5: Top-5 Recommendations ---\")\n",
    "# print(get_recommendations(example_user, user_profiles, item_feature_matrix, df, top_n=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
