{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "bdd470cd",
         "metadata": {},
         "source": [
            "# Group 17: \n",
            "### Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "eb84d178",
         "metadata": {},
         "source": [
            "# Part 2: Content-Based Recommendation System\n",
            "\n",
            "This notebook implements a comprehensive content-based recommendation system as per the project requirements."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "dbec4c88",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Results folder exists at: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION2_DomainRecommender\\results\n",
                  "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION2_DomainRecommender\\results\\tables\n"
               ]
            }
         ],
         "source": [
            "from utils import *"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "2847a7fb",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "WARNING: Skipping c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy-2.2.1.dist-info due to invalid metadata entry 'name'\n",
                  "WARNING: Skipping c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy-2.2.1.dist-info due to invalid metadata entry 'name'\n",
                  "WARNING: Skipping c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy-2.2.1.dist-info due to invalid metadata entry 'name'\n",
                  "WARNING: Skipping c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy-2.2.1.dist-info due to invalid metadata entry 'name'\n",
                  "WARNING: Skipping c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy-2.2.1.dist-info due to invalid metadata entry 'name'\n",
                  "WARNING: Skipping c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy-2.2.1.dist-info due to invalid metadata entry 'name'\n",
                  "\n",
                  "[notice] A new release of pip is available: 24.0 -> 25.3\n",
                  "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Results folder exists at: c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION2_DomainRecommender\\results\n",
                  "Subfolder exists: c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION2_DomainRecommender\\results\\tables\n"
               ]
            }
         ],
         "source": [
            "%pip install nltk\n",
            "from utils import *"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "add87d07",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Results folder exists at: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION2_DomainRecommender\\results\n",
                  "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION2_DomainRecommender\\results\\tables\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[nltk_data] Downloading package stopwords to\n",
                  "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
                  "[nltk_data]   Package stopwords is already up-to-date!\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from utils import *\n",
            "# Download necessary NLTK data\n",
            "nltk.download('stopwords')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "add87d07",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Data Loaded. Shape: (83355, 7)\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>user_id</th>\n",
                     "      <th>item</th>\n",
                     "      <th>rating</th>\n",
                     "      <th>is_green</th>\n",
                     "      <th>price</th>\n",
                     "      <th>text</th>\n",
                     "      <th>item_id_encoded</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                     "      <td>Dawn Ultra Dishwashing Liquid, Original Scent ...</td>\n",
                     "      <td>4</td>\n",
                     "      <td>0</td>\n",
                     "      <td>22.77</td>\n",
                     "      <td>Too expensive</td>\n",
                     "      <td>107204</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                     "      <td>Charmin Ultra Soft Cushiony Touch Toilet Paper...</td>\n",
                     "      <td>4</td>\n",
                     "      <td>0</td>\n",
                     "      <td>28.82</td>\n",
                     "      <td>Very good paper but too expensiveoo</td>\n",
                     "      <td>85026</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                     "      <td>Viva Signature Cloth Choose-A-Sheet Paper Towe...</td>\n",
                     "      <td>4</td>\n",
                     "      <td>0</td>\n",
                     "      <td>23.65</td>\n",
                     "      <td>Expensive but great towels</td>\n",
                     "      <td>358978</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                     "      <td>Drano Max Gel Drain Clog Remover and Cleaner f...</td>\n",
                     "      <td>2</td>\n",
                     "      <td>0</td>\n",
                     "      <td>15.47</td>\n",
                     "      <td>The old Draino worked better.  This pours easi...</td>\n",
                     "      <td>117303</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
                     "      <td>LiCB A23 23A 12V Alkaline Battery (5-Pack)</td>\n",
                     "      <td>4</td>\n",
                     "      <td>0</td>\n",
                     "      <td>5.99</td>\n",
                     "      <td>I jut got them, don’t know yet if they last long</td>\n",
                     "      <td>204987</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                        user_id  \\\n",
                     "0  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                     "1  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                     "2  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                     "3  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                     "4  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
                     "\n",
                     "                                                item  rating  is_green  price  \\\n",
                     "0  Dawn Ultra Dishwashing Liquid, Original Scent ...       4         0  22.77   \n",
                     "1  Charmin Ultra Soft Cushiony Touch Toilet Paper...       4         0  28.82   \n",
                     "2  Viva Signature Cloth Choose-A-Sheet Paper Towe...       4         0  23.65   \n",
                     "3  Drano Max Gel Drain Clog Remover and Cleaner f...       2         0  15.47   \n",
                     "4         LiCB A23 23A 12V Alkaline Battery (5-Pack)       4         0   5.99   \n",
                     "\n",
                     "                                                text  item_id_encoded  \n",
                     "0                                      Too expensive           107204  \n",
                     "1                Very good paper but too expensiveoo            85026  \n",
                     "2                         Expensive but great towels           358978  \n",
                     "3  The old Draino worked better.  This pours easi...           117303  \n",
                     "4   I jut got them, don’t know yet if they last long           204987  "
                  ]
               },
               "execution_count": 4,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# --- LOAD YOUR DATA HERE ---\n",
            "data_path = r'../data/'\n",
            "df = pd.read_csv(os.path.join(data_path, 'Amazon_health&household_label_encoded.csv'))\n",
            "\n",
            "print(\"Data Loaded. Shape:\", df.shape)\n",
            "df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "418b1230",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Data Cleaned. Generating Matrix...\n",
                  "Item-Feature Matrix Ready: (1000, 502)\n"
               ]
            }
         ],
         "source": [
            "# 1. Separate Unique Items from Ratings\n",
            "# We need one row per item to build the feature matrix\n",
            "df_items = df[['item_id_encoded', 'item', 'price', 'text', 'is_green']].drop_duplicates(subset='item_id_encoded').sort_values('item_id_encoded').set_index('item_id_encoded')\n",
            "\n",
            "\n",
            "# --- FIX 1: Handle NaNs in Text ---\n",
            "# Fill missing text with empty string\n",
            "df_items['text'] = df_items['text'].fillna('')\n",
            "\n",
            "# --- FIX 2: Handle NaNs in Price ---\n",
            "# Fill missing prices with the Median price (better than 0)\n",
            "median_price = df_items['price'].median()\n",
            "df_items['price'] = df_items['price'].fillna(median_price)\n",
            "\n",
            "# --- FIX 3: Handle NaNs in Is_Green ---\n",
            "# Fill missing boolean with False (0)\n",
            "df_items['is_green'] = df_items['is_green'].fillna(False)\n",
            "\n",
            "print(\"Data Cleaned. Generating Matrix...\")\n",
            "\n",
            "\n",
            "# 2. Re-create the Item-Feature Matrix (from Phase 3)\n",
            "# A. Text (TF-IDF)\n",
            "tfidf = TfidfVectorizer(stop_words='english', max_features=500) # Limited to 100 for speed on large data\n",
            "text_matrix = tfidf.fit_transform(df_items['text'].fillna('')).toarray()\n",
            "\n",
            "# B. Price (Normalized)\n",
            "scaler = MinMaxScaler()\n",
            "price_vec = scaler.fit_transform(df_items[['price']])\n",
            "\n",
            "# C. Is_Green (Binary)\n",
            "green_vec = df_items[['is_green']].astype(int).values\n",
            "\n",
            "# D. Combine into \"item_features\"\n",
            "item_features = np.hstack([text_matrix, price_vec, green_vec])\n",
            "\n",
            "print(f\"Item-Feature Matrix Ready: {item_features.shape}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "e428ee24",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "--- 4. User Profile Construction ---\n",
                  "   -> Cold-Start Strategy: Using Global Average Item Vector.\n",
                  "   -> Built profiles for 9591 users.\n",
                  "\n",
                  "Sample Profile (User AFNT6ZJCYQN3WDIKUSWHJDXNND2Q):\n",
                  "Vector Shape: (502,)\n",
                  "First 5 Features: [0.00292027 0.00241824 0.00286581 0.00267662 0.00677225]\n",
                  "    Saved Dict as CSV: tables/user_profiles.csv\n"
               ]
            }
         ],
         "source": [
            "# ==========================================\n",
            "# 4. USER PROFILE CONSTRUCTION\n",
            "# ==========================================\n",
            "\n",
            "def build_user_profiles(df, item_features_matrix):\n",
            "    print(\"--- 4. User Profile Construction ---\")\n",
            "    \n",
            "    # 1. Get unique users\n",
            "    unique_users = df['user_id'].unique()\n",
            "    user_profiles = {}\n",
            "    \n",
            "    # ---------------------------------------------------------\n",
            "    # 4.2 STRATEGY: Handle Cold-Start Users (Popular/Average)\n",
            "    # ---------------------------------------------------------\n",
            "    # Since we lack demographics, we calculate the \"Global Average Item\"\n",
            "    # This represents the \"average taste\" of the entire catalog.\n",
            "    # Ideally, you could weigh this by popularity (rating count), \n",
            "    # but a simple mean is sufficient for this requirement.\n",
            "    cold_start_vector = np.mean(item_features_matrix, axis=0)\n",
            "    \n",
            "    print(f\"   -> Cold-Start Strategy: Using Global Average Item Vector.\")\n",
            "    \n",
            "    # ---------------------------------------------------------\n",
            "    # 4.1 STRATEGY: Build User Profiles (Weighted Average)\n",
            "    # ---------------------------------------------------------\n",
            "    for uid in unique_users:\n",
            "        # Get user history\n",
            "        user_history = df[df['user_id'] == uid]\n",
            "        \n",
            "        # If user has no ratings (or valid item_ids), treat as Cold Start\n",
            "        if user_history.empty:\n",
            "            user_profiles[uid] = cold_start_vector\n",
            "            continue\n",
            "            \n",
            "        # Get indices and ratings\n",
            "        # Ensure indices are integers for matrix lookup\n",
            "        item_indices = user_history['item_id_encoded'].values.astype(int)\n",
            "        ratings = user_history['rating'].values.reshape(-1, 1)\n",
            "        \n",
            "        # ERROR HANDLING: Check if any item_id is out of bounds\n",
            "        # (This happens if df contains items not in our feature matrix)\n",
            "        valid_mask = item_indices < item_features_matrix.shape[0]\n",
            "        item_indices = item_indices[valid_mask]\n",
            "        ratings = ratings[valid_mask]\n",
            "        \n",
            "        if len(item_indices) == 0:\n",
            "            user_profiles[uid] = cold_start_vector\n",
            "            continue\n",
            "\n",
            "        # Fetch vectors for items rated by this user\n",
            "        rated_item_vectors = item_features_matrix[item_indices]\n",
            "        \n",
            "        # CALCULATE WEIGHTED AVERAGE\n",
            "        # Formula: Sum(Item_Vector * Rating) / Sum(Ratings)\n",
            "        weighted_sum = np.sum(rated_item_vectors * ratings, axis=0)\n",
            "        total_rating_val = np.sum(ratings)\n",
            "        \n",
            "        if total_rating_val == 0:\n",
            "            user_profiles[uid] = cold_start_vector\n",
            "        else:\n",
            "            user_profiles[uid] = weighted_sum / total_rating_val\n",
            "            \n",
            "    print(f\"   -> Built profiles for {len(user_profiles)} users.\")\n",
            "    return user_profiles\n",
            "\n",
            "# --- EXECUTE PHASE 4 ---\n",
            "# Input: Your main dataframe (df) and the matrix from Phase 3 (item_features)\n",
            "user_profiles = build_user_profiles(df, item_features)\n",
            "\n",
            "# --- VERIFICATION ---\n",
            "# Let's look at the first user's profile\n",
            "sample_uid = list(user_profiles.keys())[0]\n",
            "print(f\"\\nSample Profile (User {sample_uid}):\")\n",
            "print(f\"Vector Shape: {user_profiles[sample_uid].shape}\")\n",
            "print(f\"First 5 Features: {user_profiles[sample_uid][:5]}\")\n",
            "save_output(user_profiles, \"user_profiles.csv\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "0b84011c",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n",
                  "--- Top 10 Recommendations for User AFNT6ZJCYQN3WDIKUSWHJDXNND2Q ---\n",
                  "Item ID    | Score    | Item Name (Lookup)\n",
                  "--------------------------------------------------\n",
                  "717        | 0.7478   | Unknown Item\n",
                  "985        | 0.7396   | Unknown Item\n",
                  "491        | 0.7320   | Unknown Item\n",
                  "27         | 0.7262   | Unknown Item\n",
                  "613        | 0.7262   | Unknown Item\n",
                  "984        | 0.7262   | Unknown Item\n",
                  "821        | 0.7241   | Unknown Item\n",
                  "137        | 0.7205   | Unknown Item\n",
                  "697        | 0.7197   | Unknown Item\n",
                  "138        | 0.7187   | Unknown Item\n",
                  "\n",
                  "(Generated Top-20 list. Count: 20)\n",
                  "    Saved List as CSV: tables/content_based_top_20.csv\n"
               ]
            }
         ],
         "source": [
            "# ==========================================\n",
            "# 5. SIMILARITY & RECOMMENDATION\n",
            "# ==========================================\n",
            "\n",
            "def get_recommendations(user_id, user_profiles, item_features_matrix, df_full, top_n=10):\n",
            "    \"\"\"\n",
            "    Generates content-based recommendations for a specific user.\n",
            "    \"\"\"\n",
            "    # 1. Get the User's Profile Vector\n",
            "    if user_id not in user_profiles:\n",
            "        print(f\"User {user_id} not found in profiles.\")\n",
            "        return []\n",
            "    \n",
            "    # Reshape is needed because cosine_similarity expects a 2D array (1 sample, n features)\n",
            "    user_vector = user_profiles[user_id].reshape(1, -1)\n",
            "    \n",
            "    # ---------------------------------------------------------\n",
            "    # 5.1 COMPUTE SIMILARITY\n",
            "    # ---------------------------------------------------------\n",
            "    # Calculate similarity between this User Vector and ALL Item Vectors\n",
            "    # Result shape: (1, num_items) -> flatten to 1D array\n",
            "    similarity_scores = cosine_similarity(user_vector, item_features_matrix).flatten()\n",
            "    \n",
            "    # ---------------------------------------------------------\n",
            "    # 5.2 GENERATE TOP-N RECOMMENDATIONS\n",
            "    # ---------------------------------------------------------\n",
            "    \n",
            "    # Get the list of items the user has ALREADY rated\n",
            "    # We don't want to recommend things they already know about\n",
            "    rated_items = df_full[df_full['user_id'] == user_id]['item_id_encoded'].values\n",
            "    \n",
            "    # Create a list of tuples: (item_id_encoded, similarity_score)\n",
            "    # We enumerate to get the index (which corresponds to item_id_encoded)\n",
            "    all_scores = list(enumerate(similarity_scores))\n",
            "    \n",
            "    # Filter out already rated items\n",
            "    candidates = [\n",
            "        (item_id, score) \n",
            "        for item_id, score in all_scores \n",
            "        if item_id not in rated_items\n",
            "    ]\n",
            "    \n",
            "    # Sort by Score (Descending) -> Highest similarity first\n",
            "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
            "    \n",
            "    # Slice the top N\n",
            "    top_recommendations = candidates[:top_n]\n",
            "    \n",
            "    return top_recommendations\n",
            "\n",
            "# ==========================================\n",
            "# EXECUTION EXAMPLE\n",
            "# ==========================================\n",
            "\n",
            "# Select a sample user to test\n",
            "sample_user = df['user_id'].iloc[0]\n",
            "\n",
            "# --- Get Top 10 Recommendations ---\n",
            "recs_10 = get_recommendations(sample_user, user_profiles, item_features, df, top_n=10)\n",
            "\n",
            "print(f\"\\n--- Top 10 Recommendations for User {sample_user} ---\")\n",
            "print(f\"{'Item ID':<10} | {'Score':<8} | {'Item Name (Lookup)'}\")\n",
            "print(\"-\" * 50)\n",
            "\n",
            "# We need a helper to look up original item IDs from encoded IDs\n",
            "item_lookup = df[['item_id_encoded', 'item_id']].drop_duplicates().set_index('item_id_encoded')\n",
            "\n",
            "for item_id_enc, score in recs_10:\n",
            "    # Handle case where item_id_enc might be out of lookup range\n",
            "    try:\n",
            "        original_id = item_lookup.loc[item_id_enc, 'item_id']\n",
            "    except KeyError:\n",
            "        original_id = \"Unknown\"\n",
            "        \n",
            "    print(f\"{item_id_enc:<10} | {score:.4f}   | {original_id}\")\n",
            "\n",
            "\n",
            "# --- Get Top 20 Recommendations ---\n",
            "recs_20 = get_recommendations(sample_user, user_profiles, item_features, df, top_n=20)\n",
            "print(f\"\\n(Generated Top-20 list. Count: {len(recs_20)})\")\n",
            "save_output(recs_20, \"content_based_top_20.csv\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "54928ef2",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n",
                  "--- 6. k-Nearest Neighbors (Item-Based) with Index Mapping ---\n",
                  "Test User: AFNT6ZJCYQN3WDIKUSWHJDXNND2Q\n",
                  "Target Item ID: 81920\n",
                  "Matrix Index for Item: 178\n",
                  "Predicted Rating (k=10): 3.6000\n",
                  "Predicted Rating (k=20): 3.6000\n"
               ]
            }
         ],
         "source": [
            "import numpy as np\n",
            "from sklearn.neighbors import NearestNeighbors\n",
            "\n",
            "print(\"\\n--- 6. k-Nearest Neighbors (Item-Based) with Index Mapping ---\")\n",
            "\n",
            "# ==========================================\n",
            "# 1. CREATE INDEX MAPPINGS\n",
            "# ==========================================\n",
            "# We assume item_features was built grouping by 'item_id_encoded' (sorted by default).\n",
            "# Row 0 of matrix = Smallest Item ID\n",
            "# Row 999 of matrix = Largest Item ID\n",
            "\n",
            "sorted_item_ids = sorted(df['item_id_encoded'].unique())\n",
            "\n",
            "# Map \"Real ID\" -> \"Matrix Row Index\" (For looking up the target item)\n",
            "id_to_matrix_idx = {item_id: i for i, item_id in enumerate(sorted_item_ids)}\n",
            "\n",
            "# Map \"Matrix Row Index\" -> \"Real ID\" (For interpreting the neighbors found)\n",
            "matrix_idx_to_id = {i: item_id for i, item_id in enumerate(sorted_item_ids)}\n",
            "\n",
            "# ==========================================\n",
            "# 2. FIT THE MODEL\n",
            "# ==========================================\n",
            "knn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
            "knn_model.fit(item_features)\n",
            "\n",
            "# ==========================================\n",
            "# 3. PREDICTION FUNCTION\n",
            "# ==========================================\n",
            "def predict_rating_knn(user_id, target_item_id, df_full, knn_model, item_features, k=10):\n",
            "    \n",
            "    # --- Check if item exists in our matrix ---\n",
            "    if target_item_id not in id_to_matrix_idx:\n",
            "        print(f\"Item {target_item_id} not found in feature matrix.\")\n",
            "        return df_full['rating'].mean()\n",
            "        \n",
            "    # --- Step 1: Get Matrix Index for the Target Item ---\n",
            "    # Translate Real ID (e.g. 81920) -> Matrix Index (e.g. 5)\n",
            "    matrix_idx = id_to_matrix_idx[target_item_id]\n",
            "    \n",
            "    # Reshape for KNN input\n",
            "    target_vec = item_features[matrix_idx].reshape(1, -1)\n",
            "    \n",
            "    # --- Step 2: Find Neighbors ---\n",
            "    distances, indices = knn_model.kneighbors(target_vec, n_neighbors=k+1)\n",
            "    \n",
            "    # Flatten results\n",
            "    neighbor_matrix_indices = indices.flatten()\n",
            "    neighbor_dists = distances.flatten()\n",
            "    \n",
            "    # --- Step 3: Calculate Weighted Average ---\n",
            "    user_history = df_full[df_full['user_id'] == user_id]\n",
            "    \n",
            "    weighted_sum = 0\n",
            "    similarity_sum = 0\n",
            "    count_matches = 0\n",
            "    \n",
            "    # Skip the first neighbor (it is the item itself)\n",
            "    for i in range(1, len(neighbor_matrix_indices)):\n",
            "        # Get Matrix Index and Distance\n",
            "        n_matrix_idx = neighbor_matrix_indices[i]\n",
            "        n_dist = neighbor_dists[i]\n",
            "        \n",
            "        # Translate Matrix Index -> Real Item ID to check user history\n",
            "        n_real_id = matrix_idx_to_id[n_matrix_idx]\n",
            "        \n",
            "        similarity = 1 - n_dist\n",
            "        \n",
            "        # Check if user rated this neighbor (using Real ID)\n",
            "        if n_real_id in user_history['item_id_encoded'].values:\n",
            "            actual_rating = user_history[user_history['item_id_encoded'] == n_real_id]['rating'].values[0]\n",
            "            \n",
            "            weighted_sum += (similarity * actual_rating)\n",
            "            similarity_sum += similarity\n",
            "            count_matches += 1\n",
            "            \n",
            "    # --- Step 4: Final Prediction ---\n",
            "    if count_matches == 0:\n",
            "        if not user_history.empty:\n",
            "            return user_history['rating'].mean()\n",
            "        return df_full['rating'].mean()\n",
            "    \n",
            "    if similarity_sum == 0:\n",
            "        return 0\n",
            "\n",
            "    return weighted_sum / similarity_sum\n",
            "\n",
            "# ==========================================\n",
            "# 4. EXECUTION\n",
            "# ==========================================\n",
            "\n",
            "# Pick a user and an item they haven't rated\n",
            "sample_user = df['user_id'].iloc[0]\n",
            "all_items = set(df['item_id_encoded'].unique())\n",
            "user_rated_items = set(df[df['user_id'] == sample_user]['item_id_encoded'])\n",
            "candidate_item = list(all_items - user_rated_items)[0]\n",
            "\n",
            "print(f\"Test User: {sample_user}\")\n",
            "print(f\"Target Item ID: {candidate_item}\")\n",
            "print(f\"Matrix Index for Item: {id_to_matrix_idx.get(candidate_item, 'Not Found')}\")\n",
            "\n",
            "# Test k=10\n",
            "pred_10 = predict_rating_knn(sample_user, candidate_item, df, knn_model, item_features, k=10)\n",
            "print(f\"Predicted Rating (k=10): {pred_10:.4f}\")\n",
            "\n",
            "# Test k=20\n",
            "pred_20 = predict_rating_knn(sample_user, candidate_item, df, knn_model, item_features, k=20)\n",
            "print(f\"Predicted Rating (k=20): {pred_20:.4f}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "339ba425",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "============================================================\n",
                  "       PART 7: COMPLETE NUMERICAL EXAMPLE\n",
                  "============================================================\n",
                  "\n",
                  "[STEP 1] Sample Data Representation\n",
                  "We select 5 items. The User has rated Item A and Item B.\n",
                  "  item_id                text  price  is_green\n",
                  "0       A    green eco cotton     20         1\n",
                  "1       B    green eco bamboo     25         1\n",
                  "2       C   red plastic cheap     10         0\n",
                  "3       D    blue denim jeans     50         0\n",
                  "4       E  green cotton shirt     22         1\n",
                  "\n",
                  "User Ratings: {'A': 5.0, 'B': 4.0}\n",
                  "\n",
                  "------------------------------\n",
                  "[STEP 2] TF-IDF Calculation\n",
                  "------------------------------\n",
                  "Vocabulary (11 terms): ['bamboo', 'blue', 'cheap', 'cotton', 'denim', 'eco', 'green', 'jeans', 'plastic', 'red', 'shirt']\n",
                  "\n",
                  "TF-IDF Vectors (Sample):\n",
                  "Item A: [0.00, 0.00, 0.00, 0.61, 0.00, 0.61, 0.51, 0.00, 0.00, 0.00, 0.00]\n",
                  "Item B: [0.69, 0.00, 0.00, 0.00, 0.00, 0.56, 0.46, 0.00, 0.00, 0.00, 0.00]\n",
                  "Item C: [0.00, 0.00, 0.58, 0.00, 0.00, 0.00, 0.00, 0.00, 0.58, 0.58, 0.00]\n",
                  "\n",
                  "------------------------------\n",
                  "[STEP 3] Full Item-Feature Matrix\n",
                  "------------------------------\n",
                  "Combining: [TF-IDF Vectors] + [Norm_Price] + [Is_Green]\n",
                  "Item A Full Vector (Size 13):\n",
                  "[0.   0.   0.   0.61 0.   0.61 0.51 0.   0.   0.   0.   0.25 1.  ]\n",
                  "\n",
                  "------------------------------\n",
                  "[STEP 4] User Profile Construction (Weighted Average)\n",
                  "------------------------------\n",
                  "Math: (Vec_A * 5.0 + Vec_B * 4.0) / 9.0\n",
                  "User Profile Vector:\n",
                  "[0.31 0.   0.   0.34 0.   0.59 0.49 0.   0.   0.   0.   0.31 1.  ]\n",
                  "\n",
                  "------------------------------\n",
                  "[STEP 5] Similarity & Top Recommendations\n",
                  "------------------------------\n",
                  "\n",
                  "Calculation for Item E ('green cotton shirt'):\n",
                  "   Dot Product: 1.5052\n",
                  "   Norm(User): 1.3721, Norm(Item): 1.4457\n",
                  "   Score: 0.7588\n",
                  "\n",
                  "--- Final Recommendations ---\n",
                  "Rank | Item | Score  | Description\n",
                  "  1  |  E   | 0.7588 | green cotton shirt\n",
                  "  2  |  D   | 0.1575 | blue denim jeans\n",
                  "  3  |  C   | 0.0000 | red plastic cheap\n",
                  "\n",
                  "(Note: Item E is recommended #1 because it shares 'green' and 'cotton' with the User Profile.)\n",
                  "    Saved List as CSV: tables/content_based.csv\n"
               ]
            }
         ],
         "source": [
            "# ==========================================\n",
            "# 7.1 STEP-BY-STEP NUMERICAL EXAMPLE\n",
            "# ==========================================\n",
            "print(\"=\"*60)\n",
            "print(\"       PART 7: COMPLETE NUMERICAL EXAMPLE\")\n",
            "print(\"=\"*60)\n",
            "\n",
            "# --- STEP 1: SAMPLE DATA ---\n",
            "print(\"\\n[STEP 1] Sample Data Representation\")\n",
            "print(\"We select 5 items. The User has rated Item A and Item B.\")\n",
            "\n",
            "data = {\n",
            "    'item_id': ['A', 'B', 'C', 'D', 'E'],\n",
            "    'text': [\n",
            "        'green eco cotton',   # Item A (Rated 5.0)\n",
            "        'green eco bamboo',   # Item B (Rated 4.0)\n",
            "        'red plastic cheap',  # Item C (Unrated - Dissimilar)\n",
            "        'blue denim jeans',   # Item D (Unrated - Dissimilar)\n",
            "        'green cotton shirt'  # Item E (Unrated - Target Recommendation)\n",
            "    ],\n",
            "    'price': [20, 25, 10, 50, 22],\n",
            "    'is_green': [1, 1, 0, 0, 1]\n",
            "}\n",
            "\n",
            "# User Ratings: Likes \"Green/Eco\" items\n",
            "ratings = {'A': 5.0, 'B': 4.0} \n",
            "\n",
            "df_sample = pd.DataFrame(data)\n",
            "print(df_sample)\n",
            "print(f\"\\nUser Ratings: {ratings}\")\n",
            "\n",
            "\n",
            "# --- STEP 2: TF-IDF CALCULATION ---\n",
            "print(\"\\n\" + \"-\"*30)\n",
            "print(\"[STEP 2] TF-IDF Calculation\")\n",
            "print(\"-\" * 30)\n",
            "\n",
            "# We use a simple tokenizer to keep vocabulary small\n",
            "tfidf = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
            "tfidf_matrix = tfidf.fit_transform(df_sample['text']).toarray()\n",
            "vocab = tfidf.get_feature_names_out()\n",
            "\n",
            "print(f\"Vocabulary ({len(vocab)} terms): {list(vocab)}\")\n",
            "\n",
            "# Show vectors for the first 3 items\n",
            "print(\"\\nTF-IDF Vectors (Sample):\")\n",
            "for i in range(3):\n",
            "    # Formatting to 2 decimal places\n",
            "    vec_str = \", \".join([f\"{x:.2f}\" for x in tfidf_matrix[i]])\n",
            "    print(f\"Item {df_sample.loc[i, 'item_id']}: [{vec_str}]\")\n",
            "\n",
            "\n",
            "# --- STEP 3: FULL FEATURE MATRIX ---\n",
            "print(\"\\n\" + \"-\"*30)\n",
            "print(\"[STEP 3] Full Item-Feature Matrix\")\n",
            "print(\"-\" * 30)\n",
            "print(\"Combining: [TF-IDF Vectors] + [Norm_Price] + [Is_Green]\")\n",
            "\n",
            "# Normalize Price (0-1)\n",
            "scaler = MinMaxScaler()\n",
            "price_norm = scaler.fit_transform(df_sample[['price']])\n",
            "\n",
            "# Is_Green (already 0/1)\n",
            "green_vec = df_sample[['is_green']].values\n",
            "\n",
            "# Combine\n",
            "item_features = np.hstack([tfidf_matrix, price_norm, green_vec])\n",
            "\n",
            "# Print Item A's full vector as an example\n",
            "vec_A = item_features[0]\n",
            "print(f\"Item A Full Vector (Size {len(vec_A)}):\")\n",
            "print(np.round(vec_A, 2))\n",
            "\n",
            "\n",
            "# --- STEP 4: USER PROFILE CONSTRUCTION ---\n",
            "print(\"\\n\" + \"-\"*30)\n",
            "print(\"[STEP 4] User Profile Construction (Weighted Average)\")\n",
            "print(\"-\" * 30)\n",
            "\n",
            "# Get vectors for rated items A (index 0) and B (index 1)\n",
            "vec_A = item_features[0]\n",
            "vec_B = item_features[1]\n",
            "rating_A = 5.0\n",
            "rating_B = 4.0\n",
            "\n",
            "# Formula: (VecA * 5 + VecB * 4) / (5 + 4)\n",
            "numerator = (vec_A * rating_A) + (vec_B * rating_B)\n",
            "denominator = rating_A + rating_B\n",
            "user_profile = numerator / denominator\n",
            "\n",
            "print(f\"Math: (Vec_A * {rating_A} + Vec_B * {rating_B}) / {denominator}\")\n",
            "print(f\"User Profile Vector:\\n{np.round(user_profile, 2)}\")\n",
            "\n",
            "\n",
            "# --- STEP 5: SIMILARITY & RECOMMENDATION ---\n",
            "print(\"\\n\" + \"-\"*30)\n",
            "print(\"[STEP 5] Similarity & Top Recommendations\")\n",
            "print(\"-\" * 30)\n",
            "\n",
            "rec_scores = []\n",
            "\n",
            "# Calculate Cosine Similarity for unrated items (C, D, E)\n",
            "# Indices: C=2, D=3, E=4\n",
            "for i in [2, 3, 4]:\n",
            "    item_id = df_sample.loc[i, 'item_id']\n",
            "    vec_item = item_features[i]\n",
            "    \n",
            "    # Cosine Similarity Formula: dot(A, B) / (norm(A) * norm(B))\n",
            "    dot_product = np.dot(user_profile, vec_item)\n",
            "    norm_u = np.linalg.norm(user_profile)\n",
            "    norm_i = np.linalg.norm(vec_item)\n",
            "    \n",
            "    score = dot_product / (norm_u * norm_i)\n",
            "    rec_scores.append((item_id, score))\n",
            "    \n",
            "    # Print calculation for Item E (The expected winner)\n",
            "    if item_id == 'E':\n",
            "        print(f\"\\nCalculation for Item E ('green cotton shirt'):\")\n",
            "        print(f\"   Dot Product: {dot_product:.4f}\")\n",
            "        print(f\"   Norm(User): {norm_u:.4f}, Norm(Item): {norm_i:.4f}\")\n",
            "        print(f\"   Score: {score:.4f}\")\n",
            "\n",
            "# Sort and Recommend\n",
            "rec_scores.sort(key=lambda x: x[1], reverse=True)\n",
            "\n",
            "print(\"\\n--- Final Recommendations ---\")\n",
            "print(\"Rank | Item | Score  | Description\")\n",
            "for rank, (iid, score) in enumerate(rec_scores, 1):\n",
            "    desc = df_sample[df_sample['item_id'] == iid]['text'].values[0]\n",
            "    print(f\"  {rank}  |  {iid}   | {score:.4f} | {desc}\")\n",
            "\n",
            "print(\"\\n(Note: Item E is recommended #1 because it shares 'green' and 'cotton' with the User Profile.)\")\n",
            "save_output(rec_scores, \"content_based.csv\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ebde08d7",
         "metadata": {},
         "source": [
            "## 3. Feature Extraction and Vector Space Model\n",
            "### 3.1. Text Feature Extraction (TF-IDF)\n",
            "We use **TF-IDF vectors** for the `title_y` column with basic preprocessing (tokenization and stop-word removal)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "0764dd50",
         "metadata": {},
         "outputs": [],
         "source": [
            "# print(\"Performing TF-IDF Vectorization...\")\n",
            "# tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
            "# tfidf_matrix = tfidf.fit_transform(df['text'].fillna(''))\n",
            "# print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "af8867fd",
         "metadata": {},
         "source": [
            "### 3.3. Create Item-Feature Matrix\n",
            "As specified, the item-feature matrix is constructed from the text features."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "79de3cb6",
         "metadata": {},
         "outputs": [],
         "source": [
            "# item_feature_matrix = tfidf_matrix\n",
            "# print(f\"Final Item-Feature Matrix Shape: {item_feature_matrix.shape}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0ae90782",
         "metadata": {},
         "source": [
            "## 4. User Profile Construction\n",
            "### 4.1. Build User Profiles\n",
            "We use a **Weighted average of rated item features**, where the weights are the rating values."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "efb895ed",
         "metadata": {},
         "outputs": [],
         "source": [
            "# def build_user_profiles(df, feature_matrix):\n",
            "#     user_profiles = {}\n",
            "#     user_groups = df.groupby('user_id')\n",
            "    \n",
            "#     for user_id, group in user_groups:\n",
            "#         indices = group.index\n",
            "#         ratings = group['rating'].values.reshape(-1, 1)\n",
            "        \n",
            "#         # Weighted sum of features\n",
            "#         user_features = feature_matrix[indices]\n",
            "#         weighted_features = user_features.multiply(ratings)\n",
            "#         user_profile = weighted_features.sum(axis=0) / ratings.sum()\n",
            "        \n",
            "#         user_profiles[user_id] = np.asarray(user_profile).flatten()\n",
            "        \n",
            "#     return user_profiles\n",
            "\n",
            "# print(\"Building user profiles...\")\n",
            "# user_profiles = build_user_profiles(df, item_feature_matrix)\n",
            "# print(f\"Generated profiles for {len(user_profiles)} users.\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f912c1c0",
         "metadata": {},
         "source": [
            "### 4.2. Handle Cold-Start Users\n",
            "**Strategy**: Use **popular item features**. Since demographic data is unavailable, we use the average features of the items most frequently rated in the dataset."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "9ae2d107",
         "metadata": {},
         "outputs": [],
         "source": [
            "# def get_popular_item_profile(df, feature_matrix, top_n=100):\n",
            "#     # Identify popular items by count of appearances\n",
            "#     popular_titles = df['item'].value_counts().head(top_n).index\n",
            "#     popular_indices = df[df['item'].isin(popular_titles)].index\n",
            "    \n",
            "#     popular_profile = feature_matrix[popular_indices].mean(axis=0)\n",
            "#     return np.asarray(popular_profile).flatten()\n",
            "\n",
            "# cold_start_profile = get_popular_item_profile(df, item_feature_matrix)\n",
            "# print(\"Cold-start profile (popular) generated.\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "60104509",
         "metadata": {},
         "source": [
            "## 5. Similarity Computation and Recommendation\n",
            "### 5.1 & 5.2. Compute Similarity and Generate Top-N Recommendations\n",
            "We use **Cosine similarity** between user profiles and all items. We then rank items by score and remove items already rated by the user."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "43e5eb89",
         "metadata": {},
         "outputs": [],
         "source": [
            "# def get_recommendations(user_id, user_profiles, feature_matrix, df, top_n=10):\n",
            "#     if user_id in user_profiles:\n",
            "#         profile = user_profiles[user_id].reshape(1, -1)\n",
            "#     else:\n",
            "#         profile = cold_start_profile.reshape(1, -1)\n",
            "        \n",
            "#     # 5.1 Cosine Similarity Scores\n",
            "#     scores = cosine_similarity(profile, feature_matrix).flatten()\n",
            "    \n",
            "#     # 5.2 Ranking and Removing already-rated items\n",
            "#     rec_df = pd.DataFrame({'item_idx': range(len(df)), 'score': scores})\n",
            "    \n",
            "#     if user_id in user_profiles:\n",
            "#         rated_indices = df[df['user_id'] == user_id].index\n",
            "#         rec_df = rec_df[~rec_df['item_idx'].isin(rated_indices)]\n",
            "        \n",
            "#     top_indices = rec_df.sort_values(by='score', ascending=False).head(top_n)['item_idx'].values\n",
            "#     return df.iloc[top_indices][['item', 'rating']]\n",
            "\n",
            "# example_user = df['user_id'].iloc[0]\n",
            "# print(f\"--- Top-10 Recommendations for User {example_user} ---\")\n",
            "# print(get_recommendations(example_user, user_profiles, item_feature_matrix, df, top_n=10))\n",
            "\n",
            "# print(f\"\\n--- Top-20 Recommendations for User {example_user} ---\")\n",
            "# print(get_recommendations(example_user, user_profiles, item_feature_matrix, df, top_n=20))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "3f94e0bc",
         "metadata": {},
         "source": [
            "## 6. k-Nearest Neighbors (k-NN)\n",
            "### 6.1. Implement Item-Based k-NN\n",
            "Predict ratings using a weighted average of the $k$ most similar items."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "1f173f3d",
         "metadata": {},
         "outputs": [],
         "source": [
            "# def predict_rating_knn(user_id, item_title, df, feature_matrix, k=10):\n",
            "#     target_row = df[df['item'] == item_title].head(1)\n",
            "#     if target_row.empty: return df['rating'].mean()\n",
            "#     target_idx = target_row.index[0]\n",
            "    \n",
            "#     knn = NearestNeighbors(n_neighbors=k+1, metric='cosine')\n",
            "#     knn.fit(feature_matrix)\n",
            "#     distances, indices = knn.kneighbors(feature_matrix[target_idx])\n",
            "    \n",
            "#     user_ratings = df[df['user_id'] == user_id]\n",
            "#     pred_numerator = 0\n",
            "#     pred_denominator = 0\n",
            "    \n",
            "#     for dist, idx in zip(distances.flatten()[1:], indices.flatten()[1:]):\n",
            "#         item_title_sim = df.iloc[idx]['item']\n",
            "#         user_record = user_ratings[user_ratings['item'] == item_title_sim]\n",
            "        \n",
            "#         if not user_record.empty:\n",
            "#             similarity = 1 - dist\n",
            "#             pred_numerator += similarity * user_record['rating'].values[0]\n",
            "#             pred_denominator += similarity\n",
            "            \n",
            "#     if pred_denominator == 0: return df['rating'].mean()\n",
            "#     return pred_numerator / pred_denominator\n",
            "\n",
            "# test_item = df.iloc[10]['item']\n",
            "# prediction = predict_rating_knn(example_user, test_item, df, item_feature_matrix, k=10)\n",
            "# print(f\"Predicted rating for '{test_item}': {prediction:.2f}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "48d43f47",
         "metadata": {},
         "source": [
            "### 6.2. Compare content-based and k-NN approaches\n",
            "\n",
            "| Feature | Content-Based (User Profiles) | k-NN (Item-Based) |\n",
            "| :--- | :--- | :--- |\n",
            "| **Core Concept** | Matches user's overall keyword profile to items. | Matches a specific item to its closest 'neighbors'. |\n",
            "| **Pros** | Great for new items (no ratings needed), explains *why* based on content. | Better at capturing subtle niche similarities text can't describe. |\n",
            "| **Cons** | Can be 'boring' (always recommends similar keywords). | Subject to cold-start (needs ratings to predict well). |\n",
            "| **Use Case** | Discovery based on specific interests/topics. | 'Users who liked this also liked...' logic. |\n",
            "\n",
            "**Summary**: In this implementation, the **Content-Based** approach is more robust because it can recommend items based on text features alone, whereas the **k-NN rating prediction** relies heavily on the user having rated very similar items in the sparse high-dimensional space."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "1025b3b5",
         "metadata": {},
         "source": [
            "## 7. Complete Numerical Example\n",
            "Step-by-step example using a tiny subset of 3 items."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "id": "c45c0159",
         "metadata": {},
         "outputs": [],
         "source": [
            "# print(\"--- Step 7.1: Sample Item Descriptions ---\")\n",
            "# mini_items = df['item'].unique()[:3]\n",
            "# print(mini_items)\n",
            "\n",
            "# print(\"\\n--- Step 7.2: TF-IDF Calculation (Sample 5 terms) ---\")\n",
            "# mini_tfidf = tfidf.transform(mini_items).toarray()\n",
            "# print(pd.DataFrame(mini_tfidf[:, :5], columns=tfidf.get_feature_names_out()[:5], index=mini_items))\n",
            "\n",
            "# print(\"\\n--- Step 7.3: User Profile (from 3 items with ratings 5, 4, 3) ---\")\n",
            "# user_ratings_val = np.array([5, 4, 3])\n",
            "# user_mini_profile = np.average(mini_tfidf, axis=0, weights=user_ratings_val)\n",
            "# print(f\"User Profile Vector (first 5 terms): {user_mini_profile[:5]}\")\n",
            "\n",
            "# print(\"\\n--- Step 7.4: Similarity Scores ---\")\n",
            "# mini_scores = cosine_similarity(user_mini_profile.reshape(1, -1), mini_tfidf).flatten()\n",
            "# print(pd.Series(mini_scores, index=mini_items))\n",
            "\n",
            "# print(\"\\n--- Step 7.5: Top-5 Recommendations ---\")\n",
            "# print(get_recommendations(example_user, user_profiles, item_feature_matrix, df, top_n=5))"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.3"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}