{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a969c9",
   "metadata": {},
   "source": [
    "# Group 17: \n",
    "### Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bba9a",
   "metadata": {},
   "source": [
    "# 1- meta_data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4cbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7047ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- CONFIGURATION ---\n",
    "# # Raw string for Windows path\n",
    "# FILE_PATH = r\"C:\\Users\\hadye\\Downloads\\meta_Health_and_Household.jsonl\\meta_Health_and_Household.jsonl\"\n",
    "# OUTPUT_FILE = \"health_household_processed.csv\"\n",
    "# CHUNK_SIZE = 10000\n",
    "\n",
    "# # Logic A: Medical Exclusion Keywords (Strict Filter)\n",
    "# # We exclude items if ANY of these terms appear in their categories\n",
    "# EXCLUDED_CATEGORIES = {\n",
    "#     'medication', 'pills', 'vitamins', 'supplements', 'drugs', 'pharmacy', \n",
    "#     'medicine', 'prescription', 'otc', 'capsules', 'tablets'\n",
    "# }\n",
    "\n",
    "# # Logic B: Green Lift Keywords (Sustainability Signals)\n",
    "# # We verify items as green if they contain ANY of these keywords in their text blob\n",
    "# GREEN_KEYWORDS = [\n",
    "#     'climate pledge friendly', 'compact by design', 'biodegradable', 'epa safer choice', \n",
    "#     'rechargeable', 'reusable', 'compostable', 'plastic free', 'zero waste', \n",
    "#     'organic', 'energy star', 'bpa free', 'no butane', 'sustainable', 'ecologo', \n",
    "#     'cradle to cradle', 'made in green', 'gots certified'\n",
    "# ]\n",
    "\n",
    "# def is_green_product(text_blob):\n",
    "#     \"\"\"Checks if any green keyword exists in the text blob.\"\"\"\n",
    "#     if not isinstance(text_blob, str):\n",
    "#         return 0\n",
    "#     for keyword in GREEN_KEYWORDS:\n",
    "#         if keyword in text_blob:\n",
    "#             return 1\n",
    "#     return 0\n",
    "\n",
    "# def extract_image_url(images_list):\n",
    "#     \"\"\"Extracts the best available image URL (hi_res > large > thumb).\"\"\"\n",
    "#     if isinstance(images_list, list) and len(images_list) > 0:\n",
    "#         first_image = images_list[0]\n",
    "#         # Prioritize high resolution, then large, then thumb\n",
    "#         if isinstance(first_image, dict):\n",
    "#             return first_image.get('hi_res') or first_image.get('large') or first_image.get('thumb')\n",
    "#     return None\n",
    "\n",
    "# def flatten_details(details_dict):\n",
    "#     \"\"\"Flattens the details dictionary into a string for TF-IDF.\"\"\"\n",
    "#     if isinstance(details_dict, dict):\n",
    "#         # Convert {\"Scent\": \"Lavender\", \"Brand\": \"X\"} -> \"Scent Lavender Brand X\"\n",
    "#         return \" \".join([f\"{k} {v}\" for k, v in details_dict.items() if isinstance(v, str)])\n",
    "#     return \"\"\n",
    "\n",
    "# def process_chunk(chunk):\n",
    "#     \"\"\"Clean and process a single chunk of data.\"\"\"\n",
    "#     # 1. Feature Extraction & Text Normalization\n",
    "    \n",
    "#     # Text Fields\n",
    "#     chunk['features_str'] = chunk['features'].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\")\n",
    "#     chunk['description_str'] = chunk['description'].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\")\n",
    "#     chunk['details_str'] = chunk['details'].apply(flatten_details)\n",
    "    \n",
    "#     # Combined Text Blob for TF-IDF (Key for Content-Based Filtering)\n",
    "#     chunk['full_text'] = (\n",
    "#         chunk['title'].fillna('') + \" \" + \n",
    "#         chunk['features_str'] + \" \" + \n",
    "#         chunk['description_str'] + \" \" + \n",
    "#         chunk['details_str']\n",
    "#     ).str.lower()\n",
    "    \n",
    "#     # New Features\n",
    "#     chunk['image_url'] = chunk['images'].apply(extract_image_url)\n",
    "#     chunk['price'] = chunk['price'].astype(str).replace('nan', '') # Keep original string format like \"$19.99\"\n",
    "#     chunk['store'] = chunk['store'].fillna('')\n",
    "    \n",
    "#     # 2. Logic B: Green Lift\n",
    "#     chunk['is_green'] = chunk['full_text'].apply(is_green_product)\n",
    "    \n",
    "#     # 3. Logic A: Medical Exclusion Filter\n",
    "#     # Create a string representation of categories for searching\n",
    "#     chunk['categories_str'] = chunk['categories'].apply(lambda x: \" \".join(x).lower() if isinstance(x, list) else \"\")\n",
    "    \n",
    "#     # Identify excluded rows\n",
    "#     def has_excluded_term(cat_str):\n",
    "#         if not cat_str: return False\n",
    "#         for term in EXCLUDED_CATEGORIES:\n",
    "#             if term in cat_str:\n",
    "#                 return True\n",
    "#         return False\n",
    "\n",
    "#     mask_exclude = chunk['categories_str'].apply(has_excluded_term)\n",
    "    \n",
    "#     # Filter: Keep non-excluded items\n",
    "#     clean_chunk = chunk[~mask_exclude].copy()\n",
    "    \n",
    "#     # Return only required columns\n",
    "#     cols_to_keep = ['parent_asin', 'title', 'average_rating', 'rating_number', 'is_green', 'price', 'store', 'image_url']\n",
    "    \n",
    "#     # Select columns if they exist to be safe\n",
    "#     existing_cols = [c for c in cols_to_keep if c in clean_chunk.columns]\n",
    "    \n",
    "#     return clean_chunk[existing_cols]\n",
    "\n",
    "# def main():\n",
    "#     print(f\"Starting processing of {FILE_PATH}...\")\n",
    "    \n",
    "#     total_processed_rows = 0\n",
    "#     total_green_items = 0\n",
    "#     chunk_count = 0\n",
    "    \n",
    "#     # Remove existing output file if it exists to start fresh\n",
    "#     if os.path.exists(OUTPUT_FILE):\n",
    "#         try:\n",
    "#             os.remove(OUTPUT_FILE)\n",
    "#             print(f\"Removed existing output file: {OUTPUT_FILE}\")\n",
    "#         except OSError as e:\n",
    "#             print(f\"Error removing file: {e}\")\n",
    "#             return\n",
    "\n",
    "#     try:\n",
    "#         # Stream processing with chunks\n",
    "#         # lines=True is required for JSONL\n",
    "#         with pd.read_json(FILE_PATH, lines=True, chunksize=CHUNK_SIZE) as reader:\n",
    "#             for chunk in reader:\n",
    "#                 chunk_count += 1\n",
    "                \n",
    "#                 # Process the chunk\n",
    "#                 processed_chunk = process_chunk(chunk)\n",
    "                \n",
    "#                 # Update stats\n",
    "#                 rows_in_chunk = len(processed_chunk)\n",
    "#                 green_in_chunk = processed_chunk['is_green'].sum()\n",
    "                \n",
    "#                 total_processed_rows += rows_in_chunk\n",
    "#                 total_green_items += green_in_chunk\n",
    "                \n",
    "#                 # Save to CSV (Append mode)\n",
    "#                 # Write header only for the first chunk\n",
    "#                 header_mode = True if chunk_count == 1 else False\n",
    "#                 processed_chunk.to_csv(OUTPUT_FILE, mode='a', header=header_mode, index=False)\n",
    "                \n",
    "#                 print(f\"Processed chunk {chunk_count}... (Kept {rows_in_chunk} items, {green_in_chunk} green)\")\n",
    "                \n",
    "#     except ValueError as e:\n",
    "#         print(f\"Error parsing JSON: {e}\")\n",
    "#         print(\"Make sure the file is a valid JSONL file.\")\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {FILE_PATH}\")\n",
    "#         print(\"Please check the path and try again.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#     else:\n",
    "#         print(\"\\n--- Processing Complete ---\")\n",
    "#         print(f\"Total clean rows saved: {total_processed_rows}\")\n",
    "#         print(f\"Total Green items found: {total_green_items}\")\n",
    "#         if total_processed_rows > 0:\n",
    "#             print(f\"Green Item Percentage: {(total_green_items/total_processed_rows)*100:.2f}%\")\n",
    "#         print(f\"Saved to: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32756e2e",
   "metadata": {},
   "source": [
    "# 2- reviews_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abddc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- CONFIGURATION ---\n",
    "# PRODUCTS_FILE = \"health_household_processed.csv\"\n",
    "# REVIEWS_FILE = r\"C:\\Users\\hadye\\Downloads\\meta_Health_and_Household.jsonl\\Health_and_Household.jsonl\"\n",
    "# OUTPUT_FILE = \"reviews_processed.csv\"\n",
    "# CHUNK_SIZE = 50000 \n",
    "\n",
    "# def load_valid_asins(products_path):\n",
    "#     \"\"\"Loads a set of valid parent_asins from the processed products file.\"\"\"\n",
    "#     print(f\"Loading valid products from {products_path}...\")\n",
    "#     try:\n",
    "#         # We only need the parent_asin column\n",
    "#         df = pd.read_csv(products_path, usecols=['parent_asin'])\n",
    "#         valid_asins = set(df['parent_asin'].unique())\n",
    "#         print(f\"Loaded {len(valid_asins)} valid unique ASINs.\")\n",
    "#         return valid_asins\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: Products file not found at {products_path}\")\n",
    "#         return set()\n",
    "\n",
    "# def main():\n",
    "#     valid_asins = load_valid_asins(PRODUCTS_FILE)\n",
    "    \n",
    "#     if not valid_asins:\n",
    "#         print(\"No valid ASINs found. Aborting.\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Starting processing of reviews from {REVIEWS_FILE}...\")\n",
    "    \n",
    "#     # Remove existing output file\n",
    "#     if os.path.exists(OUTPUT_FILE):\n",
    "#         try:\n",
    "#             os.remove(OUTPUT_FILE)\n",
    "#             print(f\"Removed existing output file: {OUTPUT_FILE}\")\n",
    "#         except OSError:\n",
    "#             pass\n",
    "\n",
    "#     total_reviews = 0\n",
    "#     kept_reviews = 0\n",
    "#     chunk_count = 0\n",
    "    \n",
    "#     try:\n",
    "#         # Stream processing\n",
    "#         with pd.read_json(REVIEWS_FILE, lines=True, chunksize=CHUNK_SIZE) as reader:\n",
    "#             for chunk in reader:\n",
    "#                 chunk_count += 1\n",
    "                \n",
    "#                 # Filter: Keep reviews where parent_asin is in our valid set\n",
    "#                 # Note: Reviews file has 'parent_asin' or 'asin'? \n",
    "#                 # According to inspection, keys are ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'verified_purchase', 'helpful_vote']\n",
    "#                 # We use 'parent_asin' to join.\n",
    "                \n",
    "#                 # Ensure parent_asin is loaded\n",
    "#                 if 'parent_asin' not in chunk.columns:\n",
    "#                      print(\"Warning: 'parent_asin' column missing in chunk.\")\n",
    "#                      continue\n",
    "                \n",
    "#                 mask = chunk['parent_asin'].isin(valid_asins)\n",
    "#                 filtered_chunk = chunk[mask]\n",
    "                \n",
    "#                 rows_in_chunk = len(chunk)\n",
    "#                 rows_kept = len(filtered_chunk)\n",
    "                \n",
    "#                 total_reviews += rows_in_chunk\n",
    "#                 kept_reviews += rows_kept\n",
    "                \n",
    "#                 if rows_kept > 0:\n",
    "#                     # Select useful columns\n",
    "#                     cols = ['parent_asin', 'user_id', 'rating', 'title', 'text', 'timestamp', 'verified_purchase', 'helpful_vote']\n",
    "#                     existing_cols = [c for c in cols if c in filtered_chunk.columns]\n",
    "                    \n",
    "#                     header_mode = True if (chunk_count == 1) or (kept_reviews == rows_kept) else False \n",
    "#                     # Actually, header logic with chunking/skipping is tricky. \n",
    "#                     # Better: write header only if file is new.\n",
    "#                     write_header = not os.path.exists(OUTPUT_FILE)\n",
    "                    \n",
    "#                     filtered_chunk[existing_cols].to_csv(OUTPUT_FILE, mode='a', header=write_header, index=False)\n",
    "                \n",
    "#                 print(f\"Processed chunk {chunk_count} (Rows: {rows_in_chunk}, Kept: {rows_kept})\")\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing reviews: {e}\")\n",
    "    \n",
    "#     print(\"\\n--- Reviews Processing Complete ---\")\n",
    "#     print(f\"Total reviews scanned: {total_reviews}\")\n",
    "#     print(f\"Total reviews saved: {kept_reviews}\")\n",
    "#     if total_reviews > 0:\n",
    "#         print(f\"Retention Rate: {(kept_reviews/total_reviews)*100:.2f}%\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86752d37",
   "metadata": {},
   "source": [
    "# 3- join_meta_and_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11288519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- CONFIGURATION ---\n",
    "# PRODUCTS_FILE = \"health_household_processed.csv\"\n",
    "# REVIEWS_FILE = \"reviews_processed.csv\"\n",
    "# OUTPUT_FILE = \"merged_dataset.csv\"\n",
    "\n",
    "# def main():\n",
    "#     print(\"Starting dataset merge...\")\n",
    "    \n",
    "#     # Check if files exist\n",
    "#     if not os.path.exists(PRODUCTS_FILE) or not os.path.exists(REVIEWS_FILE):\n",
    "#         print(f\"Error: Missing input files.\\nProducts: {os.path.exists(PRODUCTS_FILE)}\\nReviews: {os.path.exists(REVIEWS_FILE)}\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         print(f\"Loading {PRODUCTS_FILE}...\")\n",
    "#         products = pd.read_csv(PRODUCTS_FILE)\n",
    "#         print(f\"Loaded {len(products)} products.\")\n",
    "        \n",
    "#         print(f\"Loading {REVIEWS_FILE}...\")\n",
    "#         # Reviews might be large, but let's try loading it as we filtered it significantly\n",
    "#         reviews = pd.read_csv(REVIEWS_FILE)\n",
    "#         print(f\"Loaded {len(reviews)} reviews.\")\n",
    "        \n",
    "#         print(\"Merging datasets on 'parent_asin'...\")\n",
    "#         # Inner join to keep only reviews for products we have (and vice-versa, though reviews were already filtered)\n",
    "#         merged_df = pd.merge(reviews, products, on='parent_asin', how='inner')\n",
    "        \n",
    "#         print(f\"Merge complete. Resulting shape: {merged_df.shape}\")\n",
    "        \n",
    "#         print(f\"Saving to {OUTPUT_FILE}...\")\n",
    "#         merged_df.to_csv(OUTPUT_FILE, index=False)\n",
    "#         print(\"Success! Merged dataset saved.\")\n",
    "        \n",
    "#         # Display sample\n",
    "#         print(\"\\nFirst 5 rows:\")\n",
    "#         print(merged_df.head())\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred during merging: {e}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a071e9",
   "metadata": {},
   "source": [
    "# 4- Amazon_health&household Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c1bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"../data/Amazon_health&household.csv\"\n",
    "OUTPUT_FILE = \"../data/Amazon_health&household_sampled.csv\"\n",
    "\n",
    "N_ITEMS = 1000\n",
    "N_USERS = 10000\n",
    "N_RATINGS = 100000\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c558ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Sampling...\n",
      "Input File: e:\\AIE425_FinalProject_Group17\\SECTION2_DomainRecommender\\data\\Amazon_health&household.csv\n",
      "Loading data...\n",
      "Loaded Data Shape: (7751115, 15)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting Data Sampling...\")\n",
    "\n",
    "# Check if file exists relative to the notebook\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"ERROR: Input file not found at {os.path.abspath(INPUT_FILE)}\")\n",
    "else:\n",
    "    print(f\"Input File: {os.path.abspath(INPUT_FILE)}\")\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_FILE, dtype={'price': str}, low_memory=False)\n",
    "        print(f\"Loaded Data Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e8bfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming 'title_y' to 'item'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>title_x</th>\n",
       "      <th>text</th>\n",
       "      <th>item</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>is_green</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Yarn storage. Excellent.</td>\n",
       "      <td>I use these to store yarn projects.  I’m very ...</td>\n",
       "      <td>Hefty Slider Jumbo Storage Bags, 2.5 Gallon Si...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Smells great!</td>\n",
       "      <td>Smells great!</td>\n",
       "      <td>Febreze AIR Effects Air Freshener Heavy Duty C...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent!</td>\n",
       "      <td>Where has this been all my life?  Makes wrappi...</td>\n",
       "      <td>Glad Press'n Seal Plastic Food Wrap - 70 Squar...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>1</td>\n",
       "      <td>Yuck!</td>\n",
       "      <td>Amazon was out of my regular brand of Charmin ...</td>\n",
       "      <td>Cottonelle Ultra ComfortCare Soft Toilet Paper...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>29.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>5</td>\n",
       "      <td>only brand I use</td>\n",
       "      <td>Like using a cloud on your nether regions.  I ...</td>\n",
       "      <td>Charmin Ultra Gentle Toilet Paper, 18 Mega Rol...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1</td>\n",
       "      <td>19.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id  rating                   title_x  \\\n",
       "0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       5  Yarn storage. Excellent.   \n",
       "1  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       5             Smells great!   \n",
       "2  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       5                Excellent!   \n",
       "3  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       1                     Yuck!   \n",
       "4  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       5          only brand I use   \n",
       "\n",
       "                                                text  \\\n",
       "0  I use these to store yarn projects.  I’m very ...   \n",
       "1                                      Smells great!   \n",
       "2  Where has this been all my life?  Makes wrappi...   \n",
       "3  Amazon was out of my regular brand of Charmin ...   \n",
       "4  Like using a cloud on your nether regions.  I ...   \n",
       "\n",
       "                                                item  average_rating  \\\n",
       "0  Hefty Slider Jumbo Storage Bags, 2.5 Gallon Si...             4.8   \n",
       "1  Febreze AIR Effects Air Freshener Heavy Duty C...             4.6   \n",
       "2  Glad Press'n Seal Plastic Food Wrap - 70 Squar...             4.7   \n",
       "3  Cottonelle Ultra ComfortCare Soft Toilet Paper...             4.5   \n",
       "4  Charmin Ultra Gentle Toilet Paper, 18 Mega Rol...             4.7   \n",
       "\n",
       "   is_green  price  \n",
       "0         1   4.39  \n",
       "1         0    NaN  \n",
       "2         1   3.32  \n",
       "3         0  29.75  \n",
       "4         1  19.97  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unused columns\n",
    "columns_to_drop = [\"timestamp\", \"image_url\", \"store\", \"rating_number\", \"helpful_vote\", \"verified_purchase\", \"parent_asin\"]\n",
    "df.drop(columns=[c for c in columns_to_drop if c in df.columns], axis=1, inplace=True)\n",
    "\n",
    "# Clean up column names (strip whitespace)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Ensure 'item' column exists\n",
    "if 'title_y' in df.columns and 'item' not in df.columns:\n",
    "    print(\"Renaming 'title_y' to 'item'...\")\n",
    "    df.rename(columns={'title_y': 'item'}, inplace=True)\n",
    "\n",
    "if 'item' not in df.columns:\n",
    "     print(\"Warning: 'item' column not found. Checking for 'parent_asin'...\")\n",
    "     if 'parent_asin' in df.columns:\n",
    "         df['item'] = df['parent_asin']\n",
    "     else:\n",
    "         print(\"Error: Could not identify item column.\")\n",
    "\n",
    "# Preview the cleaned data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfaefe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting Top 1000 Items...\n",
      "Rows after Item Filter: 2045696\n"
     ]
    }
   ],
   "source": [
    "print(f\"Selecting Top {N_ITEMS} Items...\")\n",
    "\n",
    "top_items = df['item'].value_counts().nlargest(N_ITEMS).index\n",
    "df_filtered = df[df['item'].isin(top_items)]\n",
    "\n",
    "print(f\"Rows after Item Filter: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6102c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting Random 10000 Users (Seed 42)...\n",
      "Rows after User Filter: 14554\n"
     ]
    }
   ],
   "source": [
    "print(f\"Selecting Random {N_USERS} Users (Seed {SEED})...\")\n",
    "np.random.seed(SEED)\n",
    "\n",
    "available_users = df_filtered['user_id'].unique()\n",
    "\n",
    "if len(available_users) > N_USERS:\n",
    "    selected_users = np.random.choice(available_users, N_USERS, replace=False)\n",
    "    df_filtered = df_filtered[df_filtered['user_id'].isin(selected_users)]\n",
    "else:\n",
    "    print(f\"Note: Only {len(available_users)} users available (requested {N_USERS}). keeping all.\")\n",
    "\n",
    "print(f\"Rows after User Filter: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01940981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100000 Interactions (Seed 42)...\n",
      "Note: Only 14554 interactions available (requested 100000). keeping all.\n",
      "Current Shape: (14554, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sampling {N_RATINGS} Interactions (Seed {SEED})...\")\n",
    "\n",
    "if len(df_filtered) > N_RATINGS:\n",
    "    df_filtered = df_filtered.sample(n=N_RATINGS, random_state=SEED)\n",
    "else:\n",
    "     print(f\"Note: Only {len(df_filtered)} interactions available (requested {N_RATINGS}). keeping all.\")\n",
    "     \n",
    "print(f\"Current Shape: {df_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193d1420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape: (14554, 8)\n",
      "Unique Users: 10000\n",
      "Unique Items: 1000\n",
      "Saved sampled dataset to: e:\\AIE425_FinalProject_Group17\\SECTION2_DomainRecommender\\data\\Amazon_health&household_sampled.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Shape: {df_filtered.shape}\")\n",
    "print(f\"Unique Users: {df_filtered['user_id'].nunique()}\")\n",
    "print(f\"Unique Items: {df_filtered['item'].nunique()}\")\n",
    "\n",
    "try:\n",
    "    df_filtered.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Saved sampled dataset to: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c3711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Label Encoding...\n",
      "Encoded 1000 unique items.\n",
      "Example Mapping:\n",
      "  (20 Pack) Extra Large Eraser Sponge - Extra Thick, Long Lasting, Premium Melamine Sponges in Bulk - Multi Surface Power Scrubber Foam Cleaning Pads - Bathtub, Floor, Baseboard, Bathroom, Wall Cleaner -> 0\n",
      "  1 BY ONE Scale for Body Weight, Smart Body Fat Scale, Digital Bathroom Weighing Scale with Water Percentage Muscle Mass Bluetooth BMI, 14 Body Composition Analyzer, 400 lb -> 1\n",
      "  1 Pair Happystep Comfort Memory Foam Insoles, Orthotic PU Shoe Inserts, Arch Support, Heel Cushioning, Shock Absorption, Plantar Fasciitis Foot Pain Relief for Men and Women (US W: 5-6) -> 2\n",
      "  1 X 10 Energizer 377 376 Watch Battery SR626SW SR626W -> 3\n",
      "  100% Compostable 9 Inch Paper Plates [125-Pack] Heavy-Dut, Natural Disposable Bagasse Plate, Eco-Friendly Made of Sugarcane Fibers - Natural Unbleached Brown 9\" Biodegradable Plate by Stack Man -> 4\n",
      "\n",
      "Saved compact dataset to: e:\\AIE425_FinalProject_Group17\\SECTION2_DomainRecommender\\data\\Amazon_health&household_label_encoded.csv\n",
      "File preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item</th>\n",
       "      <th>item_id_encoded</th>\n",
       "      <th>rating</th>\n",
       "      <th>price</th>\n",
       "      <th>text</th>\n",
       "      <th>is_green</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>AEV4DP5E3FJH6FHDLXUYQTDEQYCQ</td>\n",
       "      <td>Sonic Handheld Percussion Massage Gun - Deep T...</td>\n",
       "      <td>827</td>\n",
       "      <td>2</td>\n",
       "      <td>79.99</td>\n",
       "      <td>This product worked great when it worked, but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
       "      <td>DUDE Wipes On-The-Go Flushable Wet Wipes - 1 P...</td>\n",
       "      <td>255</td>\n",
       "      <td>5</td>\n",
       "      <td>6.48</td>\n",
       "      <td>These are amazing for travel or for keeping in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
       "      <td>Sleep Mask for Side Sleeper, 100% Blackout 3D ...</td>\n",
       "      <td>814</td>\n",
       "      <td>5</td>\n",
       "      <td>12.69</td>\n",
       "      <td>These are great! My other sleep mask pressed o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
       "      <td>Cottonelle Freshfeel Flushable Wet Wipes, Adul...</td>\n",
       "      <td>226</td>\n",
       "      <td>5</td>\n",
       "      <td>15.79</td>\n",
       "      <td>These are really good quality. Do not tear lik...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q</td>\n",
       "      <td>Silk Sleep Eye Mask for Men Women, Comfortable...</td>\n",
       "      <td>808</td>\n",
       "      <td>5</td>\n",
       "      <td>8.88</td>\n",
       "      <td>Great for travel. Super soft and silky. Has ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           user_id  \\\n",
       "1279  AEV4DP5E3FJH6FHDLXUYQTDEQYCQ   \n",
       "2706  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
       "2708  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
       "2709  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
       "2710  AEHWUHNTB5FX32HJ7UBOZ2WWUX3Q   \n",
       "\n",
       "                                                   item  item_id_encoded  \\\n",
       "1279  Sonic Handheld Percussion Massage Gun - Deep T...              827   \n",
       "2706  DUDE Wipes On-The-Go Flushable Wet Wipes - 1 P...              255   \n",
       "2708  Sleep Mask for Side Sleeper, 100% Blackout 3D ...              814   \n",
       "2709  Cottonelle Freshfeel Flushable Wet Wipes, Adul...              226   \n",
       "2710  Silk Sleep Eye Mask for Men Women, Comfortable...              808   \n",
       "\n",
       "      rating  price                                               text  \\\n",
       "1279       2  79.99  This product worked great when it worked, but ...   \n",
       "2706       5   6.48  These are amazing for travel or for keeping in...   \n",
       "2708       5  12.69  These are great! My other sleep mask pressed o...   \n",
       "2709       5  15.79  These are really good quality. Do not tear lik...   \n",
       "2710       5   8.88  Great for travel. Super soft and silky. Has ad...   \n",
       "\n",
       "      is_green  \n",
       "1279         1  \n",
       "2706         1  \n",
       "2708         0  \n",
       "2709         1  \n",
       "2710         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Applying Label Encoding...\")\n",
    "\n",
    "# 1. Initialize Encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# 2. Fit and Transform the 'item' column\n",
    "# This converts strings to numbers (0 to N_ITEMS-1)\n",
    "df_filtered['item_id_encoded'] = le.fit_transform(df_filtered['item'])\n",
    "\n",
    "# 3. Create a mapping dictionary (Optional, but good for reference)\n",
    "# So you know that 42 = \"Colgate Toothpaste\"\n",
    "item_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print(f\"Encoded {len(item_mapping)} unique items.\")\n",
    "print(\"Example Mapping:\")\n",
    "for k, v in list(item_mapping.items())[:5]:\n",
    "    print(f\"  {k} -> {v}\")\n",
    "\n",
    "# 4. Save the compact file\n",
    "# We save the encoded ID, but we can also keep the original 'item' name if you want reference\n",
    "OUTPUT_ENCODED_FILE = \"../data/Amazon_health&household_label_encoded.csv\"\n",
    "\n",
    "# Reorder columns to be cleaner\n",
    "cols = ['user_id', 'item', 'item_id_encoded', 'rating','price','text', 'is_green'] \n",
    "# Add any other columns you have (like 'timestamp') to this list if they exist\n",
    "\n",
    "df_filtered[cols].to_csv(OUTPUT_ENCODED_FILE, index=False)\n",
    "\n",
    "print(f\"\\nSaved compact dataset to: {os.path.abspath(OUTPUT_ENCODED_FILE)}\")\n",
    "print(f\"File preview:\")\n",
    "df_filtered[cols].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
