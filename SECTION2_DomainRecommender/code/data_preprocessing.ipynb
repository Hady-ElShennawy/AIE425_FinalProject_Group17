{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a969c9",
   "metadata": {},
   "source": [
    "# Group 17: \n",
    "### Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bba9a",
   "metadata": {},
   "source": [
    "# 1- meta_data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4cbec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder exists at: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION2_DomainRecommender\\results\n",
      "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION2_DomainRecommender\\results\\tables\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7047ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- CONFIGURATION ---\n",
    "# # Raw string for Windows path\n",
    "# FILE_PATH = r\"C:\\Users\\hadye\\Downloads\\meta_Health_and_Household.jsonl\\meta_Health_and_Household.jsonl\"\n",
    "# OUTPUT_FILE = \"health_household_processed.csv\"\n",
    "# CHUNK_SIZE = 10000\n",
    "\n",
    "# # Logic A: Medical Exclusion Keywords (Strict Filter)\n",
    "# # We exclude items if ANY of these terms appear in their categories\n",
    "# EXCLUDED_CATEGORIES = {\n",
    "#     'medication', 'pills', 'vitamins', 'supplements', 'drugs', 'pharmacy', \n",
    "#     'medicine', 'prescription', 'otc', 'capsules', 'tablets'\n",
    "# }\n",
    "\n",
    "# # Logic B: Green Lift Keywords (Sustainability Signals)\n",
    "# # We verify items as green if they contain ANY of these keywords in their text blob\n",
    "# GREEN_KEYWORDS = [\n",
    "#     'climate pledge friendly', 'compact by design', 'biodegradable', 'epa safer choice', \n",
    "#     'rechargeable', 'reusable', 'compostable', 'plastic free', 'zero waste', \n",
    "#     'organic', 'energy star', 'bpa free', 'no butane', 'sustainable', 'ecologo', \n",
    "#     'cradle to cradle', 'made in green', 'gots certified'\n",
    "# ]\n",
    "\n",
    "# def is_green_product(text_blob):\n",
    "#     \"\"\"Checks if any green keyword exists in the text blob.\"\"\"\n",
    "#     if not isinstance(text_blob, str):\n",
    "#         return 0\n",
    "#     for keyword in GREEN_KEYWORDS:\n",
    "#         if keyword in text_blob:\n",
    "#             return 1\n",
    "#     return 0\n",
    "\n",
    "# def extract_image_url(images_list):\n",
    "#     \"\"\"Extracts the best available image URL (hi_res > large > thumb).\"\"\"\n",
    "#     if isinstance(images_list, list) and len(images_list) > 0:\n",
    "#         first_image = images_list[0]\n",
    "#         # Prioritize high resolution, then large, then thumb\n",
    "#         if isinstance(first_image, dict):\n",
    "#             return first_image.get('hi_res') or first_image.get('large') or first_image.get('thumb')\n",
    "#     return None\n",
    "\n",
    "# def flatten_details(details_dict):\n",
    "#     \"\"\"Flattens the details dictionary into a string for TF-IDF.\"\"\"\n",
    "#     if isinstance(details_dict, dict):\n",
    "#         # Convert {\"Scent\": \"Lavender\", \"Brand\": \"X\"} -> \"Scent Lavender Brand X\"\n",
    "#         return \" \".join([f\"{k} {v}\" for k, v in details_dict.items() if isinstance(v, str)])\n",
    "#     return \"\"\n",
    "\n",
    "# def process_chunk(chunk):\n",
    "#     \"\"\"Clean and process a single chunk of data.\"\"\"\n",
    "#     # 1. Feature Extraction & Text Normalization\n",
    "    \n",
    "#     # Text Fields\n",
    "#     chunk['features_str'] = chunk['features'].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\")\n",
    "#     chunk['description_str'] = chunk['description'].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\")\n",
    "#     chunk['details_str'] = chunk['details'].apply(flatten_details)\n",
    "    \n",
    "#     # Combined Text Blob for TF-IDF (Key for Content-Based Filtering)\n",
    "#     chunk['full_text'] = (\n",
    "#         chunk['title'].fillna('') + \" \" + \n",
    "#         chunk['features_str'] + \" \" + \n",
    "#         chunk['description_str'] + \" \" + \n",
    "#         chunk['details_str']\n",
    "#     ).str.lower()\n",
    "    \n",
    "#     # New Features\n",
    "#     chunk['image_url'] = chunk['images'].apply(extract_image_url)\n",
    "#     chunk['price'] = chunk['price'].astype(str).replace('nan', '') # Keep original string format like \"$19.99\"\n",
    "#     chunk['store'] = chunk['store'].fillna('')\n",
    "    \n",
    "#     # 2. Logic B: Green Lift\n",
    "#     chunk['is_green'] = chunk['full_text'].apply(is_green_product)\n",
    "    \n",
    "#     # 3. Logic A: Medical Exclusion Filter\n",
    "#     # Create a string representation of categories for searching\n",
    "#     chunk['categories_str'] = chunk['categories'].apply(lambda x: \" \".join(x).lower() if isinstance(x, list) else \"\")\n",
    "    \n",
    "#     # Identify excluded rows\n",
    "#     def has_excluded_term(cat_str):\n",
    "#         if not cat_str: return False\n",
    "#         for term in EXCLUDED_CATEGORIES:\n",
    "#             if term in cat_str:\n",
    "#                 return True\n",
    "#         return False\n",
    "\n",
    "#     mask_exclude = chunk['categories_str'].apply(has_excluded_term)\n",
    "    \n",
    "#     # Filter: Keep non-excluded items\n",
    "#     clean_chunk = chunk[~mask_exclude].copy()\n",
    "    \n",
    "#     # Return only required columns\n",
    "#     cols_to_keep = ['parent_asin', 'title', 'average_rating', 'rating_number', 'is_green', 'price', 'store', 'image_url']\n",
    "    \n",
    "#     # Select columns if they exist to be safe\n",
    "#     existing_cols = [c for c in cols_to_keep if c in clean_chunk.columns]\n",
    "    \n",
    "#     return clean_chunk[existing_cols]\n",
    "\n",
    "# def main():\n",
    "#     print(f\"Starting processing of {FILE_PATH}...\")\n",
    "    \n",
    "#     total_processed_rows = 0\n",
    "#     total_green_items = 0\n",
    "#     chunk_count = 0\n",
    "    \n",
    "#     # Remove existing output file if it exists to start fresh\n",
    "#     if os.path.exists(OUTPUT_FILE):\n",
    "#         try:\n",
    "#             os.remove(OUTPUT_FILE)\n",
    "#             print(f\"Removed existing output file: {OUTPUT_FILE}\")\n",
    "#         except OSError as e:\n",
    "#             print(f\"Error removing file: {e}\")\n",
    "#             return\n",
    "\n",
    "#     try:\n",
    "#         # Stream processing with chunks\n",
    "#         # lines=True is required for JSONL\n",
    "#         with pd.read_json(FILE_PATH, lines=True, chunksize=CHUNK_SIZE) as reader:\n",
    "#             for chunk in reader:\n",
    "#                 chunk_count += 1\n",
    "                \n",
    "#                 # Process the chunk\n",
    "#                 processed_chunk = process_chunk(chunk)\n",
    "                \n",
    "#                 # Update stats\n",
    "#                 rows_in_chunk = len(processed_chunk)\n",
    "#                 green_in_chunk = processed_chunk['is_green'].sum()\n",
    "                \n",
    "#                 total_processed_rows += rows_in_chunk\n",
    "#                 total_green_items += green_in_chunk\n",
    "                \n",
    "#                 # Save to CSV (Append mode)\n",
    "#                 # Write header only for the first chunk\n",
    "#                 header_mode = True if chunk_count == 1 else False\n",
    "#                 processed_chunk.to_csv(OUTPUT_FILE, mode='a', header=header_mode, index=False)\n",
    "                \n",
    "#                 print(f\"Processed chunk {chunk_count}... (Kept {rows_in_chunk} items, {green_in_chunk} green)\")\n",
    "                \n",
    "#     except ValueError as e:\n",
    "#         print(f\"Error parsing JSON: {e}\")\n",
    "#         print(\"Make sure the file is a valid JSONL file.\")\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {FILE_PATH}\")\n",
    "#         print(\"Please check the path and try again.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#     else:\n",
    "#         print(\"\\n--- Processing Complete ---\")\n",
    "#         print(f\"Total clean rows saved: {total_processed_rows}\")\n",
    "#         print(f\"Total Green items found: {total_green_items}\")\n",
    "#         if total_processed_rows > 0:\n",
    "#             print(f\"Green Item Percentage: {(total_green_items/total_processed_rows)*100:.2f}%\")\n",
    "#         print(f\"Saved to: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32756e2e",
   "metadata": {},
   "source": [
    "# 2- reviews_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abddc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- CONFIGURATION ---\n",
    "# PRODUCTS_FILE = \"health_household_processed.csv\"\n",
    "# REVIEWS_FILE = r\"C:\\Users\\hadye\\Downloads\\meta_Health_and_Household.jsonl\\Health_and_Household.jsonl\"\n",
    "# OUTPUT_FILE = \"reviews_processed.csv\"\n",
    "# CHUNK_SIZE = 50000 \n",
    "\n",
    "# def load_valid_asins(products_path):\n",
    "#     \"\"\"Loads a set of valid parent_asins from the processed products file.\"\"\"\n",
    "#     print(f\"Loading valid products from {products_path}...\")\n",
    "#     try:\n",
    "#         # We only need the parent_asin column\n",
    "#         df = pd.read_csv(products_path, usecols=['parent_asin'])\n",
    "#         valid_asins = set(df['parent_asin'].unique())\n",
    "#         print(f\"Loaded {len(valid_asins)} valid unique ASINs.\")\n",
    "#         return valid_asins\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: Products file not found at {products_path}\")\n",
    "#         return set()\n",
    "\n",
    "# def main():\n",
    "#     valid_asins = load_valid_asins(PRODUCTS_FILE)\n",
    "    \n",
    "#     if not valid_asins:\n",
    "#         print(\"No valid ASINs found. Aborting.\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Starting processing of reviews from {REVIEWS_FILE}...\")\n",
    "    \n",
    "#     # Remove existing output file\n",
    "#     if os.path.exists(OUTPUT_FILE):\n",
    "#         try:\n",
    "#             os.remove(OUTPUT_FILE)\n",
    "#             print(f\"Removed existing output file: {OUTPUT_FILE}\")\n",
    "#         except OSError:\n",
    "#             pass\n",
    "\n",
    "#     total_reviews = 0\n",
    "#     kept_reviews = 0\n",
    "#     chunk_count = 0\n",
    "    \n",
    "#     try:\n",
    "#         # Stream processing\n",
    "#         with pd.read_json(REVIEWS_FILE, lines=True, chunksize=CHUNK_SIZE) as reader:\n",
    "#             for chunk in reader:\n",
    "#                 chunk_count += 1\n",
    "                \n",
    "#                 # Filter: Keep reviews where parent_asin is in our valid set\n",
    "#                 # Note: Reviews file has 'parent_asin' or 'asin'? \n",
    "#                 # According to inspection, keys are ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'verified_purchase', 'helpful_vote']\n",
    "#                 # We use 'parent_asin' to join.\n",
    "                \n",
    "#                 # Ensure parent_asin is loaded\n",
    "#                 if 'parent_asin' not in chunk.columns:\n",
    "#                      print(\"Warning: 'parent_asin' column missing in chunk.\")\n",
    "#                      continue\n",
    "                \n",
    "#                 mask = chunk['parent_asin'].isin(valid_asins)\n",
    "#                 filtered_chunk = chunk[mask]\n",
    "                \n",
    "#                 rows_in_chunk = len(chunk)\n",
    "#                 rows_kept = len(filtered_chunk)\n",
    "                \n",
    "#                 total_reviews += rows_in_chunk\n",
    "#                 kept_reviews += rows_kept\n",
    "                \n",
    "#                 if rows_kept > 0:\n",
    "#                     # Select useful columns\n",
    "#                     cols = ['parent_asin', 'user_id', 'rating', 'title', 'text', 'timestamp', 'verified_purchase', 'helpful_vote']\n",
    "#                     existing_cols = [c for c in cols if c in filtered_chunk.columns]\n",
    "                    \n",
    "#                     header_mode = True if (chunk_count == 1) or (kept_reviews == rows_kept) else False \n",
    "#                     # Actually, header logic with chunking/skipping is tricky. \n",
    "#                     # Better: write header only if file is new.\n",
    "#                     write_header = not os.path.exists(OUTPUT_FILE)\n",
    "                    \n",
    "#                     filtered_chunk[existing_cols].to_csv(OUTPUT_FILE, mode='a', header=write_header, index=False)\n",
    "                \n",
    "#                 print(f\"Processed chunk {chunk_count} (Rows: {rows_in_chunk}, Kept: {rows_kept})\")\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing reviews: {e}\")\n",
    "    \n",
    "#     print(\"\\n--- Reviews Processing Complete ---\")\n",
    "#     print(f\"Total reviews scanned: {total_reviews}\")\n",
    "#     print(f\"Total reviews saved: {kept_reviews}\")\n",
    "#     if total_reviews > 0:\n",
    "#         print(f\"Retention Rate: {(kept_reviews/total_reviews)*100:.2f}%\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86752d37",
   "metadata": {},
   "source": [
    "# 3- join_meta_and_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11288519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- CONFIGURATION ---\n",
    "# PRODUCTS_FILE = \"health_household_processed.csv\"\n",
    "# REVIEWS_FILE = \"reviews_processed.csv\"\n",
    "# OUTPUT_FILE = \"merged_dataset.csv\"\n",
    "\n",
    "# def main():\n",
    "#     print(\"Starting dataset merge...\")\n",
    "    \n",
    "#     # Check if files exist\n",
    "#     if not os.path.exists(PRODUCTS_FILE) or not os.path.exists(REVIEWS_FILE):\n",
    "#         print(f\"Error: Missing input files.\\nProducts: {os.path.exists(PRODUCTS_FILE)}\\nReviews: {os.path.exists(REVIEWS_FILE)}\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         print(f\"Loading {PRODUCTS_FILE}...\")\n",
    "#         products = pd.read_csv(PRODUCTS_FILE)\n",
    "#         print(f\"Loaded {len(products)} products.\")\n",
    "        \n",
    "#         print(f\"Loading {REVIEWS_FILE}...\")\n",
    "#         # Reviews might be large, but let's try loading it as we filtered it significantly\n",
    "#         reviews = pd.read_csv(REVIEWS_FILE)\n",
    "#         print(f\"Loaded {len(reviews)} reviews.\")\n",
    "        \n",
    "#         print(\"Merging datasets on 'parent_asin'...\")\n",
    "#         # Inner join to keep only reviews for products we have (and vice-versa, though reviews were already filtered)\n",
    "#         merged_df = pd.merge(reviews, products, on='parent_asin', how='inner')\n",
    "        \n",
    "#         print(f\"Merge complete. Resulting shape: {merged_df.shape}\")\n",
    "        \n",
    "#         print(f\"Saving to {OUTPUT_FILE}...\")\n",
    "#         merged_df.to_csv(OUTPUT_FILE, index=False)\n",
    "#         print(\"Success! Merged dataset saved.\")\n",
    "        \n",
    "#         # Display sample\n",
    "#         print(\"\\nFirst 5 rows:\")\n",
    "#         print(merged_df.head())\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred during merging: {e}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a071e9",
   "metadata": {},
   "source": [
    "# 4- Amazon_health&household Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c1bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input path set to: ..\\data\\Amazon_health&household.csv\n"
     ]
    }
   ],
   "source": [
    "# Define paths relative to the current working directory\n",
    "# Assuming the structure is:\n",
    "# project/\n",
    "# ├── data/\n",
    "# │   └── Amazon_health&household.csv\n",
    "# └── notebooks/\n",
    "#     └── this_notebook.ipynb\n",
    "\n",
    "# '..' moves one directory up from where this notebook is saved\n",
    "RANDOM_SEED = 42\n",
    "data_dir = os.path.join('..', 'data') \n",
    "\n",
    "input_file = os.path.join(data_dir, 'Amazon_health&household.csv')\n",
    "preprocessed_file = os.path.join(data_dir, 'Amazon_health&household_preprocessed.csv')\n",
    "sampled_file = os.path.join(data_dir, 'Amazon_health&household_label_encoded.csv')\n",
    "\n",
    "print(f\"Input path set to: {input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c558ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ..\\data\\Amazon_health&household.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hadye\\AppData\\Local\\Temp\\ipykernel_21476\\2440301609.py:4: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Initial shape: (7751115, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>title_x</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>title_y</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>is_green</th>\n",
       "      <th>price</th>\n",
       "      <th>store</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0C85LM3CK</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Yarn storage. Excellent.</td>\n",
       "      <td>I use these to store yarn projects.  I’m very ...</td>\n",
       "      <td>2020-05-20 00:19:14.431</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Hefty Slider Jumbo Storage Bags, 2.5 Gallon Si...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>22780</td>\n",
       "      <td>1</td>\n",
       "      <td>4.39</td>\n",
       "      <td>Hefty</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71fQiCJcrj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B01MT5XV33</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Smells great!</td>\n",
       "      <td>Smells great!</td>\n",
       "      <td>2020-02-09 02:36:22.965</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Febreze AIR Effects Air Freshener Heavy Duty C...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>659</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Febreze</td>\n",
       "      <td>https://m.media-amazon.com/images/I/817x7hr2Zk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0CBQN1MCY</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent!</td>\n",
       "      <td>Where has this been all my life?  Makes wrappi...</td>\n",
       "      <td>2020-02-06 00:43:27.541</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Glad Press'n Seal Plastic Food Wrap - 70 Squar...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>12349</td>\n",
       "      <td>1</td>\n",
       "      <td>3.32</td>\n",
       "      <td>GLAD</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61P0BHuoS2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0CBWPKH5K</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>1</td>\n",
       "      <td>Yuck!</td>\n",
       "      <td>Amazon was out of my regular brand of Charmin ...</td>\n",
       "      <td>2019-06-26 03:26:44.596</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>Cottonelle Ultra ComfortCare Soft Toilet Paper...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>28254</td>\n",
       "      <td>0</td>\n",
       "      <td>29.75</td>\n",
       "      <td>Cottonelle</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81wbtrbCB-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0BV1PP5LQ</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>5</td>\n",
       "      <td>only brand I use</td>\n",
       "      <td>Like using a cloud on your nether regions.  I ...</td>\n",
       "      <td>2019-03-18 03:02:30.254</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Charmin Ultra Gentle Toilet Paper, 18 Mega Rol...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>43453</td>\n",
       "      <td>1</td>\n",
       "      <td>19.97</td>\n",
       "      <td>Charmin</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81lm8f9Y5t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parent_asin                       user_id  rating                   title_x  \\\n",
       "0  B0C85LM3CK  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       5  Yarn storage. Excellent.   \n",
       "1  B01MT5XV33  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       5             Smells great!   \n",
       "2  B0CBQN1MCY  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       5                Excellent!   \n",
       "3  B0CBWPKH5K  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       1                     Yuck!   \n",
       "4  B0BV1PP5LQ  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ       5          only brand I use   \n",
       "\n",
       "                                                text                timestamp  \\\n",
       "0  I use these to store yarn projects.  I’m very ...  2020-05-20 00:19:14.431   \n",
       "1                                      Smells great!  2020-02-09 02:36:22.965   \n",
       "2  Where has this been all my life?  Makes wrappi...  2020-02-06 00:43:27.541   \n",
       "3  Amazon was out of my regular brand of Charmin ...  2019-06-26 03:26:44.596   \n",
       "4  Like using a cloud on your nether regions.  I ...  2019-03-18 03:02:30.254   \n",
       "\n",
       "   verified_purchase  helpful_vote  \\\n",
       "0               True             0   \n",
       "1               True             0   \n",
       "2               True             0   \n",
       "3               True             3   \n",
       "4               True             0   \n",
       "\n",
       "                                             title_y  average_rating  \\\n",
       "0  Hefty Slider Jumbo Storage Bags, 2.5 Gallon Si...             4.8   \n",
       "1  Febreze AIR Effects Air Freshener Heavy Duty C...             4.6   \n",
       "2  Glad Press'n Seal Plastic Food Wrap - 70 Squar...             4.7   \n",
       "3  Cottonelle Ultra ComfortCare Soft Toilet Paper...             4.5   \n",
       "4  Charmin Ultra Gentle Toilet Paper, 18 Mega Rol...             4.7   \n",
       "\n",
       "   rating_number  is_green  price       store  \\\n",
       "0          22780         1   4.39       Hefty   \n",
       "1            659         0    NaN     Febreze   \n",
       "2          12349         1   3.32        GLAD   \n",
       "3          28254         0  29.75  Cottonelle   \n",
       "4          43453         1  19.97     Charmin   \n",
       "\n",
       "                                           image_url  \n",
       "0  https://m.media-amazon.com/images/I/71fQiCJcrj...  \n",
       "1  https://m.media-amazon.com/images/I/817x7hr2Zk...  \n",
       "2  https://m.media-amazon.com/images/I/61P0BHuoS2...  \n",
       "3  https://m.media-amazon.com/images/I/81wbtrbCB-...  \n",
       "4  https://m.media-amazon.com/images/I/81lm8f9Y5t...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Loading data from {input_file}...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(\"Initial shape:\", df.shape)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {input_file}. Please check your path.\")\n",
    "\n",
    "# Display the first few rows to inspect data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e8bfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "Filled missing ratings with mean: 4.26\n",
      "Filled missing prices with mean: 26.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rating    0\n",
       "price     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Handling missing values...\")\n",
    "\n",
    "# 1. Handle Rating\n",
    "if 'rating' in df.columns:\n",
    "    df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "    rating_mean = df['rating'].mean()\n",
    "    df['rating'] = df['rating'].fillna(rating_mean)\n",
    "    print(f\"Filled missing ratings with mean: {rating_mean:.2f}\")\n",
    "\n",
    "# 2. Handle Price\n",
    "if 'price' in df.columns:\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    price_mean = df['price'].mean()\n",
    "    df['price'] = df['price'].fillna(price_mean)\n",
    "    print(f\"Filled missing prices with mean: {price_mean:.2f}\")\n",
    "\n",
    "# Check if any nulls remain in these columns\n",
    "df[['rating', 'price']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfaefe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling duplicates...\n",
      "Dropped 33068 duplicates.\n",
      "Renaming columns...\n",
      "Renamed 'title_y' to 'item'\n"
     ]
    }
   ],
   "source": [
    "# Handle duplicates\n",
    "print(\"Handling duplicates...\")\n",
    "initial_len = len(df)\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Dropped {initial_len - len(df)} duplicates.\")\n",
    "\n",
    "# Rename columns\n",
    "print(\"Renaming columns...\")\n",
    "if 'title_y' in df.columns:\n",
    "    df.rename(columns={'title_y': 'item'}, inplace=True)\n",
    "    print(\"Renamed 'title_y' to 'item'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01940981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering columns...\n",
      "Columns after filtering: ['user_id', 'item', 'rating', 'is_green', 'price', 'text']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>is_green</th>\n",
       "      <th>price</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>Hefty Slider Jumbo Storage Bags, 2.5 Gallon Si...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4.390000</td>\n",
       "      <td>I use these to store yarn projects.  I’m very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>Febreze AIR Effects Air Freshener Heavy Duty C...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>26.948325</td>\n",
       "      <td>Smells great!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>Glad Press'n Seal Plastic Food Wrap - 70 Squar...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>Where has this been all my life?  Makes wrappi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>Cottonelle Ultra ComfortCare Soft Toilet Paper...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>Amazon was out of my regular brand of Charmin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>Charmin Ultra Gentle Toilet Paper, 18 Mega Rol...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>19.970000</td>\n",
       "      <td>Like using a cloud on your nether regions.  I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id  \\\n",
       "0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ   \n",
       "1  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ   \n",
       "2  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ   \n",
       "3  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ   \n",
       "4  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ   \n",
       "\n",
       "                                                item  rating  is_green  \\\n",
       "0  Hefty Slider Jumbo Storage Bags, 2.5 Gallon Si...       5         1   \n",
       "1  Febreze AIR Effects Air Freshener Heavy Duty C...       5         0   \n",
       "2  Glad Press'n Seal Plastic Food Wrap - 70 Squar...       5         1   \n",
       "3  Cottonelle Ultra ComfortCare Soft Toilet Paper...       1         0   \n",
       "4  Charmin Ultra Gentle Toilet Paper, 18 Mega Rol...       5         1   \n",
       "\n",
       "       price                                               text  \n",
       "0   4.390000  I use these to store yarn projects.  I’m very ...  \n",
       "1  26.948325                                      Smells great!  \n",
       "2   3.320000  Where has this been all my life?  Makes wrappi...  \n",
       "3  29.750000  Amazon was out of my regular brand of Charmin ...  \n",
       "4  19.970000  Like using a cloud on your nether regions.  I ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep: user_id, item_id, rating, is_green, price, text\n",
    "print(\"Filtering columns...\")\n",
    "\n",
    "cols_to_keep = ['user_id', 'item', 'rating', 'is_green', 'price', 'text']\n",
    "# Filter to only keep columns that actually exist in the dataframe\n",
    "existing_cols = [c for c in cols_to_keep if c in df.columns]\n",
    "\n",
    "df = df[existing_cols]\n",
    "print(\"Columns after filtering:\", df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "193d1420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Label Encoding on item...\n",
      "Encoding complete.\n",
      "Saving preprocessed data to ..\\data\\Amazon_health&household_preprocessed.csv...\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing Label Encoding on item...\")\n",
    "\n",
    "if 'item' in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    # Convert to string to ensure consistency before encoding\n",
    "    df['item'] = df['item'].astype(str)\n",
    "    df['item_id_encoded'] = le.fit_transform(df['item'])\n",
    "    print(\"Encoding complete.\")\n",
    "    \n",
    "    # Save the full preprocessed file\n",
    "    print(f\"Saving preprocessed data to {preprocessed_file}...\")\n",
    "    df.to_csv(preprocessed_file, index=False)\n",
    "else:\n",
    "    print(\"Warning: 'item' column not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c3711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n",
      "Final Sampled data shape: (83355, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Sampling data...\")\n",
    "\n",
    "if 'user_id' in df.columns and 'item_id_encoded' in df.columns:\n",
    "    # 1. Get top 10k users by frequency\n",
    "    top_users = df['user_id'].value_counts().nlargest(10000).index\n",
    "    df_sampled = df[df['user_id'].isin(top_users)]\n",
    "    \n",
    "    # 2. Get top 1k items by frequency within the user filtered data\n",
    "    top_items = df_sampled['item_id_encoded'].value_counts().nlargest(1000).index\n",
    "    df_sampled = df_sampled[df_sampled['item_id_encoded'].isin(top_items)]\n",
    "    \n",
    "    # 3. Sample 100k ratings if dataset is larger than that\n",
    "    if len(df_sampled) > 100000:\n",
    "        df_sampled = df_sampled.sample(n=100000, random_state=RANDOM_SEED)\n",
    "    \n",
    "    print(\"Final Sampled data shape:\", df_sampled.shape)\n",
    "    \n",
    "else:\n",
    "    print(\"Required columns for sampling (user_id, item_id) not found.\")\n",
    "    df_sampled = pd.DataFrame() # Create empty df to prevent errors in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6e7c934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sampled data to ..\\data\\Amazon_health&household_label_encoded.csv...\n",
      "Process completed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>is_green</th>\n",
       "      <th>price</th>\n",
       "      <th>text</th>\n",
       "      <th>item_id_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
       "      <td>Dawn Ultra Dishwashing Liquid, Original Scent ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22.77</td>\n",
       "      <td>Too expensive</td>\n",
       "      <td>107204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
       "      <td>Charmin Ultra Soft Cushiony Touch Toilet Paper...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>28.82</td>\n",
       "      <td>Very good paper but too expensiveoo</td>\n",
       "      <td>85026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
       "      <td>Viva Signature Cloth Choose-A-Sheet Paper Towe...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23.65</td>\n",
       "      <td>Expensive but great towels</td>\n",
       "      <td>358978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
       "      <td>Drano Max Gel Drain Clog Remover and Cleaner f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15.47</td>\n",
       "      <td>The old Draino worked better.  This pours easi...</td>\n",
       "      <td>117303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>AFNT6ZJCYQN3WDIKUSWHJDXNND2Q</td>\n",
       "      <td>LiCB A23 23A 12V Alkaline Battery (5-Pack)</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.99</td>\n",
       "      <td>I jut got them, don’t know yet if they last long</td>\n",
       "      <td>204987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          user_id  \\\n",
       "96   AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
       "102  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
       "103  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
       "106  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
       "107  AFNT6ZJCYQN3WDIKUSWHJDXNND2Q   \n",
       "\n",
       "                                                  item  rating  is_green  \\\n",
       "96   Dawn Ultra Dishwashing Liquid, Original Scent ...       4         0   \n",
       "102  Charmin Ultra Soft Cushiony Touch Toilet Paper...       4         0   \n",
       "103  Viva Signature Cloth Choose-A-Sheet Paper Towe...       4         0   \n",
       "106  Drano Max Gel Drain Clog Remover and Cleaner f...       2         0   \n",
       "107         LiCB A23 23A 12V Alkaline Battery (5-Pack)       4         0   \n",
       "\n",
       "     price                                               text  item_id_encoded  \n",
       "96   22.77                                      Too expensive           107204  \n",
       "102  28.82                Very good paper but too expensiveoo            85026  \n",
       "103  23.65                         Expensive but great towels           358978  \n",
       "106  15.47  The old Draino worked better.  This pours easi...           117303  \n",
       "107   5.99   I jut got them, don’t know yet if they last long           204987  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not df_sampled.empty:\n",
    "    print(f\"Saving sampled data to {sampled_file}...\")\n",
    "    df_sampled.to_csv(sampled_file, index=False)\n",
    "    print(\"Process completed successfully.\")\n",
    "else:\n",
    "    print(\"No data to save.\")\n",
    "\n",
    "df_sampled.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
