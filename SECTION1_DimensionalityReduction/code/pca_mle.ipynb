{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5657ee74",
   "metadata": {},
   "source": [
    "# Group 17: \n",
    "### Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c9972",
   "metadata": {},
   "source": [
    "# Part 2: PCA Method with Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## Assignment Requirements\n",
    "**Objective**: Use the PCA method with MLE technique to compute the covariance matrix, then compute rating predictions for the target items I1 and I2 using the \"Top Peers + Regression\" approach (same methodology as Part 1).\n",
    "\n",
    "**MLE Definition**: \n",
    "\"For simplicity, assume the Maximum Likelihood Estimate of the covariance between each pair of items is estimated as the covariance between only the specified entries. i.e, only the users that have specified ratings for a particular pair of items are used to estimate the covariance. If there are no users in common between a pair of items, the covariance is estimated to be 0.\"\n",
    "\n",
    "**Steps**:\n",
    "1. Generate the covariance matrix.\n",
    "2. Determine the top 5-peers and top 10-peers for each of the target items (I1 and I2) using the transformed representation (covariance matrix).\n",
    "3. Determine reduced dimensional space for each user in case of using the top 5-peers.\n",
    "4. Use the results from point 3 compute the rating predictions of the original missing rating for each of the target items (I1 and I2) using the top 5-peers.\n",
    "5. Determine reduced dimensional space for each user in case of using the top 10-peers.\n",
    "6. Use the results from point 5 compute the rating predictions of the original missing rating for each of the target items (I1 and I2) using the top 10-peers.\n",
    "7. Comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1cc6a",
   "metadata": {},
   "source": [
    "### Step 1: Load Data & Generate MLE Covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794028f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder exists at: c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION1_DimensionalityReduction\\results\n",
      "Subfolder exists: c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION1_DimensionalityReduction\\results\\plots\n",
      "Subfolder exists: c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION1_DimensionalityReduction\\results\\tables\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbe76f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder exists at: c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION1_DimensionalityReduction\\results\n",
      "Subfolder exists: c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION1_DimensionalityReduction\\results\\plots\n",
      "Subfolder exists: c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION1_DimensionalityReduction\\results\\tables\n"
     ]
    }
   ],
   "source": [
    "results_dir = ensure_results_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b26e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found cached sample at: ..\\data\\ml-20m\\ratings_cleaned_sampled.csv\n",
      "Data Loaded. Shape: (1000000, 3)\n",
      "Unique Users: 96345, Unique Items: 1000\n",
      "User-Item Matrix Shape: (96345, 1000)\n",
      "Using Default Target Items: [1556, 1499]\n",
      "Active Target Items: [1556, 1499]\n"
     ]
    }
   ],
   "source": [
    "df=load_data()\n",
    "\n",
    "print(f\"Data Loaded. Shape: {df.shape}\")\n",
    "print(f\"Unique Users: {df['userId'].nunique()}, Unique Items: {df['movieId'].nunique()}\")\n",
    "\n",
    "# 2. Pivot to User-Item Matrix\n",
    "# Columns are MovieIDs, Index is UserIDs\n",
    "user_item_matrix = df.pivot(index='userId', columns='movieId', values='rating')\n",
    "print(f\"User-Item Matrix Shape: {user_item_matrix.shape}\")\n",
    "\n",
    "# 3. Identify Targets (Load from file if possible, else defaults)\n",
    "target_items_path = os.path.join(results_dir, 'lowest_two_rateditems.csv')\n",
    "if os.path.exists(target_items_path):\n",
    "    target_items_df = pd.read_csv(target_items_path)\n",
    "    target_ids = target_items_df['movieId'].tolist()[:2]\n",
    "    print(f\"Loaded Target Items: {target_ids}\")\n",
    "else:\n",
    "    target_ids = [1556, 1499] # Fallback/Example\n",
    "    print(f\"Using Default Target Items: {target_ids}\")\n",
    "\n",
    "# Ensure targets are in the matrix\n",
    "target_ids = [t for t in target_ids if t in user_item_matrix.columns]\n",
    "print(f\"Active Target Items: {target_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999f170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing MLE Covariance Matrix...\n",
      "- Calculating Numerator...\n",
      "- Calculating Denominator...\n",
      "- Dividing...\n",
      "Covariance Matrix Generated.\n",
      "movieId         1         2         3         5         6\n",
      "movieId                                                  \n",
      "1        0.784061  0.257519  0.265043  0.193229  0.077289\n",
      "2        0.257519  0.965164  0.367336  0.259827  0.262220\n",
      "3        0.265043  0.367336  0.936565  0.289819  0.400179\n",
      "5        0.193229  0.259827  0.289819  1.044418  0.407888\n",
      "6        0.077289  0.262220  0.400179  0.407888  0.795272\n",
      "    Saved CSV: tables/3.3.1_mle_covariance.csv\n"
     ]
    }
   ],
   "source": [
    "cov_matrix, item_means, centered_df_raw = calculate_mle_covariance(user_item_matrix)\n",
    "print(\"Covariance Matrix Generated.\")\n",
    "print(cov_matrix.iloc[:5, :5])\n",
    "\n",
    "# Save covariance matrix for reference\n",
    "save_csv(cov_matrix, \"3.3.1_mle_covariance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e00c2c",
   "metadata": {},
   "source": [
    "### Step 2: Determine Top 5-Peers and Top 10-Peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afeea317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Target 1556 ---\n",
      "Top 5 Peers: [53125, 1103, 69757, 2421, 3275]\n",
      "Values: [1.53675873 1.19301815 1.12165668 1.04980911 0.98559018]\n",
      "Top 10 Peers: [53125, 1103, 69757, 2421, 3275, 78499, 5015, 160, 48385, 1591]\n",
      "Values: [1.53675873 1.19301815 1.12165668 1.04980911 0.98559018 0.97810952\n",
      " 0.97784548 0.97211373 0.95355605 0.91181265]\n",
      "\n",
      "--- Target 1499 ---\n",
      "Top 5 Peers: [362, 1296, 276, 261, 379]\n",
      "Values: [1.45527386 1.06729786 1.05805999 1.01015344 0.9694005 ]\n",
      "Top 10 Peers: [362, 1296, 276, 261, 379, 2340, 1591, 1380, 231, 2722]\n",
      "Values: [1.45527386 1.06729786 1.05805999 1.01015344 0.9694005  0.9062974\n",
      " 0.89877922 0.85796729 0.8534604  0.84931266]\n"
     ]
    }
   ],
   "source": [
    "peers_data = get_top_peers(cov_matrix, target_ids, k_list=[5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8af06c",
   "metadata": {},
   "source": [
    "### Steps 3-6: Reduced Dimensional Space & Prediction (Top 5 & Top 10)\n",
    "\n",
    "**Methodology**: \n",
    "1. **Reduced Space**: Select columns corresponding to the Top-K Peers from the centered rating matrix. `fillna(0)` is used because 0 represents the mean in centered data.\n",
    "2. **Prediction**: Train a Linear Regression model where:\n",
    "   - $X$: Ratings of Peer items (Reduced Space)\n",
    "   - $y$: Rating of Target item (Centered)\n",
    "   - Train on users who rated BOTH Target and Peers (or at least Target, filling missing peers with 0).\n",
    "   - Predict for users who have NOT rated the Target.\n",
    "   - Add Target Mean to get final rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f36e10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing K=5 Peers ===\n",
      "\n",
      "Target: 1556, Peers: [53125, 1103, 69757, 2421, 3275]\n",
      "Reduced Space Shape: (96345, 5)\n",
      "Training Samples: 422, Prediction Samples: 95923\n",
      "    Saved CSV: tables/3.3_mle_predictions_k5_target_1556.csv\n",
      "\n",
      "Target: 1499, Peers: [362, 1296, 276, 261, 379]\n",
      "Reduced Space Shape: (96345, 5)\n",
      "Training Samples: 453, Prediction Samples: 95892\n",
      "    Saved CSV: tables/3.3_mle_predictions_k5_target_1499.csv\n",
      "\n",
      "=== Processing K=10 Peers ===\n",
      "\n",
      "Target: 1556, Peers: [53125, 1103, 69757, 2421, 3275, 78499, 5015, 160, 48385, 1591]\n",
      "Reduced Space Shape: (96345, 10)\n",
      "Training Samples: 422, Prediction Samples: 95923\n",
      "    Saved CSV: tables/3.3_mle_predictions_k10_target_1556.csv\n",
      "\n",
      "Target: 1499, Peers: [362, 1296, 276, 261, 379, 2340, 1591, 1380, 231, 2722]\n",
      "Reduced Space Shape: (96345, 10)\n",
      "Training Samples: 453, Prediction Samples: 95892\n",
      "    Saved CSV: tables/3.3_mle_predictions_k10_target_1499.csv\n"
     ]
    }
   ],
   "source": [
    "# Run for K=5\n",
    "reduced_space_5, preds_5 = solve_pca_regression(5, peers_data, centered_df_raw, item_means, target_ids)\n",
    "\n",
    "# Run for K=10\n",
    "reduced_space_10, preds_10 = solve_pca_regression(10, peers_data, centered_df_raw, item_means, target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0d6be",
   "metadata": {},
   "source": [
    "### Step 7: Comparisons\n",
    "1. Compare Point 3 (Reduced Space K=5) vs Point 6 (Predictions K=10)? No, prompt says:\n",
    "   - \"Compare the results of point 3 with results of point 6\" -> (Reduced Space K=5 vs Reduced Space K=10 Prediction? Wait. Point 3 is Reduced Space K=5. Point 6 is Preds K=10. This is weird comparison. Maybe checks dimensionality vs accuracy? Or maybe typo in prompt referring to Preds K=5 vs Preds K=10. Let's compare Preds K=5 vs Preds K=10)\n",
    "   - \"Compare the results of point 9 in part 1 (Mean Filling Preds K=5) with results of point 4 (MLE Preds K=5)\"\n",
    "   - \"Compare the results of point 11 in part 1 (Mean Filling Preds K=10) with results of point 6 (MLE Preds K=10)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f10325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparisons ===\n",
      "--- MLE K=5 vs MLE K=10 (Predictions) ---\n",
      "Target 1556: MAE between K=5 and K=10 predictions: 0.0160\n",
      "Target 1499: MAE between K=5 and K=10 predictions: 0.0226\n",
      "\n",
      "--- Part 1 (Mean Fill) vs Part 2 (MLE) ---\n",
      "Target 1556 (K=5): MAE MeanFill vs MLE: 0.0519\n",
      "Target 1499 (K=5): MAE MeanFill vs MLE: 0.0325\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Comparisons ===\")\n",
    "\n",
    "# 1. Compare MLE K=5 vs MLE K=10 Predictions\n",
    "print(\"--- MLE K=5 vs MLE K=10 (Predictions) ---\")\n",
    "for tid in target_ids:\n",
    "    if tid in preds_5 and tid in preds_10:\n",
    "        p5 = preds_5[tid].set_index('userId')['predicted_rating_mle']\n",
    "        p10 = preds_10[tid].set_index('userId')['predicted_rating_mle']\n",
    "        \n",
    "        # Align indices\n",
    "        common = p5.index.intersection(p10.index)\n",
    "        mae = np.mean(np.abs(p5.loc[common] - p10.loc[common]))\n",
    "        print(f\"Target {tid}: MAE between K=5 and K=10 predictions: {mae:.4f}\")\n",
    "\n",
    "# 2. Compare Part 1 (Mean Filling) vs Part 2 (MLE)\n",
    "# Load Part 1 Predictions\n",
    "print(\"\\n--- Part 1 (Mean Fill) vs Part 2 (MLE) ---\")\n",
    "utils_tables_path = \"../results/tables\"\n",
    "\n",
    "for tid in target_ids:\n",
    "    # Load Part 1 K=5 (Point 9 in Part 1)\n",
    "    p1_k5_file = os.path.join(utils_tables_path, f\"3.2.9_predictions_target_{tid}.csv\")\n",
    "    \n",
    "    if os.path.exists(p1_k5_file):\n",
    "        p1_k5_df = pd.read_csv(p1_k5_file)\n",
    "        # Check column names in Part 1 file\n",
    "        # Usually 'predicted_rating_final'\n",
    "        if 'predicted_rating_final' in p1_k5_df.columns:\n",
    "            p1_series = p1_k5_df.set_index('userId')['predicted_rating_final']\n",
    "            \n",
    "            # Compare with MLE K=5\n",
    "            if tid in preds_5:\n",
    "                mle_series = preds_5[tid].set_index('userId')['predicted_rating_mle']\n",
    "                common = p1_series.index.intersection(mle_series.index)\n",
    "                mae = np.mean(np.abs(p1_series.loc[common] - mle_series.loc[common]))\n",
    "                print(f\"Target {tid} (K=5): MAE MeanFill vs MLE: {mae:.4f}\")\n",
    "            \n",
    "            # Compare with MLE K=10\n",
    "            # Assuming Part 1 K=10 file follows pattern? \n",
    "            # (Accessing Part 1 point 11 - assuming it was saved similarly or we skip)\n",
    "            # If Part 1 didn't explicitly save K=10 separate from K=5 (Part 1 notebook ended at Task 9 with K=5),\n",
    "            # we might only compare K=5.\n",
    "            pass\n",
    "    else:\n",
    "        print(f\"Part 1 Predictions for Target {tid} not found at {p1_k5_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9fdb01",
   "metadata": {},
   "source": [
    "### Steps 8-9: Comparison with Part 1\n",
    "Compare with results from Part 1 Points 9 and 11.\n",
    "- Reference your previous run's output for Part 1 to make specific comments.\n",
    "- Generally, MLE PCA (Part 2) ignores missing entries for correlations, providing valid structure analysis even with sparse data, whereas Mean Filling (Part 1) tends to dampen correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "036e6a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmarking PCA MLE (Part 2) ---\n",
      "Computing MLE Covariance Matrix...\n",
      "- Calculating Numerator...\n",
      "- Calculating Denominator...\n",
      "- Dividing...\n",
      "Results: Time Decomp 102.5571s, RMSE 0.8738\n",
      "Benchmark saved to c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION1_DimensionalityReduction\\code\\../results/tables\\pca_benchmarks.csv\n"
     ]
    }
   ],
   "source": [
    "# --- DYNAMIC BENCHMARKING: PCA MLE (Part 2) ---\\n\n",
    "method_name = \"PCA MLE (Part 2)\"\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.linalg import eigh\n",
    "# Ensure utils is loaded if locals missing (fallback)\n",
    "if 'load_data' not in locals():\n",
    "    try:\n",
    "        from utils import load_data, calculate_mle_covariance\n",
    "    except ImportError:\n",
    "        print(\"Warning: utils module not found. Benchmarking might fail.\")\n",
    "\n",
    "print(f\"\\n--- Benchmarking {method_name} ---\")\n",
    "\n",
    "# 1. Setup Results Path\n",
    "results_table_dir = os.path.join(os.getcwd(), '../results/tables')\n",
    "if not os.path.exists(results_table_dir):\n",
    "   os.makedirs(results_table_dir)\n",
    "\n",
    "# 2. Data Preparation (Robust Loading)\n",
    "R_bench = None\n",
    "\n",
    "# Attempt 1: Check for existing pivoted variables\n",
    "if 'user_item_matrix' in locals():\n",
    "    R_bench = user_item_matrix\n",
    "elif 'R_df' in locals():\n",
    "    R_bench = R_df\n",
    "\n",
    "# Attempt 2: Check for raw ratings and pivot\n",
    "if R_bench is None:\n",
    "    if 'ratings_df' in locals():\n",
    "        print(\"Pivoting ratings_df for benchmark...\")\n",
    "        R_bench = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
    "    elif 'df' in locals() and 'rating' in df.columns:\n",
    "        print(\"Pivoting df for benchmark...\")\n",
    "        R_bench = df.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# Attempt 3: Load from Disk\n",
    "if R_bench is None and 'load_data' in locals():\n",
    "    print(\"Loading data from disk for benchmark...\")\n",
    "    _df = load_data()\n",
    "    if _df is not None:\n",
    "        R_bench = _df.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "if R_bench is None:\n",
    "    print(\"Error: Could not obtain User-Item Matrix. Skipping Benchmark.\")\n",
    "else:\n",
    "    # Ensure float\n",
    "    R_bench = R_bench.astype(float)\n",
    "    \n",
    "    # 3. Decomposition & Prediction\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if 'Mean Filling' in method_name:\n",
    "        # Mean Filling Logic\n",
    "        item_means_bench = R_bench.mean()\n",
    "        R_filled_bench = R_bench.fillna(item_means_bench)\n",
    "        R_centered_bench = R_filled_bench - item_means_bench\n",
    "        X = R_centered_bench.values\n",
    "        # Cov = X.T @ X / (N-1)\n",
    "        cov_matrix_bench = np.dot(X.T, X) / (X.shape[0] - 1)\n",
    "        \n",
    "    else:\n",
    "        # MLE Logic\n",
    "        if 'calculate_mle_covariance' in locals():\n",
    "             cov_matrix_bench, _, _ = calculate_mle_covariance(R_bench)\n",
    "             if isinstance(cov_matrix_bench, pd.DataFrame): cov_matrix_bench = cov_matrix_bench.values\n",
    "        else:\n",
    "             print(\"MLE Util not found. Skipping.\")\n",
    "             cov_matrix_bench = None\n",
    "\n",
    "    if cov_matrix_bench is not None:\n",
    "        # Eigh\n",
    "        evals_b, evecs_b = eigh(cov_matrix_bench)\n",
    "        idx_b = np.argsort(evals_b)[::-1]\n",
    "        evecs_b = evecs_b[:, idx_b]\n",
    "        \n",
    "        t1 = time.time()\n",
    "        time_decomp = t1 - t0\n",
    "        \n",
    "        # 4. Prediction Timing\n",
    "        t2 = time.time()\n",
    "        k_limit = 50\n",
    "        V_k = evecs_b[:, :k_limit]\n",
    "        \n",
    "        if 'Mean Filling' in method_name:\n",
    "             # U = X V\n",
    "             U_b = np.dot(X, V_k)\n",
    "             # X_hat = U V.T + Mean\n",
    "             X_hat_b = np.dot(U_b, V_k.T) + item_means_bench.values\n",
    "        else:\n",
    "             means_mle = R_bench.mean()\n",
    "             X_mle = R_bench.fillna(means_mle).values - means_mle.values\n",
    "             U_b = np.dot(X_mle, V_k)\n",
    "             X_hat_b = np.dot(U_b, V_k.T) + means_mle.values\n",
    "             \n",
    "        t3 = time.time()\n",
    "        time_pred = t3 - t2\n",
    "        \n",
    "        # 5. Metrics\n",
    "        mask_b = ~np.isnan(R_bench.values)\n",
    "        diff_b = R_bench.values[mask_b] - X_hat_b[mask_b]\n",
    "        rmse_val = np.sqrt(np.mean(diff_b**2))\n",
    "        mae_val = np.mean(np.abs(diff_b))\n",
    "        mem_mb = (cov_matrix_bench.nbytes + evecs_b.nbytes) / 1024 / 1024\n",
    "        \n",
    "        print(f\"Results: Time Decomp {time_decomp:.4f}s, RMSE {rmse_val:.4f}\")\n",
    "        \n",
    "        # 6. Save\n",
    "        bench_file = os.path.join(results_table_dir, 'pca_benchmarks.csv')\n",
    "        new_data = {\n",
    "            'Method': method_name,\n",
    "            'k': k_limit,\n",
    "            'RMSE': rmse_val,\n",
    "            'MAE': mae_val,\n",
    "            'Time_Decomp(s)': time_decomp,\n",
    "            'Time_Pred(s)': time_pred,\n",
    "            'Memory(MB)': mem_mb\n",
    "        }\n",
    "        \n",
    "        if os.path.exists(bench_file):\n",
    "            df_bench = pd.read_csv(bench_file)\n",
    "            df_bench = df_bench[df_bench['Method'] != method_name]\n",
    "            df_bench = pd.concat([df_bench, pd.DataFrame([new_data])], ignore_index=True)\n",
    "        else:\n",
    "            df_bench = pd.DataFrame([new_data])\n",
    "            \n",
    "        df_bench.to_csv(bench_file, index=False)\n",
    "        print(f\"Benchmark saved to {bench_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
