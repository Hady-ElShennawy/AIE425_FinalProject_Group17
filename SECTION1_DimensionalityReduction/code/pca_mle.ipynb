{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5657ee74",
   "metadata": {},
   "source": [
    "# Group 17: \n",
    "### Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c9972",
   "metadata": {},
   "source": [
    "# PCA with Maximum Likelihood Estimation (MLE) Approach\n",
    "\n",
    "This notebook implements PCA-based collaborative filtering using the MLE covariance estimation method to handle sparse rating matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794028f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder exists at: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\n",
      "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\\plots\n",
      "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\\tables\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbe76f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder exists at: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\n",
      "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\\plots\n",
      "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\\tables\n"
     ]
    }
   ],
   "source": [
    "results_dir = ensure_results_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eda2ab",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Load the dataset and create the user-item matrix. Identify target items and users for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b26e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found cached sample at: ..\\data\\ml-20m\\ratings_cleaned_sampled.csv\n",
      "Loaded dataset with shape: (1000000, 3)\n",
      "User-Item Matrix dimensions: (96345, 1000)\n",
      " Found cached sample at: ..\\data\\ml-20m\\ratings_cleaned_sampled.csv\n",
      "Loaded target items from file: [4011, 786]\n",
      "Active targets in matrix: [4011, 786]\n",
      "Users with missing target ratings: 96309\n"
     ]
    }
   ],
   "source": [
    "ratings_df =load_data()\n",
    "\n",
    "# Fallback to local file if utility function fails\n",
    "if ratings_df is None:\n",
    "    local_file = 'ratings_cleaned_sampled.csv'\n",
    "    if os.path.exists(local_file):\n",
    "        ratings_df = pd.read_csv(local_file)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Dataset not found! Check file paths.\")\n",
    "\n",
    "print(f\"Loaded dataset with shape: {ratings_df.shape}\")\n",
    "\n",
    "# Create user-item matrix\n",
    "ui_matrix = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
    "print(f\"User-Item Matrix dimensions: {ui_matrix.shape}\")\n",
    "\n",
    "# Load or set target items for prediction\n",
    "targets_df = load_data('lowest_two_rateditems.csv')\n",
    "\n",
    "target_items = targets_df['movieId'].tolist()[:2]\n",
    "print(f\"Loaded target items from file: {target_items}\")\n",
    "\n",
    "\n",
    "# Filter to active targets in the matrix\n",
    "active_targets = [item for item in target_items if item in ui_matrix.columns]\n",
    "print(f\"Active targets in matrix: {active_targets}\")\n",
    "\n",
    "# Identify users who haven't rated at least one target item\n",
    "users_with_missing = ui_matrix[ui_matrix[active_targets].isna().any(axis=1)].index.tolist()\n",
    "print(f\"Users with missing target ratings: {len(users_with_missing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4f808",
   "metadata": {},
   "source": [
    "## Step 1: MLE-based Covariance Matrix Construction\n",
    "\n",
    "Calculate the true MLE covariance between items by computing pairwise covariances using only users who rated both items in each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building MLE Covariance Matrix...\n"
     ]
    }
   ],
   "source": [
    "# Center the data by subtracting item means\n",
    "item_avg = ui_matrix.mean(axis=0)\n",
    "ui_centered = ui_matrix - item_avg\n",
    "\n",
    "# Extract values and dimensions\n",
    "matrix_vals = ui_centered.values\n",
    "n_items = ui_centered.shape[1]\n",
    "item_indices = ui_centered.columns\n",
    "\n",
    "# Initialize covariance matrix\n",
    "cov_matrix = np.zeros((n_items, n_items), dtype=float)\n",
    "\n",
    "print(\"Building MLE Covariance Matrix...\")\n",
    "# Compute pairwise covariances\n",
    "for i in range(n_items):\n",
    "    col_i = matrix_vals[:, i]\n",
    "    for j in range(i, n_items):\n",
    "        col_j = matrix_vals[:, j]\n",
    "        \n",
    "        # Find users who rated both items\n",
    "        valid_mask = ~np.isnan(col_i) & ~np.isnan(col_j)\n",
    "        count = int(valid_mask.sum())\n",
    "\n",
    "        # Calculate MLE covariance\n",
    "        if count == 0:\n",
    "            cov_val = 0.0\n",
    "        else:\n",
    "            cov_val = float(np.dot(col_i[valid_mask], col_j[valid_mask]) / count)\n",
    "\n",
    "        # Store symmetric values\n",
    "        cov_matrix[i, j] = cov_val\n",
    "        cov_matrix[j, i] = cov_val\n",
    "\n",
    "# Convert to DataFrame\n",
    "cov_df = pd.DataFrame(cov_matrix, index=item_indices, columns=item_indices)\n",
    "\n",
    "print(\"Step 1: TRUE MLE Covariance Matrix Generated.\")\n",
    "cov_df.to_csv(output_path + '/pca_mle_step1_cov_matrix.csv')\n",
    "print(\"Step 1 Output Saved: pca_mle_step1_cov_matrix.csv\")\n",
    "print(cov_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da75cf",
   "metadata": {},
   "source": [
    "## Step 2: Latent Space Construction and Peer Discovery\n",
    "\n",
    "Apply eigen-decomposition to extract principal components and identify similar items in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74788b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing Eigen-decomposition...\")\n",
    "\n",
    "# Eigenvalue decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_df.values)\n",
    "\n",
    "# Sort by eigenvalues (descending)\n",
    "sort_idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sort_idx]\n",
    "eigenvectors = eigenvectors[:, sort_idx]\n",
    "\n",
    "# Calculate variance explained\n",
    "total_var = np.sum(eigenvalues)\n",
    "var_ratio = eigenvalues / total_var\n",
    "cumulative_var = np.cumsum(var_ratio)\n",
    "\n",
    "# Determine components for 75% variance\n",
    "k_components = np.argmax(cumulative_var >= 0.75) + 1\n",
    "print(f\"\\nNumber of components to explain 75% variance: {k_components}\")\n",
    "print(f\"Correlation Variance at k={k_components}: {cumulative_var[k_components-1]:.4f}\")\n",
    "\n",
    "print(f\"Selected Top {k_components} Eigenvalues based on 75% Variance:\")\n",
    "print(eigenvalues[:k_components])\n",
    "\n",
    "# Helper function to find similar items in latent space\n",
    "\n",
    "\n",
    "# Get all item IDs\n",
    "all_items = list(cov_df.columns)\n",
    "\n",
    "# Find Top-5 and Top-10 neighbors\n",
    "neighbors_5 = compute_latent_neighbors(\n",
    "    n_comp=k_components, n_neighbors=5,\n",
    "    all_items=all_items, target_list=active_targets, evec_matrix=eigenvectors\n",
    ")\n",
    "\n",
    "neighbors_10 = compute_latent_neighbors(\n",
    "    n_comp=k_components, n_neighbors=10,\n",
    "    all_items=all_items, target_list=active_targets, evec_matrix=eigenvectors\n",
    ")\n",
    "\n",
    "# Create summary table\n",
    "peer_summary = []\n",
    "for target in active_targets:\n",
    "    if target in neighbors_5 and target in neighbors_10:\n",
    "        peer_summary.append({\n",
    "            \"TargetItem\": target,\n",
    "            \"Top5_Peers_MLE\": str(neighbors_5[target][\"top_ids\"]),\n",
    "            \"Top10_Peers_MLE\": str(neighbors_10[target][\"top_ids\"])\n",
    "        })\n",
    "\n",
    "peers_table = pd.DataFrame(peer_summary)\n",
    "peers_table.to_csv(output_path + \"/pca_mle_step2_peers.csv\", index=False)\n",
    "print(\"Step 2 Output Saved: pca_mle_step2_peers.csv\")\n",
    "\n",
    "# Display top-5 peers for each target\n",
    "for target in active_targets:\n",
    "    if target in neighbors_5:\n",
    "        print(f\"Target {target} Top-5 Peers: {neighbors_5[target]['top_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ccdb7",
   "metadata": {},
   "source": [
    "## Step 3: Reduced Dimensional Space (Top 5 Neighbors)\n",
    "\n",
    "Document the latent peer space and construct user feature vectors in the reduced dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build reduced space table\n",
    "reduced_space_5 = []\n",
    "for target in active_targets:\n",
    "    if target not in neighbors_5:\n",
    "        continue\n",
    "    peer_list = neighbors_5[target]['top_ids'][:5]\n",
    "    similarity_vals = neighbors_5[target]['sim_series']\n",
    "\n",
    "    for position, peer_id in enumerate(peer_list, 1):\n",
    "        reduced_space_5.append({\n",
    "            'TargetItem': target,\n",
    "            'Peer_Rank': position,\n",
    "            'Peer_ItemID': peer_id,\n",
    "            'Latent_Similarity': float(similarity_vals[peer_id]),\n",
    "            'Space_Type': 'Top5_MLE'\n",
    "        })\n",
    "\n",
    "reduced_5_df = pd.DataFrame(reduced_space_5)\n",
    "reduced_5_df.to_csv(output_path + '/pca_mle_step3_reduced_space_top5.csv', index=False)\n",
    "print(\"Step 3 Output Saved: pca_mle_step3_reduced_space_top5.csv\")\n",
    "\n",
    "# Build user feature vectors in reduced space\n",
    "user_features_5 = []\n",
    "for target in active_targets:\n",
    "    if target not in neighbors_5:\n",
    "        continue\n",
    "    peer_items = neighbors_5[target]['top_ids'][:5]\n",
    "\n",
    "    for user in users_with_missing:\n",
    "        if user not in ui_centered.index:\n",
    "            continue\n",
    "        \n",
    "        # Extract user's ratings on peer items\n",
    "        user_vector = ui_centered.loc[user, peer_items].values\n",
    "        feature_dict = {'UserID': user, 'TargetItem': target}\n",
    "        \n",
    "        for idx, peer in enumerate(peer_items, 1):\n",
    "            feature_dict[f'Peer{idx}_{peer}'] = user_vector[idx-1]\n",
    "        \n",
    "        user_features_5.append(feature_dict)\n",
    "\n",
    "user_vec_5_df = pd.DataFrame(user_features_5)\n",
    "user_vec_5_df.to_csv(output_path + '/pca_mle_step3_user_reduced_vectors_top5.csv', index=False)\n",
    "print(\"Step 3 (extra) Output Saved: pca_mle_step3_user_reduced_vectors_top5.csv\")\n",
    "\n",
    "print(reduced_5_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7fbcf",
   "metadata": {},
   "source": [
    "## Step 4: Rating Predictions Using Top 5 Neighbors\n",
    "\n",
    "Generate rating predictions using similarity-weighted averaging over the top 5 latent neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_5 = []\n",
    "\n",
    "for user in users_with_missing:\n",
    "    if user not in ui_matrix.index:\n",
    "        continue\n",
    "\n",
    "    for target in active_targets:\n",
    "        if target not in neighbors_5:\n",
    "            continue\n",
    "\n",
    "        # Check if rating exists\n",
    "        rating_status = \"Existing\" if pd.notna(ui_matrix.loc[user, target]) else \"Missing\"\n",
    "\n",
    "        peer_items = neighbors_5[target]['top_ids'][:5]\n",
    "        similarity_weights = neighbors_5[target]['sim_series']\n",
    "\n",
    "        # Compute weighted prediction\n",
    "        numerator = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for peer in peer_items:\n",
    "            user_rating = ui_centered.loc[user, peer]\n",
    "            if pd.notna(user_rating):\n",
    "                weight = float(similarity_weights[peer])\n",
    "                numerator += weight * float(user_rating)\n",
    "                denominator += abs(weight)\n",
    "\n",
    "        # Add back the item mean\n",
    "        baseline = float(item_avg[target]) if target in item_avg.index else 0.0\n",
    "        predicted_rating = baseline + (numerator / denominator) if denominator != 0 else baseline\n",
    "\n",
    "        predictions_5.append({\n",
    "            'UserID': user, \n",
    "            'ItemID': target, \n",
    "            'Pred_MLE_Top5': predicted_rating, \n",
    "            'Status': rating_status\n",
    "        })\n",
    "\n",
    "predictions_5_df = pd.DataFrame(predictions_5)\n",
    "predictions_5_df.to_csv(output_path + '/pca_mle_step4_preds_top5.csv', index=False)\n",
    "print(\"Step 4 Output Saved: pca_mle_step4_preds_top5.csv\")\n",
    "print(predictions_5_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106691a",
   "metadata": {},
   "source": [
    "## Step 5: Reduced Dimensional Space (Top 10 Neighbors)\n",
    "\n",
    "Expand the latent space to include top 10 neighbors and generate corresponding user feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build reduced space table for top 10\n",
    "reduced_space_10 = []\n",
    "for target in active_targets:\n",
    "    if target not in neighbors_10:\n",
    "        continue\n",
    "    peer_list = neighbors_10[target]['top_ids'][:10]\n",
    "    similarity_vals = neighbors_10[target]['sim_series']\n",
    "\n",
    "    for position, peer_id in enumerate(peer_list, 1):\n",
    "        reduced_space_10.append({\n",
    "            'TargetItem': target,\n",
    "            'Peer_Rank': position,\n",
    "            'Peer_ItemID': peer_id,\n",
    "            'Latent_Similarity': float(similarity_vals[peer_id]),\n",
    "            'Space_Type': 'Top10_MLE'\n",
    "        })\n",
    "\n",
    "reduced_10_df = pd.DataFrame(reduced_space_10)\n",
    "reduced_10_df.to_csv(output_path + '/pca_mle_step5_reduced_space_top10.csv', index=False)\n",
    "\n",
    "\n",
    "# Build user feature vectors with 10 dimensions\n",
    "user_features_10 = []\n",
    "for target in active_targets:\n",
    "    if target not in neighbors_10:\n",
    "        continue\n",
    "    peer_items = neighbors_10[target]['top_ids'][:10]\n",
    "\n",
    "    for user in users_with_missing:\n",
    "        if user not in ui_centered.index:\n",
    "            continue\n",
    "        \n",
    "        # Extract user's ratings on peer items\n",
    "        user_vector = ui_centered.loc[user, peer_items].values\n",
    "        feature_dict = {'UserID': user, 'TargetItem': target}\n",
    "        \n",
    "        for idx, peer in enumerate(peer_items, 1):\n",
    "            feature_dict[f'Peer{idx}_{peer}'] = user_vector[idx-1]\n",
    "        \n",
    "        user_features_10.append(feature_dict)\n",
    "\n",
    "user_vec_10_df = pd.DataFrame(user_features_10)\n",
    "user_vec_10_df.to_csv(output_path + '/pca_mle_step5_user_reduced_vectors_top10.csv', index=False)\n",
    "print(\"Step 5 (extra) Output Saved: pca_mle_step5_user_reduced_vectors_top10.csv\")\n",
    "\n",
    "print(reduced_10_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858974a8",
   "metadata": {},
   "source": [
    "## Step 6: Rating Predictions Using Top 10 Neighbors\n",
    "\n",
    "Generate predictions with expanded neighborhood for potentially improved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a041ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_10 = []\n",
    "\n",
    "for user in users_with_missing:\n",
    "    if user not in ui_matrix.index:\n",
    "        continue\n",
    "\n",
    "    for target in active_targets:\n",
    "        if target not in neighbors_10:\n",
    "            continue\n",
    "\n",
    "        # Determine rating status\n",
    "        rating_status = \"Existing\" if pd.notna(ui_matrix.loc[user, target]) else \"Missing\"\n",
    "\n",
    "        peer_items = neighbors_10[target]['top_ids'][:10]\n",
    "        similarity_weights = neighbors_10[target]['sim_series']\n",
    "\n",
    "        # Calculate weighted average\n",
    "        numerator = 0.0\n",
    "        denominator = 0.0\n",
    "\n",
    "        for peer in peer_items:\n",
    "            user_rating = ui_centered.loc[user, peer]\n",
    "            if pd.notna(user_rating):\n",
    "                weight = float(similarity_weights[peer])\n",
    "                numerator += weight * float(user_rating)\n",
    "                denominator += abs(weight)\n",
    "\n",
    "        # Add item baseline\n",
    "        baseline = float(item_avg[target]) if target in item_avg.index else 0.0\n",
    "        predicted_rating = baseline + (numerator / denominator) if denominator != 0 else baseline\n",
    "\n",
    "        predictions_10.append({\n",
    "            'UserID': user, \n",
    "            'ItemID': target, \n",
    "            'Pred_MLE_Top10': predicted_rating, \n",
    "            'Status': rating_status\n",
    "        })\n",
    "\n",
    "predictions_10_df = pd.DataFrame(predictions_10)\n",
    "predictions_10_df.to_csv(output_path + '/pca_mle_step6_preds_top10.csv', index=False)\n",
    "print(\"Step 6 Output Saved: pca_mle_step6_preds_top10.csv\")\n",
    "print(predictions_10_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371c410",
   "metadata": {},
   "source": [
    "## Step 7/8/9: Comparison Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c593a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the reduced dimensional space from Step 3 (Top 5)\n",
    "print(\"Step 3 - Reduced Dimensional Space (Top 5 Peers):\")\n",
    "print(reduced_5_df)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Display the prediction results from Step 6 (Top 10)\n",
    "print(\"Step 6 - Rating Predictions (Top 10 Peers):\")\n",
    "print(predictions_10_df.head(20))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Comparison Analysis\n",
    "print(\"Comparison Analysis:\")\n",
    "print(f\"Number of peer items in reduced space (Step 3): {len(reduced_5_df)}\")\n",
    "print(f\"Number of predictions made (Step 6): {len(predictions_10_df)}\")\n",
    "print(f\"\\nStep 3 uses {reduced_5_df['Space_Type'].iloc[0]} for dimensionality reduction\")\n",
    "print(f\"Step 6 predictions are based on Top 10 neighbors\")\n",
    "print(f\"\\nAverage prediction value (Step 6): {predictions_10_df['Pred_MLE_Top10'].mean():.4f}\")\n",
    "print(f\"Average latent similarity (Step 3): {reduced_5_df['Latent_Similarity'].mean():.4f}\")\n",
    "\n",
    "# Save comparison summary\n",
    "comparison_summary = {\n",
    "    'Step3_Peers_Count': len(reduced_5_df),\n",
    "    'Step6_Predictions_Count': len(predictions_10_df),\n",
    "    'Step3_Avg_Similarity': reduced_5_df['Latent_Similarity'].mean(),\n",
    "    'Step6_Avg_Prediction': predictions_10_df['Pred_MLE_Top10'].mean()\n",
    "}\n",
    "summary_df = pd.DataFrame([comparison_summary])\n",
    "summary_df.to_csv(output_path + '/pca_mle_step7_comparison.csv', index=False)\n",
    "print(\"\\nStep 7 Output Saved: pca_mle_step7_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df966c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Compare Linear Regression Method with PCA MLE Method\n",
    "\n",
    "# Load linear regression predictions from Task 9 (from results folder)\n",
    "lr_pred_1499 = pd.read_csv('../results/3.2.9_predictions_target_1499.csv')\n",
    "lr_pred_1556 = pd.read_csv('../results/3.2.9_predictions_target_1556.csv')\n",
    "\n",
    "# Combine both target items\n",
    "lr_predictions = pd.concat([lr_pred_1499, lr_pred_1556], ignore_index=True)\n",
    "\n",
    "# Rename columns to match PCA naming convention\n",
    "lr_predictions.rename(columns={\n",
    "    'userId': 'UserID',\n",
    "    'movieId': 'ItemID',\n",
    "    'predicted_rating_final': 'Predicted_Rating'  # Fixed: removed leading space\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge with PCA MLE predictions from Step 4\n",
    "comparison_df = pd.merge(\n",
    "    lr_predictions[['UserID', 'ItemID', 'Predicted_Rating']],\n",
    "    predictions_5_df[['UserID', 'ItemID', 'Pred_MLE_Top5']],\n",
    "    on=['UserID', 'ItemID'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "comparison_df.rename(columns={\n",
    "    'Predicted_Rating': 'LinearRegression_Pred',\n",
    "    'Pred_MLE_Top5': 'PCA_MLE_Pred'\n",
    "}, inplace=True)\n",
    "\n",
    "# Calculate differences\n",
    "comparison_df['Diff'] = comparison_df['LinearRegression_Pred'] - comparison_df['PCA_MLE_Pred']\n",
    "comparison_df['AbsDiff'] = comparison_df['Diff'].abs()\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv(output_path + '/pca_mle_step8_method_comparison.csv', index=False)\n",
    "print(\"Step 8 Output Saved: pca_mle_step8_method_comparison.csv\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: Comparison of Linear Regression vs PCA MLE Top-5 Predictions\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFirst 10 comparisons:\")\n",
    "print(comparison_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Total predictions compared: {len(comparison_df)}\")\n",
    "print(f\"\\nLinear Regression Method:\")\n",
    "print(f\"  Mean Prediction: {comparison_df['LinearRegression_Pred'].mean():.6f}\")\n",
    "print(f\"  Std Deviation:   {comparison_df['LinearRegression_Pred'].std():.6f}\")\n",
    "print(f\"\\nPCA MLE Method:\")\n",
    "print(f\"  Mean Prediction: {comparison_df['PCA_MLE_Pred'].mean():.6f}\")\n",
    "print(f\"  Std Deviation:   {comparison_df['PCA_MLE_Pred'].std():.6f}\")\n",
    "print(f\"\\nDifference Analysis:\")\n",
    "print(f\"  Mean Absolute Difference: {comparison_df['AbsDiff'].mean():.6f}\")\n",
    "print(f\"  Max Absolute Difference:  {comparison_df['AbsDiff'].max():.6f}\")\n",
    "print(f\"  Min Absolute Difference:  {comparison_df['AbsDiff'].min():.6f}\")\n",
    "print(f\"  Std Dev of Differences:   {comparison_df['Diff'].std():.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4015765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Compare Linear Regression Top-10 with PCA MLE Top-10\n",
    "\n",
    "# Load linear regression Top-10 predictions\n",
    "lr_pred_1499_top10 = pd.read_csv('../results/3.2.11_predictions_target_1499_top10.csv')\n",
    "lr_pred_1556_top10 = pd.read_csv('../results/3.2.11_predictions_target_1556_top10.csv')\n",
    "\n",
    "# Combine both target items\n",
    "lr_predictions_top10 = pd.concat([lr_pred_1499_top10, lr_pred_1556_top10], ignore_index=True)\n",
    "\n",
    "# Rename columns to match PCA naming convention\n",
    "lr_predictions_top10.rename(columns={\n",
    "    'userId': 'UserID',\n",
    "    'movieId': 'ItemID',\n",
    "    'predicted_rating_final': 'Predicted_Rating'\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge with PCA MLE Top-10 predictions from Step 6\n",
    "comparison_df = pd.merge(\n",
    "    lr_predictions_top10[['UserID', 'ItemID', 'Predicted_Rating']],\n",
    "    predictions_10_df[['UserID', 'ItemID', 'Pred_MLE_Top10']],\n",
    "    on=['UserID', 'ItemID'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "comparison_df.rename(columns={\n",
    "    'Predicted_Rating': 'LinearRegression_Top10_Pred',\n",
    "    'Pred_MLE_Top10': 'PCA_MLE_Top10_Pred'\n",
    "}, inplace=True)\n",
    "\n",
    "# Calculate differences\n",
    "comparison_df['Diff'] = comparison_df['LinearRegression_Top10_Pred'] - comparison_df['PCA_MLE_Top10_Pred']\n",
    "comparison_df['AbsDiff'] = comparison_df['Diff'].abs()\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv(output_path + '/pca_mle_step9_method_comparison_top10.csv', index=False)\n",
    "print(\"Step 9 Output Saved: pca_mle_step9_method_comparison_top10.csv\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 9: Comparison of Linear Regression Top-10 vs PCA MLE Top-10 Predictions\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFirst 10 comparisons:\")\n",
    "print(comparison_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Total predictions compared: {len(comparison_df)}\")\n",
    "print(f\"\\nLinear Regression Top-10 Method:\")\n",
    "print(f\"  Mean Prediction: {comparison_df['LinearRegression_Top10_Pred'].mean():.6f}\")\n",
    "print(f\"  Std Deviation:   {comparison_df['LinearRegression_Top10_Pred'].std():.6f}\")\n",
    "print(f\"\\nPCA MLE Top-10 Method:\")\n",
    "print(f\"  Mean Prediction: {comparison_df['PCA_MLE_Top10_Pred'].mean():.6f}\")\n",
    "print(f\"  Std Deviation:   {comparison_df['PCA_MLE_Top10_Pred'].std():.6f}\")\n",
    "print(f\"\\nDifference Analysis:\")\n",
    "print(f\"  Mean Absolute Difference: {comparison_df['AbsDiff'].mean():.6f}\")\n",
    "print(f\"  Max Absolute Difference:  {comparison_df['AbsDiff'].max():.6f}\")\n",
    "print(f\"  Min Absolute Difference:  {comparison_df['AbsDiff'].min():.6f}\")\n",
    "print(f\"  Std Dev of Differences:   {comparison_df['Diff'].std():.6f}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
