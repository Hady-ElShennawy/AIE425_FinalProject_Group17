{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5657ee74",
   "metadata": {},
   "source": [
    "# Group 17: \n",
    "# Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c9972",
   "metadata": {},
   "source": [
    "# Part 2: PCA Method with Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## Assignment Requirements\n",
    "**Objective**: Use the PCA method with MLE technique to compute the covariance matrix, then compute rating predictions for the target items I1 and I2 using the \"Top Peers + Regression\" approach (same methodology as Part 1).\n",
    "\n",
    "**MLE Definition**: \n",
    "\"For simplicity, assume the Maximum Likelihood Estimate of the covariance between each pair of items is estimated as the covariance between only the specified entries. i.e, only the users that have specified ratings for a particular pair of items are used to estimate the covariance. If there are no users in common between a pair of items, the covariance is estimated to be 0.\"\n",
    "\n",
    "**Steps**:\n",
    "1. Generate the covariance matrix.\n",
    "2. Determine the top 5-peers and top 10-peers for each of the target items (I1 and I2) using the transformed representation (covariance matrix).\n",
    "3. Determine reduced dimensional space for each user in case of using the top 5-peers.\n",
    "4. Use the results from point 3 compute the rating predictions of the original missing rating for each of the target items (I1 and I2) using the top 5-peers.\n",
    "5. Determine reduced dimensional space for each user in case of using the top 10-peers.\n",
    "6. Use the results from point 5 compute the rating predictions of the original missing rating for each of the target items (I1 and I2) using the top 10-peers.\n",
    "7. Comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1cc6a",
   "metadata": {},
   "source": [
    "### Step 1: Load Data & Generate MLE Covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794028f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder exists at: d:\\University\\semester 9\\IRS\\AIE425_FinalProject\\SECTION1_DimensionalityReduction\\results\n",
      "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject\\SECTION1_DimensionalityReduction\\results\\plots\n",
      "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject\\SECTION1_DimensionalityReduction\\results\\tables\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbe76f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results folder exists at: d:\\University\\semester 9\\IRS\\AIE425_FinalProject\\SECTION1_DimensionalityReduction\\results\n",
      "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject\\SECTION1_DimensionalityReduction\\results\\plots\n",
      "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject\\SECTION1_DimensionalityReduction\\results\\tables\n"
     ]
    }
   ],
   "source": [
    "results_dir = ensure_results_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b26e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found cached sample at: ..\\data\\ml-20m\\ratings_cleaned_sampled.csv\n",
      "Data Loaded. Shape: (1000000, 3)\n",
      "Unique Users: 96345, Unique Items: 1000\n",
      "User-Item Matrix Shape: (96345, 1000)\n",
      "Using Default Target Items: [1556, 1499]\n",
      "Active Target Items: [1556, 1499]\n"
     ]
    }
   ],
   "source": [
    "df=load_data()\n",
    "\n",
    "print(f\"Data Loaded. Shape: {df.shape}\")\n",
    "print(f\"Unique Users: {df['userId'].nunique()}, Unique Items: {df['movieId'].nunique()}\")\n",
    "\n",
    "# 2. Pivot to User-Item Matrix\n",
    "# Columns are MovieIDs, Index is UserIDs\n",
    "user_item_matrix = df.pivot(index='userId', columns='movieId', values='rating')\n",
    "print(f\"User-Item Matrix Shape: {user_item_matrix.shape}\")\n",
    "\n",
    "# 3. Identify Targets (Load from file if possible, else defaults)\n",
    "target_items_path = os.path.join(results_dir, 'lowest_two_rateditems.csv')\n",
    "if os.path.exists(target_items_path):\n",
    "    target_items_df = pd.read_csv(target_items_path)\n",
    "    target_ids = target_items_df['movieId'].tolist()[:2]\n",
    "    print(f\"Loaded Target Items: {target_ids}\")\n",
    "else:\n",
    "    target_ids = [1556, 1499] # Fallback/Example\n",
    "    print(f\"Using Default Target Items: {target_ids}\")\n",
    "\n",
    "# Ensure targets are in the matrix\n",
    "target_ids = [t for t in target_ids if t in user_item_matrix.columns]\n",
    "print(f\"Active Target Items: {target_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing MLE Covariance Matrix...\n",
      "- Calculating Numerator...\n",
      "- Calculating Denominator...\n"
     ]
    }
   ],
   "source": [
    "cov_matrix, item_means, centered_df_raw = calculate_mle_covariance(user_item_matrix)\n",
    "print(\"Covariance Matrix Generated.\")\n",
    "print(cov_matrix.iloc[:5, :5])\n",
    "\n",
    "# Save covariance matrix for reference\n",
    "save_csv(cov_matrix, \"3.3.1_mle_covariance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e00c2c",
   "metadata": {},
   "source": [
    "### Step 2: Determine Top 5-Peers and Top 10-Peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeea317",
   "metadata": {},
   "outputs": [],
   "source": [
    "peers_data = get_top_peers(cov_matrix, target_ids, k_list=[5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8af06c",
   "metadata": {},
   "source": [
    "### Steps 3-6: Reduced Dimensional Space & Prediction (Top 5 & Top 10)\n",
    "\n",
    "**Methodology**: \n",
    "1. **Reduced Space**: Select columns corresponding to the Top-K Peers from the centered rating matrix. `fillna(0)` is used because 0 represents the mean in centered data.\n",
    "2. **Prediction**: Train a Linear Regression model where:\n",
    "   - $X$: Ratings of Peer items (Reduced Space)\n",
    "   - $y$: Rating of Target item (Centered)\n",
    "   - Train on users who rated BOTH Target and Peers (or at least Target, filling missing peers with 0).\n",
    "   - Predict for users who have NOT rated the Target.\n",
    "   - Add Target Mean to get final rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f36e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for K=5\n",
    "reduced_space_5, preds_5 = solve_pca_regression(5, peers_data, centered_df_raw, item_means, target_ids)\n",
    "\n",
    "# Run for K=10\n",
    "reduced_space_10, preds_10 = solve_pca_regression(10, peers_data, centered_df_raw, item_means, target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0d6be",
   "metadata": {},
   "source": [
    "### Step 7: Comparisons\n",
    "1. Compare Point 3 (Reduced Space K=5) vs Point 6 (Predictions K=10)? No, prompt says:\n",
    "   - \"Compare the results of point 3 with results of point 6\" -> (Reduced Space K=5 vs Reduced Space K=10 Prediction? Wait. Point 3 is Reduced Space K=5. Point 6 is Preds K=10. This is weird comparison. Maybe checks dimensionality vs accuracy? Or maybe typo in prompt referring to Preds K=5 vs Preds K=10. Let's compare Preds K=5 vs Preds K=10)\n",
    "   - \"Compare the results of point 9 in part 1 (Mean Filling Preds K=5) with results of point 4 (MLE Preds K=5)\"\n",
    "   - \"Compare the results of point 11 in part 1 (Mean Filling Preds K=10) with results of point 6 (MLE Preds K=10)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f10325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Comparisons ===\")\n",
    "\n",
    "# 1. Compare MLE K=5 vs MLE K=10 Predictions\n",
    "print(\"--- MLE K=5 vs MLE K=10 (Predictions) ---\")\n",
    "for tid in target_ids:\n",
    "    if tid in preds_5 and tid in preds_10:\n",
    "        p5 = preds_5[tid].set_index('userId')['predicted_rating_mle']\n",
    "        p10 = preds_10[tid].set_index('userId')['predicted_rating_mle']\n",
    "        \n",
    "        # Align indices\n",
    "        common = p5.index.intersection(p10.index)\n",
    "        mae = np.mean(np.abs(p5.loc[common] - p10.loc[common]))\n",
    "        print(f\"Target {tid}: MAE between K=5 and K=10 predictions: {mae:.4f}\")\n",
    "\n",
    "# 2. Compare Part 1 (Mean Filling) vs Part 2 (MLE)\n",
    "# Load Part 1 Predictions\n",
    "print(\"\\n--- Part 1 (Mean Fill) vs Part 2 (MLE) ---\")\n",
    "utils_tables_path = \"../results/tables\"\n",
    "\n",
    "for tid in target_ids:\n",
    "    # Load Part 1 K=5 (Point 9 in Part 1)\n",
    "    p1_k5_file = os.path.join(utils_tables_path, f\"3.2.9_predictions_target_{tid}.csv\")\n",
    "    \n",
    "    if os.path.exists(p1_k5_file):\n",
    "        p1_k5_df = pd.read_csv(p1_k5_file)\n",
    "        # Check column names in Part 1 file\n",
    "        # Usually 'predicted_rating_final'\n",
    "        if 'predicted_rating_final' in p1_k5_df.columns:\n",
    "            p1_series = p1_k5_df.set_index('userId')['predicted_rating_final']\n",
    "            \n",
    "            # Compare with MLE K=5\n",
    "            if tid in preds_5:\n",
    "                mle_series = preds_5[tid].set_index('userId')['predicted_rating_mle']\n",
    "                common = p1_series.index.intersection(mle_series.index)\n",
    "                mae = np.mean(np.abs(p1_series.loc[common] - mle_series.loc[common]))\n",
    "                print(f\"Target {tid} (K=5): MAE MeanFill vs MLE: {mae:.4f}\")\n",
    "            \n",
    "            # Compare with MLE K=10\n",
    "            # Assuming Part 1 K=10 file follows pattern? \n",
    "            # (Accessing Part 1 point 11 - assuming it was saved similarly or we skip)\n",
    "            # If Part 1 didn't explicitly save K=10 separate from K=5 (Part 1 notebook ended at Task 9 with K=5),\n",
    "            # we might only compare K=5.\n",
    "            pass\n",
    "    else:\n",
    "        print(f\"Part 1 Predictions for Target {tid} not found at {p1_k5_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9fdb01",
   "metadata": {},
   "source": [
    "### Steps 8-9: Comparison with Part 1\n",
    "Compare with results from Part 1 Points 9 and 11.\n",
    "- Reference your previous run's output for Part 1 to make specific comments.\n",
    "- Generally, MLE PCA (Part 2) ignores missing entries for correlations, providing valid structure analysis even with sparse data, whereas Mean Filling (Part 1) tends to dampen correlations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
