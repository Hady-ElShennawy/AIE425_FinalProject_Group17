{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2b1be7",
   "metadata": {},
   "source": [
    "# PCA with Mean Filling\n",
    "Eyad Medhat 221100279/ Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed872307",
   "metadata": {},
   "source": [
    "Task 1: Calculate Average Rating for Target Items I1 and I2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9293850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2254df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading target items from: ..\\results\\tables\\lowest_two_rateditems.csv\n",
      "Target Items Loaded:\n",
      "   movieId  mean_rating_per_movie  rating_count_per_movie\n",
      "0     1556               1.919431                     422\n",
      "1     1499               2.059603                     453\n",
      "Target Item 1 (I1): 1556.0\n",
      "Target Item 2 (I2): 1499.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Target Items (I1, I2) from the previous results\n",
    "target_items_path = os.path.join('..', 'results', 'tables', 'lowest_two_rateditems.csv')\n",
    "print(f\"Loading target items from: {target_items_path}\")\n",
    "\n",
    "if os.path.exists(target_items_path):\n",
    "    target_items_df = pd.read_csv(target_items_path)\n",
    "    print(\"Target Items Loaded:\")\n",
    "    print(target_items_df)\n",
    "    \n",
    "    # Access by position to ensure we get the first two rows regardless of index\n",
    "    I1 = target_items_df.iloc[0]['movieId']\n",
    "    I2 = target_items_df.iloc[1]['movieId']\n",
    "    print(f\"Target Item 1 (I1): {I1}\")\n",
    "    print(f\"Target Item 2 (I2): {I2}\")\n",
    "else:\n",
    "    print(\"ERROR: Target items file not found. Make sure previous steps were run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb8145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found cached sample at: ..\\data\\ml-20m\\ratings_cleaned_sampled.csv\n",
      "Ratings loaded. Shape: (1000000, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the Ratings Data\n",
    "ratings_df = load_data()\n",
    "\n",
    "if ratings_df is None:\n",
    "    print(\"Trying raw ratings...\")\n",
    "    ratings_df = utils.load_data(os.path.join('ml-20m', 'ratings.csv'))\n",
    "\n",
    "if ratings_df is not None:\n",
    "    print(f\"Ratings loaded. Shape: {ratings_df.shape}\")\n",
    "else:\n",
    "    print(\"FAILED to load ratings data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b77b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculated Stats ---\n",
      "Item I1 (ID: 1556.0): Mean Rating = 1.9194, Count = 422\n",
      "Item I2 (ID: 1499.0): Mean Rating = 2.0596, Count = 453\n",
      "\n",
      "--- Verification ---\n",
      "Stored I1 Mean: 1.919431279620853\n",
      "Stored I2 Mean: 2.0596026490066226\n",
      "    Saved CSV: tables/task3.2.1.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Calculate Average Rating for I1 and I2\n",
    "if ratings_df is not None and 'I1' in locals():\n",
    "    # Filter for I1\n",
    "    i1_ratings = ratings_df[ratings_df['movieId'] == I1]\n",
    "    i1_mean = i1_ratings['rating'].mean()\n",
    "    i1_count = len(i1_ratings)\n",
    "    \n",
    "    # Filter for I2\n",
    "    i2_ratings = ratings_df[ratings_df['movieId'] == I2]\n",
    "    i2_mean = i2_ratings['rating'].mean()\n",
    "    i2_count = len(i2_ratings)\n",
    "    \n",
    "    print(f\"\\n--- Calculated Stats ---\")\n",
    "    print(f\"Item I1 (ID: {I1}): Mean Rating = {i1_mean:.4f}, Count = {i1_count}\")\n",
    "    print(f\"Item I2 (ID: {I2}): Mean Rating = {i2_mean:.4f}, Count = {i2_count}\")\n",
    "    \n",
    "    # Verification against loaded values\n",
    "    print(f\"\\n--- Verification ---\")\n",
    "    print(f\"Stored I1 Mean: {target_items_df.iloc[0]['mean_rating_per_movie']}\")\n",
    "    print(f\"Stored I2 Mean: {target_items_df.iloc[1]['mean_rating_per_movie']}\")\n",
    "    \n",
    "    # --- SAVE RESULT ---\n",
    "    task1_data = [\n",
    "        {'movieId': I1, 'mean_rating': i1_mean, 'count': i1_count},\n",
    "        {'movieId': I2, 'mean_rating': i2_mean, 'count': i2_count}\n",
    "    ]\n",
    "    task1_df = pd.DataFrame(task1_data)\n",
    "    save_csv(task1_df, 'task3.2.1.csv')\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot calculate stats: Missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a900d7",
   "metadata": {},
   "source": [
    "## Task 2: Mean Filling and Dataset Augmentation\n",
    "Fill missing ratings for Target Items I1 and I2 with their mean value (1.0) and save the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15438210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting dataset with filled ratings for I1 and I2...\n",
      "Total unique users: 96345\n",
      "Users missing rating for I1: 95923\n",
      "Users missing rating for I2: 95892\n",
      "Creating dataframe for 191815 new ratings...\n",
      "Appending to original dataset...\n",
      "Original Shape: (1000000, 3)\n",
      "Augmented Shape: (1191815, 3)\n",
      "Final Count I1: 96345 (Should be 96345) \n",
      "Final Count I2: 96345 (Should be 96345) \n",
      "Saving augmented dataset to ..\\results\\tables\\ratings_filled_targets.csv...\n",
      "Save Complete.\n"
     ]
    }
   ],
   "source": [
    "augmented_df = None # Initialize\n",
    "if ratings_df is not None and 'I1' in locals():\n",
    "    print(\"Augmenting dataset with filled ratings for I1 and I2...\")\n",
    "    \n",
    "    all_users = ratings_df['userId'].unique()\n",
    "    print(f\"Total unique users: {len(all_users)}\")\n",
    "    \n",
    "    new_rows = []\n",
    "    \n",
    "    # --- Process I1 ---\n",
    "    # Find users who rated I1\n",
    "    users_rated_i1 = set(ratings_df[ratings_df['movieId'] == I1]['userId'].unique())\n",
    "    # Find missing\n",
    "    users_missing_i1 = [u for u in all_users if u not in users_rated_i1]\n",
    "    print(f\"Users missing rating for I1: {len(users_missing_i1)}\")\n",
    "    \n",
    "    # Create rows for missing I1\n",
    "    for u in users_missing_i1:\n",
    "        new_rows.append({'userId': u, 'movieId': I1, 'rating': i1_mean})\n",
    "        \n",
    "    # --- Process I2 ---\n",
    "    # Find users who rated I2\n",
    "    users_rated_i2 = set(ratings_df[ratings_df['movieId'] == I2]['userId'].unique())\n",
    "    # Find missing\n",
    "    users_missing_i2 = [u for u in all_users if u not in users_rated_i2]\n",
    "    print(f\"Users missing rating for I2: {len(users_missing_i2)}\")\n",
    "    \n",
    "    # Create rows for missing I2\n",
    "    for u in users_missing_i2:\n",
    "        new_rows.append({'userId': u, 'movieId': I2, 'rating': i2_mean})\n",
    "    \n",
    "    print(f\"Creating dataframe for {len(new_rows)} new ratings...\")\n",
    "    new_ratings_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    # Concatenate\n",
    "    print(\"Appending to original dataset...\")\n",
    "    augmented_df = pd.concat([ratings_df, new_ratings_df], ignore_index=True)\n",
    "    \n",
    "    # Sort for tidiness (optional)\n",
    "    augmented_df.sort_values(by=['userId', 'movieId'], inplace=True)\n",
    "    \n",
    "    print(f\"Original Shape: {ratings_df.shape}\")\n",
    "    print(f\"Augmented Shape: {augmented_df.shape}\")\n",
    "    \n",
    "    # Verify counts\n",
    "    final_i1_count = len(augmented_df[augmented_df['movieId'] == I1])\n",
    "    final_i2_count = len(augmented_df[augmented_df['movieId'] == I2])\n",
    "    print(f\"Final Count I1: {final_i1_count} (Should be {len(all_users)}) \")\n",
    "    print(f\"Final Count I2: {final_i2_count} (Should be {len(all_users)}) \")\n",
    "    \n",
    "    # --- SAVE RESULT ---\n",
    "    # Saving to Results folder as requested: 'ratings_filled_targets.csv'\n",
    "    output_filename = 'ratings_filled_targets.csv'\n",
    "    output_path = os.path.join('..', 'results', 'tables', output_filename)\n",
    "    \n",
    "    # Ensure dir exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving augmented dataset to {output_path}...\")\n",
    "    augmented_df.to_csv(output_path, index=False)\n",
    "    print(\"Save Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping augmentation due to missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09100790",
   "metadata": {},
   "source": [
    "## Task 3: Calculate Average Ratings from Augmented Data\n",
    "Calculate the mean rating for each item in the new augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb5df75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stats for augmented dataset...\n",
      "Stats calculated.\n",
      "   movieId  mean_rating  rating_count\n",
      "0      1.0     3.915355          3875\n",
      "1      2.0     3.202874          1740\n",
      "2      3.0     3.209073          1014\n",
      "3      5.0     3.061538           975\n",
      "4      6.0     3.770856          1846\n",
      "\n",
      "Final Stats I1:\n",
      "     movieId  mean_rating  rating_count\n",
      "429   1556.0     1.919431         96345\n",
      "Final Stats I2:\n",
      "     movieId  mean_rating  rating_count\n",
      "422   1499.0     2.059603         96345\n",
      "Saving final item stats...\n",
      "    Saved CSV: tables/task3.2.3_final_item_stats.csv\n",
      "Task 3 Complete.\n"
     ]
    }
   ],
   "source": [
    "final_stats = None # Initialize\n",
    "if augmented_df is not None:\n",
    "    print(\"Calculating stats for augmented dataset...\")\n",
    "    \n",
    "    # Calculate stats\n",
    "    final_stats = augmented_df.groupby('movieId')['rating'].agg(['mean', 'count']).reset_index()\n",
    "    final_stats.rename(columns={'mean': 'mean_rating', 'count': 'rating_count'}, inplace=True)\n",
    "    \n",
    "    print(\"Stats calculated.\")\n",
    "    print(final_stats.head())\n",
    "    \n",
    "    # Verification for I1 and I2\n",
    "    if 'I1' in locals():\n",
    "        i1_final = final_stats[final_stats['movieId'] == I1]\n",
    "        print(f\"\\nFinal Stats I1:\\n{i1_final}\")\n",
    "        \n",
    "        i2_final = final_stats[final_stats['movieId'] == I2]\n",
    "        print(f\"Final Stats I2:\\n{i2_final}\")\n",
    "        \n",
    "    # --- SAVE RESULT ---\n",
    "    print(\"Saving final item stats...\")\n",
    "    save_csv(final_stats, 'task3.2.3_final_item_stats.csv')\n",
    "    print(\"Task 3 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Augmented dataframe not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21db64",
   "metadata": {},
   "source": [
    "## Task 4: Calculate Diffrence between Ratings and Mean Rating\n",
    "Load the augmented ratings and final item stats from CSVs, then calculate the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee299842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files for Task 4...\n",
      "Loaded Augmented Ratings. Shape: (1191815, 3)\n",
      "Loaded Item Stats. Shape: (1000, 3)\n",
      "Calculating rating differences (Centering)...\n",
      "Difference calculated.\n",
      "   userId  movieId  rating  mean_rating  rating_diff\n",
      "0       1     32.0     4.0     3.881264     0.118736\n",
      "1       1    337.0     4.0     3.755682     0.244318\n",
      "2       1   1193.0     4.0     4.191716    -0.191716\n",
      "3       1   1261.0     4.0     3.848649     0.151351\n",
      "4       1   1370.0     3.0     3.454844    -0.454844\n",
      "\n",
      "Average Global Difference (should be ~0): -0.000000\n",
      "Saving centered ratings...\n",
      "    Saved CSV: tables/task3.2.4_centered_ratings.csv\n",
      "Task 4 Complete.\n"
     ]
    }
   ],
   "source": [
    "# Load data from results folder\n",
    "augmented_csv_path = os.path.join('..', 'results', 'tables', 'ratings_filled_targets.csv')\n",
    "stats_csv_path = os.path.join('..', 'results', 'tables', 'task3.2.3_final_item_stats.csv')\n",
    "\n",
    "print(\"Loading files for Task 4...\")\n",
    "\n",
    "has_data_task4 = True\n",
    "if os.path.exists(augmented_csv_path):\n",
    "    # Use float32 for ratings to save memory if dataset is large\n",
    "    # Reading only necessary cols if we were pipelining, but here we read all\n",
    "    task4_ratings = pd.read_csv(augmented_csv_path)\n",
    "    print(f\"Loaded Augmented Ratings. Shape: {task4_ratings.shape}\")\n",
    "else:\n",
    "    print(f\"Error: {augmented_csv_path} not found.\")\n",
    "    has_data_task4 = False\n",
    "    \n",
    "if os.path.exists(stats_csv_path):\n",
    "    task4_stats = pd.read_csv(stats_csv_path)\n",
    "    print(f\"Loaded Item Stats. Shape: {task4_stats.shape}\")\n",
    "else:\n",
    "    print(f\"Error: {stats_csv_path} not found.\")\n",
    "    has_data_task4 = False\n",
    "\n",
    "if has_data_task4:\n",
    "    print(\"Calculating rating differences (Centering)...\")\n",
    "    \n",
    "    # Merge\n",
    "    # We want merged_df to contain 'mean_rating' from task4_stats matching 'movieId'\n",
    "    merged_df = task4_ratings.merge(task4_stats[['movieId', 'mean_rating']], on='movieId', how='left')\n",
    "    \n",
    "    # Calculate difference\n",
    "    merged_df['rating_diff'] = merged_df['rating'] - merged_df['mean_rating']\n",
    "    \n",
    "    print(\"Difference calculated.\")\n",
    "    print(merged_df[['userId', 'movieId', 'rating', 'mean_rating', 'rating_diff']].head())\n",
    "    \n",
    "    # Verify centering (Mean difference should be close to 0)\n",
    "    avg_diff = merged_df['rating_diff'].mean()\n",
    "    print(f\"\\nAverage Global Difference (should be ~0): {avg_diff:.6f}\")\n",
    "    \n",
    "    # --- SAVE RESULT ---\n",
    "    print(\"Saving centered ratings...\")\n",
    "    save_csv(merged_df, 'task3.2.4_centered_ratings.csv')\n",
    "    print(\"Task 4 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot proceed with Task 4 due to missing files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8011c87",
   "metadata": {},
   "source": [
    "## Task 5: Calculate Partial Covariance (Targets vs All)\n",
    "Compute the covariance where we calculate Cov(Target, Item_j) for all Items j.\n",
    "We use the helper function from utils to do this efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120c063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Items IDs: 1556.0, 1499.0\n",
      "Loading dataset...\n",
      "Calculating partial covariance matrix (2 x All Items)...\n",
      "Starting MANUAL covariance calculation...\n",
      "Total N (Users): 96345\n",
      "Total Items: 1000\n",
      "Users who rated targets: 96345\n",
      "Building efficient lookup dictionary...\n",
      "Lookup built. Calculating sums...\n",
      "Processing Target Item: 1556.0...\n",
      "Processing Target Item: 1499.0...\n",
      "Formatting results...\n",
      "Calculation Complete.\n",
      "Result Shape: (2, 1000)\n",
      "Sample (first 5 cols):\n",
      "             1.0      2.0       3.0       5.0       6.0\n",
      "1556.0  0.000013  0.00007 -0.000016  0.000042  0.000027\n",
      "1499.0  0.000058  0.00010  0.000024  0.000046  0.000014\n",
      "Saving intermediate partial covariance to ..\\results\\tables\\3.2.5_target_only_covariances.csv...\n",
      "Task 5 Complete.\n"
     ]
    }
   ],
   "source": [
    "centered_ratings_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
    "target_items_path = os.path.join('..', 'results', 'tables', 'lowest_two_rateditems.csv')\n",
    "\n",
    "if os.path.exists(centered_ratings_path) and os.path.exists(target_items_path):\n",
    "    # 1. Load Target Items to identify I1 and I2\n",
    "    target_items_df = pd.read_csv(target_items_path)\n",
    "    I1 = target_items_df.iloc[0]['movieId']\n",
    "    I2 = target_items_df.iloc[1]['movieId']\n",
    "    print(f\"Target Items IDs: {I1}, {I2}\")\n",
    "\n",
    "    # 2. Load Centered Ratings\n",
    "    print(\"Loading dataset...\")\n",
    "    # Need all data to calculate correlation against everything\n",
    "    df = pd.read_csv(centered_ratings_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
    "    \n",
    "    # 3. Calculate Target Covariances using Utils\n",
    "    print(\"Calculating partial covariance matrix (2 x All Items)...\")\n",
    "    \n",
    "    targets = [I1, I2]\n",
    "    partial_cov_df = calculate_target_covariance(df, targets)\n",
    "    \n",
    "    print(\"Calculation Complete.\")\n",
    "    print(f\"Result Shape: {partial_cov_df.shape}\")\n",
    "    print(\"Sample (first 5 cols):\")\n",
    "    print(partial_cov_df.iloc[:, :5])\n",
    "    \n",
    "    # Task 5 & 6 effectively merge here because Task 6 was just \"Generate Matrix\"\n",
    "    # But to separate them as requested:\n",
    "    \n",
    "    # Task 5 Save: We can save this intermediate result\n",
    "    output_filename = '3.2.5_target_only_covariances.csv'\n",
    "    output_path = os.path.join('..', 'results', 'tables', output_filename)\n",
    "    print(f\"Saving intermediate partial covariance to {output_path}...\")\n",
    "    partial_cov_df.to_csv(output_path)\n",
    "    print(\"Task 5 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Required files not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557451f",
   "metadata": {},
   "source": [
    "## Task 6: Reference Full Covariance Matrix Calculation\n",
    "Calculates the FULL N x N covariance matrix for all items using sparse algebra and saves it as a compressed .npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e03664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading centered ratings for Full Matrix calculation...\n",
      "Calculating Full Sparse Matrix (N x N)...\n",
      "Preparing for Full Sparse Covariance Calculation...\n",
      "Dimensions: 96345 Users x 1000 Items\n",
      "Constructing User-Item Sparse Matrix...\n",
      "Computing X.T @ X ... (This may take a moment)\n",
      "Dividing by N-1...\n",
      "Matrix Shape: (1000, 1000)\n",
      "Saving Sparse Matrix to ..\\results\\tables\\3.2.6_full_covariance.npz...\n",
      "Writing PARTIAL (60%) Full Matrix to CSV: ..\\results\\tables\\3.2.6_full_covariance_partial.csv\n",
      "Writing in chunks...\n",
      "Total items: 1000. Writing first 600 items.\n",
      "  Written rows 0 to 600 / 600 (0.8s)\n",
      "CSV Write Complete. Time: 0.8s\n",
      "Task 6 Complete.\n"
     ]
    }
   ],
   "source": [
    "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
    "\n",
    "if os.path.exists(centered_path):\n",
    "    print(\"Loading centered ratings for Full Matrix calculation...\")\n",
    "    df_full = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
    "    \n",
    "    print(\"Calculating Full Sparse Matrix (N x N)...\")\n",
    "    sparse_cov, movie_ids = calculate_full_covariance_sparse(df_full)\n",
    "    \n",
    "    print(f\"Matrix Shape: {sparse_cov.shape}\")\n",
    "    \n",
    "    # Save Matrix (Sparse)\n",
    "    out_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance.npz')\n",
    "    print(f\"Saving Sparse Matrix to {out_path}...\")\n",
    "    sparse.save_npz(out_path, sparse_cov)\n",
    "    \n",
    "    # Save ID Mapping\n",
    "    id_map_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance_ids.csv')\n",
    "    pd.DataFrame(movie_ids, columns=['movieId']).to_csv(id_map_path, index=False)\n",
    "    \n",
    "    # --- MASSIVE CSV WRITE WITH CHUNKING (PARTIAL 60%) ---\n",
    "    full_csv_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance_partial.csv')\n",
    "    print(f\"Writing PARTIAL (60%) Full Matrix to CSV: {full_csv_path}\")\n",
    "    print(\"Writing in chunks...\")\n",
    "    \n",
    "    chunk_size = 1000\n",
    "    num_items = sparse_cov.shape[0]\n",
    "    limit_items = int(num_items * 0.60) # 60% Limit\n",
    "    \n",
    "    print(f\"Total items: {num_items}. Writing first {limit_items} items.\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, limit_items, chunk_size):\n",
    "        end_i = min(i + chunk_size, limit_items)\n",
    "        \n",
    "        # Extract chunk and make dense\n",
    "        chunk_dense = sparse_cov[i:end_i].toarray()\n",
    "        \n",
    "        # Create DF\n",
    "        # Index is subset of IDs, Columns is ALL IDs\n",
    "        chunk_index = movie_ids[i:end_i]\n",
    "        chunk_df = pd.DataFrame(chunk_dense, index=chunk_index, columns=movie_ids)\n",
    "        \n",
    "        # Write mode: 'w' for first chunk, 'a' for others\n",
    "        # Header: True for first chunk, False for others\n",
    "        if i == 0:\n",
    "            chunk_df.to_csv(full_csv_path, mode='w', header=True)\n",
    "        else:\n",
    "            chunk_df.to_csv(full_csv_path, mode='a', header=False)\n",
    "            \n",
    "        # Progress log\n",
    "        if i % 3000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  Written rows {i} to {end_i} / {limit_items} ({elapsed:.1f}s)\")\n",
    "    \n",
    "    print(f\"CSV Write Complete. Time: {time.time() - start_time:.1f}s\")\n",
    "    print(\"Task 6 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Input file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d16fd",
   "metadata": {},
   "source": [
    "## Task 7: Determine Top 5 and Top 10 Peers\n",
    "Identify the items with the highest covariance with each target item (I1 and I2) using the generated full matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f43929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Full Sparse Matrix...\n",
      "Loading IDs...\n",
      "Loading Targets...\n",
      "\n",
      "Processing Peers for Target: 1556.0\n",
      "Top 5 Peers for 1556.0:\n",
      "1370.0    0.000210\n",
      "1580.0    0.000203\n",
      "1722.0    0.000160\n",
      "1591.0    0.000151\n",
      "780.0     0.000136\n",
      "dtype: float64\n",
      "\n",
      "Top 10 Peers for 1556.0:\n",
      "1370.0    0.000210\n",
      "1580.0    0.000203\n",
      "1722.0    0.000160\n",
      "1591.0    0.000151\n",
      "780.0     0.000136\n",
      "1831.0    0.000133\n",
      "1917.0    0.000132\n",
      "595.0     0.000129\n",
      "2407.0    0.000127\n",
      "173.0     0.000126\n",
      "dtype: float64\n",
      "\n",
      "Processing Peers for Target: 1499.0\n",
      "Top 5 Peers for 1499.0:\n",
      "231.0     0.000195\n",
      "2722.0    0.000194\n",
      "1438.0    0.000169\n",
      "379.0     0.000161\n",
      "780.0     0.000154\n",
      "dtype: float64\n",
      "\n",
      "Top 10 Peers for 1499.0:\n",
      "231.0     0.000195\n",
      "2722.0    0.000194\n",
      "1438.0    0.000169\n",
      "379.0     0.000161\n",
      "780.0     0.000154\n",
      "377.0     0.000154\n",
      "1552.0    0.000151\n",
      "434.0     0.000144\n",
      "3623.0    0.000140\n",
      "1573.0    0.000139\n",
      "dtype: float64\n",
      "\n",
      "Saving Top Peers to ..\\results\\tables\\3.2.7_top_peers.csv...\n",
      "Task 7 Complete.\n"
     ]
    }
   ],
   "source": [
    "matrix_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance.npz')\n",
    "id_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance_ids.csv')\n",
    "target_items_path = os.path.join('..', 'results', 'tables', 'lowest_two_rateditems.csv')\n",
    "\n",
    "if os.path.exists(matrix_path) and os.path.exists(id_path) and os.path.exists(target_items_path):\n",
    "    print(\"Loading Full Sparse Matrix...\")\n",
    "    cov_matrix_sparse = sparse.load_npz(matrix_path)\n",
    "    \n",
    "    print(\"Loading IDs...\")\n",
    "    movie_ids_df = pd.read_csv(id_path)\n",
    "    all_movie_ids = movie_ids_df['movieId'].tolist()\n",
    "    # Create index map for fast lookup\n",
    "    id_to_idx = {mid: i for i, mid in enumerate(all_movie_ids)}\n",
    "    \n",
    "    print(\"Loading Targets...\")\n",
    "    targets_df = pd.read_csv(target_items_path)\n",
    "    targets = [targets_df.iloc[0]['movieId'], targets_df.iloc[1]['movieId']]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for tid in targets:\n",
    "        print(f\"\\nProcessing Peers for Target: {tid}\")\n",
    "        if tid in id_to_idx:\n",
    "            row_idx = id_to_idx[tid]\n",
    "            \n",
    "            # Extract dense row from sparse matrix\n",
    "            # Get row slice -> to dense array -> flatten to 1D\n",
    "            cov_row = cov_matrix_sparse.getrow(row_idx).toarray().flatten()\n",
    "            \n",
    "            # Create Series for sorting\n",
    "            row_series = pd.Series(cov_row, index=all_movie_ids)\n",
    "            \n",
    "            # Exclude self\n",
    "            if tid in row_series.index:\n",
    "                row_series = row_series.drop(tid)\n",
    "                \n",
    "            # Sort & Top 10\n",
    "            top_10 = row_series.sort_values(ascending=False).head(10)\n",
    "            \n",
    "            print(f\"Top 5 Peers for {tid}:\")\n",
    "            print(top_10.head(5))\n",
    "\n",
    "            print(f\"\\nTop 10 Peers for {tid}:\")\n",
    "            print(top_10)\n",
    "            \n",
    "            # Store results\n",
    "            rank = 1\n",
    "            for peer_id, cv in top_10.items():\n",
    "                results.append({\n",
    "                    'Target_Item': tid,\n",
    "                    'Rank': rank,\n",
    "                    'Peer_Item': peer_id,\n",
    "                    'Covariance': cv\n",
    "                })\n",
    "                rank += 1\n",
    "        else:\n",
    "            print(f\"Target {tid} not found in matrix indices.\")\n",
    "            \n",
    "    # Save Results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    out_path = os.path.join('..', 'results', 'tables', '3.2.7_top_peers.csv')\n",
    "    \n",
    "    print(f\"\\nSaving Top Peers to {out_path}...\")\n",
    "    results_df.to_csv(out_path, index=False)\n",
    "    print(\"Task 7 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Missing required files for Task 7. Run Task 6 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39adba8a",
   "metadata": {},
   "source": [
    "## Task 8: Determine Reduced Dimensional Space (Top 5 Peers)\n",
    "Create a reduced dimensional space for each user by selecting their centered ratings for the Top 5 peer items identified in Task 7.\n",
    "Missing values are filled with 0 (since ratings are centered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb63cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Top Peers and Centered Ratings...\n",
      "Total Users: 96345\n",
      "\n",
      "--- Processing Target 1556.0 ---\n",
      "Top 5 Peers: [1370.0, 1580.0, 1722.0, 1591.0, 780.0]\n",
      "Creating Reduced Space Matrix (Pivoting)...\n",
      "Reduced Matrix Shape: (96345, 5)\n",
      "movieId  780.0     1370.0  1580.0  1591.0  1722.0\n",
      "userId                                           \n",
      "1           0.0 -0.454844     0.0     0.0     0.0\n",
      "2           0.0  0.000000     0.0     0.0     0.0\n",
      "3           0.0  0.000000     0.0     0.0     0.0\n",
      "4           0.0  0.000000     0.0     0.0     0.0\n",
      "5           0.0  0.000000     0.0     0.0     0.0\n",
      "Saving to 3.2.8_reduced_space_target_1556.csv...\n",
      "    Saved CSV: tables/3.2.8_reduced_space_target_1556.csv\n",
      "\n",
      "--- Processing Target 1499.0 ---\n",
      "Top 5 Peers: [231.0, 2722.0, 1438.0, 379.0, 780.0]\n",
      "Creating Reduced Space Matrix (Pivoting)...\n",
      "Reduced Matrix Shape: (96345, 5)\n",
      "movieId  231.0   379.0   780.0   1438.0  2722.0\n",
      "userId                                         \n",
      "1           0.0     0.0     0.0     0.0     0.0\n",
      "2           0.0     0.0     0.0     0.0     0.0\n",
      "3           0.0     0.0     0.0     0.0     0.0\n",
      "4           0.0     0.0     0.0     0.0     0.0\n",
      "5           0.0     0.0     0.0     0.0     0.0\n",
      "Saving to 3.2.8_reduced_space_target_1499.csv...\n",
      "    Saved CSV: tables/3.2.8_reduced_space_target_1499.csv\n",
      "\n",
      "Task 8 Complete.\n"
     ]
    }
   ],
   "source": [
    "peers_path = os.path.join('..', 'results', 'tables', '3.2.7_top_peers.csv')\n",
    "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
    "\n",
    "if os.path.exists(peers_path) and os.path.exists(centered_path):\n",
    "    print(\"Loading Top Peers and Centered Ratings...\")\n",
    "    peers_df = pd.read_csv(peers_path)\n",
    "    # Load centered ratings (we need userId, movieId, rating_diff)\n",
    "    centered_df = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
    "    \n",
    "    # Get all unique users to ensure complete index\n",
    "    all_users = centered_df['userId'].unique()\n",
    "    print(f\"Total Users: {len(all_users)}\")\n",
    "    \n",
    "    targets = peers_df['Target_Item'].unique()\n",
    "    \n",
    "    for target in targets:\n",
    "        print(f\"\\n--- Processing Target {target} ---\")\n",
    "        \n",
    "        # 1. Identify Top 5 Peers\n",
    "        # Sort just in case, though file should be sorted\n",
    "        target_peers = peers_df[peers_df['Target_Item'] == target].sort_values('Rank')\n",
    "        top_5_peers = target_peers.head(5)['Peer_Item'].tolist()\n",
    "        print(f\"Top 5 Peers: {top_5_peers}\")\n",
    "        \n",
    "        # 2. Filter Centered Ratings for these items\n",
    "        subset_df = centered_df[centered_df['movieId'].isin(top_5_peers)]\n",
    "        \n",
    "        # 3. Pivot to create User x Peer Matrix\n",
    "        print(\"Creating Reduced Space Matrix (Pivoting)...\")\n",
    "        reduced_matrix = subset_df.pivot(index='userId', columns='movieId', values='rating_diff')\n",
    "        \n",
    "        # 4. Handle Missing Values (Fill with 0 -> Mean)\n",
    "        reduced_matrix = reduced_matrix.fillna(0.0)\n",
    "        \n",
    "        # 5. Ensure all users are represented (Reindex)\n",
    "        # This adds rows for users who rated NONE of the top 5 peers, filling with 0\n",
    "        reduced_matrix = reduced_matrix.reindex(all_users, fill_value=0.0)\n",
    "        \n",
    "        print(f\"Reduced Matrix Shape: {reduced_matrix.shape}\")\n",
    "        print(reduced_matrix.head())\n",
    "        \n",
    "        # 6. Save\n",
    "        out_filename = f'3.2.8_reduced_space_target_{int(target)}.csv'\n",
    "        print(f\"Saving to {out_filename}...\")\n",
    "        # Reset index to include userId in the CSV\n",
    "        save_csv(reduced_matrix.reset_index(), out_filename)\n",
    "        \n",
    "    print(\"\\nTask 8 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: Required files (Top Peers or Centered Ratings) not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb784d5",
   "metadata": {},
   "source": [
    "## Task 9: Rating Prediction using Linear Regression (Top 5 Peers)\n",
    "Use the reduced dimensional space (Top 5 Peers) to train a Linear Regression model for each target item.\n",
    "Predict the centered ratings for users who haven't rated the target item, then add the mean rating to get the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5546c106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stats, centered ratings, and original ratings... (Task 9)\n",
      "\n",
      "--- Modeling for Target Item: 1499 ---\n",
      "Original Raters: 5781\n",
      "Training set size: 4132\n",
      "Prediction set size: 92213\n",
      "Model Coefficients: [0.05714483 0.09677566 0.02999486 0.08145581 0.10631333]\n",
      "Model Intercept: 0.0006293768857684765\n",
      "Saving predictions to 3.2.9_predictions_target_1499.csv...\n",
      "    Saved CSV: tables/3.2.9_predictions_target_1499.csv\n",
      "\n",
      "--- Modeling for Target Item: 1556 ---\n",
      "Original Raters: 5326\n",
      "Training set size: 3813\n",
      "Prediction set size: 92532\n",
      "Model Coefficients: [0.02859996 0.08750451 0.06818709 0.09675695 0.07495646]\n",
      "Model Intercept: 0.00046602415125713686\n",
      "Saving predictions to 3.2.9_predictions_target_1556.csv...\n",
      "    Saved CSV: tables/3.2.9_predictions_target_1556.csv\n",
      "\n",
      "Task 9 Complete.\n"
     ]
    }
   ],
   "source": [
    "# Task 9: Prediction\n",
    "stats_path = os.path.join('..', 'results', 'tables', 'task3.2.3_final_item_stats.csv')\n",
    "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
    "ratings_path = os.path.join('ml-20m', 'ratings_cleaned.csv') \n",
    "\n",
    "# Flexible path lookup\n",
    "if not os.path.exists(ratings_path):\n",
    "    ratings_path = os.path.join('..', 'data', 'ml-20m', 'ratings_cleaned.csv')\n",
    "\n",
    "if os.path.exists(stats_path) and os.path.exists(centered_path) and os.path.exists(ratings_path):\n",
    "    print(\"Loading stats, centered ratings, and original ratings... (Task 9)\")\n",
    "    item_stats = pd.read_csv(stats_path)\n",
    "    # Load centered ratings\n",
    "    centered_df = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
    "    # Load ORIGINAL ratings to find real raters (because centered_df has filled means)\n",
    "    original_ratings = pd.read_csv(ratings_path, usecols=['userId', 'movieId'])\n",
    "    \n",
    "    results_dir = os.path.join('..', 'results', 'tables')\n",
    "    target_files = [f for f in os.listdir(results_dir) if f.startswith('3.2.8_reduced_space_target_')]\n",
    "    \n",
    "    for t_file in target_files:\n",
    "        try:\n",
    "            target_id = int(t_file.split('_')[-1].replace('.csv', ''))\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- Modeling for Target Item: {target_id} ---\")\n",
    "        \n",
    "        # 1. Identify ORIGINAL Raters (Train Set)\n",
    "        real_raters = set(original_ratings[original_ratings['movieId'] == target_id]['userId'].unique())\n",
    "        print(f\"Original Raters: {len(real_raters)}\")\n",
    "        \n",
    "        # 2. Load Reduced Space (Features X)\n",
    "        reduced_space_path = os.path.join(results_dir, t_file)\n",
    "        reduced_df = pd.read_csv(reduced_space_path)\n",
    "        \n",
    "        # 3. Prepare Target Variable (y)\n",
    "        target_centered = centered_df[centered_df['movieId'] == target_id][['userId', 'rating_diff']]\n",
    "        \n",
    "        # 4. Merge\n",
    "        data = reduced_df.merge(target_centered, on='userId', how='left')\n",
    "        \n",
    "        feature_cols = [c for c in reduced_df.columns if c != 'userId']\n",
    "        \n",
    "        # 5. Split using Real Raters\n",
    "        train_data = data[data['userId'].isin(real_raters)]\n",
    "        predict_data = data[~data['userId'].isin(real_raters)]\n",
    "        \n",
    "        # Filter NaNs in Train if any\n",
    "        train_data = train_data[train_data['rating_diff'].notna()]\n",
    "        \n",
    "        print(f\"Training set size: {len(train_data)}\")\n",
    "        print(f\"Prediction set size: {len(predict_data)}\")\n",
    "        \n",
    "        if len(train_data) > 0:\n",
    "            # Train\n",
    "            X_train = train_data[feature_cols]\n",
    "            y_train = train_data['rating_diff']\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            print(f\"Model Coefficients: {model.coef_}\")\n",
    "            print(f\"Model Intercept: {model.intercept_}\")\n",
    "            \n",
    "            # Predict\n",
    "            if len(predict_data) > 0:\n",
    "                X_predict = predict_data[feature_cols]\n",
    "                y_pred_centered = model.predict(X_predict)\n",
    "                \n",
    "                target_mean = item_stats[item_stats['movieId'] == target_id]['mean_rating'].values[0]\n",
    "                y_pred_final = y_pred_centered + target_mean\n",
    "                \n",
    "                predictions_df = pd.DataFrame({\n",
    "                    'userId': predict_data['userId'],\n",
    "                    'movieId': target_id,\n",
    "                    'predicted_rating_centered': y_pred_centered,\n",
    "                    'predicted_rating_final': y_pred_final\n",
    "                })\n",
    "                \n",
    "                out_name = f'3.2.9_predictions_target_{target_id}.csv'\n",
    "                print(f\"Saving predictions to {out_name}...\")\n",
    "                save_csv(predictions_df, out_name)\n",
    "            else:\n",
    "                print(\"No users to predict for.\")\n",
    "        else:\n",
    "            print(\"No training data available.\")\n",
    "            \n",
    "    print(\"\\nTask 9 Complete.\")\n",
    "else:\n",
    "    print(\"Required files for Task 9 missing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
