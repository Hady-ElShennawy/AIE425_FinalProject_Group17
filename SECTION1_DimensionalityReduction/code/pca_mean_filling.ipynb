{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "092af0a7",
            "metadata": {},
            "source": [
                "# Group 17: \n",
                "### Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1e2b1be7",
            "metadata": {},
            "source": [
                "# PCA with Mean Filling\n",
                "Eyad Medhat 221100279/ Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ed872307",
            "metadata": {},
            "source": [
                "Task 1: Calculate Average Rating for Target Items I1 and I2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "9293850c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Results folder exists at: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\n",
                        "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\\plots\n",
                        "Subfolder exists: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\\tables\n"
                    ]
                }
            ],
            "source": [
                "from utils import *"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "3a2254df",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " Found requested table at: ..\\results\\tables\\lowest_two_rateditems.csv\n",
                        "Target Items Loaded:\n",
                        "   movieId  mean_rating_per_movie  rating_count_per_movie\n",
                        "0     1556               1.919431                     422\n",
                        "1     1499               2.059603                     453\n",
                        "Target Item 1 (I1): 1556.0\n",
                        "Target Item 2 (I2): 1499.0\n"
                    ]
                }
            ],
            "source": [
                "# 1. Load the Target Items (I1, I2) from the previous results\n",
                "target_items_df = load_data(table_name='lowest_two_rateditems.csv')\n",
                "\n",
                "if target_items_df is not None:\n",
                "    print(\"Target Items Loaded:\")\n",
                "    print(target_items_df)\n",
                "    \n",
                "    # Access by position to ensure we get the first two rows regardless of index\n",
                "    I1 = target_items_df.iloc[0]['movieId']\n",
                "    I2 = target_items_df.iloc[1]['movieId']\n",
                "    print(f\"Target Item 1 (I1): {I1}\")\n",
                "    print(f\"Target Item 2 (I2): {I2}\")\n",
                "else:\n",
                "    print(\"ERROR: Target items file not found. Make sure previous steps were run.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "fbb8145c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " Found cached sample at: ..\\data\\ml-20m\\ratings_cleaned_sampled.csv\n",
                        "Ratings loaded. Shape: (1000000, 3)\n"
                    ]
                }
            ],
            "source": [
                "# 2. Load the Ratings Data\n",
                "ratings_df = load_data()\n",
                "\n",
                "if ratings_df is not None:\n",
                "    print(f\"Ratings loaded. Shape: {ratings_df.shape}\")\n",
                "else:\n",
                "    print(\"FAILED to load ratings data.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "63b77b6a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Calculated Stats ---\n",
                        "Item I1 (ID: 1556.0): Mean Rating = 1.9194, Count = 422\n",
                        "Item I2 (ID: 1499.0): Mean Rating = 2.0596, Count = 453\n",
                        "\n",
                        "--- Verification ---\n",
                        "Stored I1 Mean: 1.919431279620853\n",
                        "Stored I2 Mean: 2.0596026490066226\n",
                        "    Saved CSV: tables/task3.2.1.csv\n"
                    ]
                }
            ],
            "source": [
                "# 3. Calculate Average Rating for I1 and I2\n",
                "if ratings_df is not None and 'I1' in locals():\n",
                "    # Filter for I1\n",
                "    i1_ratings = ratings_df[ratings_df['movieId'] == I1]\n",
                "    i1_mean = i1_ratings['rating'].mean()\n",
                "    i1_count = len(i1_ratings)\n",
                "    \n",
                "    # Filter for I2\n",
                "    i2_ratings = ratings_df[ratings_df['movieId'] == I2]\n",
                "    i2_mean = i2_ratings['rating'].mean()\n",
                "    i2_count = len(i2_ratings)\n",
                "    \n",
                "    print(f\"\\n--- Calculated Stats ---\")\n",
                "    print(f\"Item I1 (ID: {I1}): Mean Rating = {i1_mean:.4f}, Count = {i1_count}\")\n",
                "    print(f\"Item I2 (ID: {I2}): Mean Rating = {i2_mean:.4f}, Count = {i2_count}\")\n",
                "    \n",
                "    # Verification against loaded values\n",
                "    print(f\"\\n--- Verification ---\")\n",
                "    print(f\"Stored I1 Mean: {target_items_df.iloc[0]['mean_rating_per_movie']}\")\n",
                "    print(f\"Stored I2 Mean: {target_items_df.iloc[1]['mean_rating_per_movie']}\")\n",
                "    \n",
                "    # --- SAVE RESULT ---\n",
                "    task1_data = [\n",
                "        {'movieId': I1, 'mean_rating': i1_mean, 'count': i1_count},\n",
                "        {'movieId': I2, 'mean_rating': i2_mean, 'count': i2_count}\n",
                "    ]\n",
                "    task1_df = pd.DataFrame(task1_data)\n",
                "    save_csv(task1_df, 'task3.2.1.csv')\n",
                "    \n",
                "else:\n",
                "    print(\"Cannot calculate stats: Missing data.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00a900d7",
            "metadata": {},
            "source": [
                "## Task 2: Mean Filling and Dataset Augmentation\n",
                "Fill missing ratings for Target Items I1 and I2 with their mean value (1.0) and save the augmented dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "15438210",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Augmenting dataset with filled ratings for I1 and I2...\n",
                        "Total unique users: 96345\n",
                        "Users missing rating for I1: 95923\n",
                        "Users missing rating for I2: 95892\n",
                        "Creating dataframe for 191815 new ratings...\n",
                        "Appending to original dataset...\n",
                        "Original Shape: (1000000, 3)\n",
                        "Augmented Shape: (1191815, 3)\n",
                        "Final Count I1: 96345 (Should be 96345) \n",
                        "Final Count I2: 96345 (Should be 96345) \n",
                        "    Saved CSV: tables/ratings_filled_targets.csv\n",
                        "Save Complete.\n"
                    ]
                }
            ],
            "source": [
                "augmented_df = None # Initialize\n",
                "if ratings_df is not None and 'I1' in locals():\n",
                "    print(\"Augmenting dataset with filled ratings for I1 and I2...\")\n",
                "    \n",
                "    all_users = ratings_df['userId'].unique()\n",
                "    print(f\"Total unique users: {len(all_users)}\")\n",
                "    \n",
                "    new_rows = []\n",
                "    \n",
                "    # --- Process I1 ---\n",
                "    # Find users who rated I1\n",
                "    users_rated_i1 = set(ratings_df[ratings_df['movieId'] == I1]['userId'].unique())\n",
                "    # Find missing\n",
                "    users_missing_i1 = [u for u in all_users if u not in users_rated_i1]\n",
                "    print(f\"Users missing rating for I1: {len(users_missing_i1)}\")\n",
                "    \n",
                "    # Create rows for missing I1\n",
                "    for u in users_missing_i1:\n",
                "        new_rows.append({'userId': u, 'movieId': I1, 'rating': i1_mean})\n",
                "        \n",
                "    # --- Process I2 ---\n",
                "    # Find users who rated I2\n",
                "    users_rated_i2 = set(ratings_df[ratings_df['movieId'] == I2]['userId'].unique())\n",
                "    # Find missing\n",
                "    users_missing_i2 = [u for u in all_users if u not in users_rated_i2]\n",
                "    print(f\"Users missing rating for I2: {len(users_missing_i2)}\")\n",
                "    \n",
                "    # Create rows for missing I2\n",
                "    for u in users_missing_i2:\n",
                "        new_rows.append({'userId': u, 'movieId': I2, 'rating': i2_mean})\n",
                "    \n",
                "    print(f\"Creating dataframe for {len(new_rows)} new ratings...\")\n",
                "    new_ratings_df = pd.DataFrame(new_rows)\n",
                "    \n",
                "    # Concatenate\n",
                "    print(\"Appending to original dataset...\")\n",
                "    augmented_df = pd.concat([ratings_df, new_ratings_df], ignore_index=True)\n",
                "    \n",
                "    # Sort for tidiness (optional)\n",
                "    augmented_df.sort_values(by=['userId', 'movieId'], inplace=True)\n",
                "    \n",
                "    print(f\"Original Shape: {ratings_df.shape}\")\n",
                "    print(f\"Augmented Shape: {augmented_df.shape}\")\n",
                "    \n",
                "    # Verify counts\n",
                "    final_i1_count = len(augmented_df[augmented_df['movieId'] == I1])\n",
                "    final_i2_count = len(augmented_df[augmented_df['movieId'] == I2])\n",
                "    print(f\"Final Count I1: {final_i1_count} (Should be {len(all_users)}) \")\n",
                "    print(f\"Final Count I2: {final_i2_count} (Should be {len(all_users)}) \")\n",
                "    \n",
                "    # --- SAVE RESULT ---\n",
                "    save_csv(augmented_df, 'ratings_filled_targets.csv')\n",
                "    print(\"Save Complete.\")\n",
                "    \n",
                "else:\n",
                "    print(\"Skipping augmentation due to missing data.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "09100790",
            "metadata": {},
            "source": [
                "## Task 3: Calculate Average Ratings from Augmented Data\n",
                "Calculate the mean rating for each item in the new augmented dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "dcb5df75",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Calculating stats for augmented dataset...\n",
                        "Stats calculated.\n",
                        "   movieId  mean_rating  rating_count\n",
                        "0      1.0     3.915355          3875\n",
                        "1      2.0     3.202874          1740\n",
                        "2      3.0     3.209073          1014\n",
                        "3      5.0     3.061538           975\n",
                        "4      6.0     3.770856          1846\n",
                        "\n",
                        "Final Stats I1:\n",
                        "     movieId  mean_rating  rating_count\n",
                        "429   1556.0     1.919431         96345\n",
                        "Final Stats I2:\n",
                        "     movieId  mean_rating  rating_count\n",
                        "422   1499.0     2.059603         96345\n",
                        "Saving final item stats...\n",
                        "    Saved CSV: tables/task3.2.3_final_item_stats.csv\n",
                        "Task 3 Complete.\n"
                    ]
                }
            ],
            "source": [
                "final_stats = None # Initialize\n",
                "if augmented_df is not None:\n",
                "    print(\"Calculating stats for augmented dataset...\")\n",
                "    \n",
                "    # Calculate stats\n",
                "    final_stats = augmented_df.groupby('movieId')['rating'].agg(['mean', 'count']).reset_index()\n",
                "    final_stats.rename(columns={'mean': 'mean_rating', 'count': 'rating_count'}, inplace=True)\n",
                "    \n",
                "    print(\"Stats calculated.\")\n",
                "    print(final_stats.head())\n",
                "    \n",
                "    # Verification for I1 and I2\n",
                "    if 'I1' in locals():\n",
                "        i1_final = final_stats[final_stats['movieId'] == I1]\n",
                "        print(f\"\\nFinal Stats I1:\\n{i1_final}\")\n",
                "        \n",
                "        i2_final = final_stats[final_stats['movieId'] == I2]\n",
                "        print(f\"Final Stats I2:\\n{i2_final}\")\n",
                "        \n",
                "    # --- SAVE RESULT ---\n",
                "    print(\"Saving final item stats...\")\n",
                "    save_csv(final_stats, 'task3.2.3_final_item_stats.csv')\n",
                "    print(\"Task 3 Complete.\")\n",
                "    \n",
                "else:\n",
                "    print(\"Augmented dataframe not available.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e21db64",
            "metadata": {},
            "source": [
                "## Task 4: Calculate Diffrence between Ratings and Mean Rating\n",
                "Load the augmented ratings and final item stats from CSVs, then calculate the difference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "ee299842",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading files for Task 4...\n",
                        " Found requested table at: ..\\results\\tables\\ratings_filled_targets.csv\n",
                        " Found requested table at: ..\\results\\tables\\task3.2.3_final_item_stats.csv\n",
                        "Loaded Augmented Ratings. Shape: (1191815, 3)\n",
                        "Loaded Item Stats. Shape: (1000, 3)\n",
                        "Calculating rating differences (Centering)...\n",
                        "Difference calculated.\n",
                        "   userId  movieId  rating  mean_rating  rating_diff\n",
                        "0       1     32.0     4.0     3.881264     0.118736\n",
                        "1       1    337.0     4.0     3.755682     0.244318\n",
                        "2       1   1193.0     4.0     4.191716    -0.191716\n",
                        "3       1   1261.0     4.0     3.848649     0.151351\n",
                        "4       1   1370.0     3.0     3.454844    -0.454844\n",
                        "\n",
                        "Average Global Difference (should be ~0): -0.000000\n",
                        "Saving centered ratings...\n",
                        "    Saved CSV: tables/task3.2.4_centered_ratings.csv\n",
                        "Task 4 Complete.\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading files for Task 4...\")\n",
                "\n",
                "task4_ratings = load_data(table_name='ratings_filled_targets.csv')\n",
                "task4_stats = load_data(table_name='task3.2.3_final_item_stats.csv')\n",
                "\n",
                "if task4_ratings is not None and task4_stats is not None:\n",
                "    print(f\"Loaded Augmented Ratings. Shape: {task4_ratings.shape}\")\n",
                "    print(f\"Loaded Item Stats. Shape: {task4_stats.shape}\")\n",
                "    \n",
                "    print(\"Calculating rating differences (Centering)...\")\n",
                "    \n",
                "    # Merge\n",
                "    merged_df = task4_ratings.merge(task4_stats[['movieId', 'mean_rating']], on='movieId', how='left')\n",
                "    \n",
                "    # Calculate difference\n",
                "    merged_df['rating_diff'] = merged_df['rating'] - merged_df['mean_rating']\n",
                "    \n",
                "    print(\"Difference calculated.\")\n",
                "    print(merged_df[['userId', 'movieId', 'rating', 'mean_rating', 'rating_diff']].head())\n",
                "    \n",
                "    # Verify centering (Mean difference should be close to 0)\n",
                "    avg_diff = merged_df['rating_diff'].mean()\n",
                "    print(f\"\\nAverage Global Difference (should be ~0): {avg_diff:.6f}\")\n",
                "    \n",
                "    # --- SAVE RESULT ---\n",
                "    print(\"Saving centered ratings...\")\n",
                "    save_csv(merged_df, 'task3.2.4_centered_ratings.csv')\n",
                "    print(\"Task 4 Complete.\")\n",
                "    \n",
                "else:\n",
                "    print(\"Cannot proceed with Task 4 due to missing files.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d8011c87",
            "metadata": {},
            "source": [
                "## Task 5: Calculate Partial Covariance (Targets vs All)\n",
                "Compute the covariance where we calculate Cov(Target, Item_j) for all Items j.\n",
                "We use the helper function from utils to do this efficiently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "120c063f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " Found requested table at: ..\\results\\tables\\lowest_two_rateditems.csv\n",
                        " Target Items IDs found: 1556.0, 1499.0\n",
                        " Found requested table at: ..\\results\\tables\\task3.2.4_centered_ratings.csv\n",
                        "Calculating partial covariance matrix (2 x All Items)...\n",
                        "Starting MANUAL covariance calculation...\n",
                        "Total N (Users): 96345\n",
                        "Total Items: 1000\n",
                        "Users who rated targets: 96345\n",
                        "Building efficient lookup dictionary...\n",
                        "Lookup built. Calculating sums...\n",
                        "Processing Target Item: 1556.0...\n",
                        "Processing Target Item: 1499.0...\n",
                        "Formatting results...\n",
                        "Calculation Complete.\n",
                        "Result Shape: (2, 1000)\n",
                        "Sample (first 5 cols):\n",
                        "             1.0      2.0       3.0       5.0       6.0\n",
                        "1556.0  0.000013  0.00007 -0.000016  0.000042  0.000027\n",
                        "1499.0  0.000058  0.00010  0.000024  0.000046  0.000014\n",
                        "    Saved CSV: tables/3.2.5_target_only_covariances.csv\n",
                        "Task 5 Complete.\n"
                    ]
                }
            ],
            "source": [
                "target_items_df = load_data(table_name='lowest_two_rateditems.csv')\n",
                "\n",
                "if target_items_df is not None:\n",
                "    I1 = target_items_df.iloc[0]['movieId']\n",
                "    I2 = target_items_df.iloc[1]['movieId']\n",
                "    print(f\" Target Items IDs found: {I1}, {I2}\")\n",
                "\n",
                "    # 2. Load Centered Ratings\n",
                "    # Note: load_data loads full CSV. \n",
                "    df = load_data(table_name='task3.2.4_centered_ratings.csv')\n",
                "    \n",
                "    if df is not None:\n",
                "        # 3. Calculate Target Covariances using Utils\n",
                "        print(\"Calculating partial covariance matrix (2 x All Items)...\")\n",
                "        \n",
                "        targets = [I1, I2]\n",
                "        partial_cov_df = calculate_target_covariance(df, targets)\n",
                "        \n",
                "        print(\"Calculation Complete.\")\n",
                "        print(f\"Result Shape: {partial_cov_df.shape}\")\n",
                "        print(\"Sample (first 5 cols):\")\n",
                "        print(partial_cov_df.iloc[:, :5])\n",
                "        \n",
                "        # Task 5 Save\n",
                "        save_csv(partial_cov_df, '3.2.5_target_only_covariances.csv')\n",
                "        print(\"Task 5 Complete.\")\n",
                "    else:\n",
                "        print(\"Centered ratings data missing.\")\n",
                "else:\n",
                "    print(\"Target items data missing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f557451f",
            "metadata": {},
            "source": [
                "## Task 6: Reference Full Covariance Matrix Calculation\n",
                "Calculates the FULL N x N covariance matrix for all items using sparse algebra and saves it as a compressed .npz file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "d2e03664",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading centered ratings for Full Matrix calculation...\n",
                        " Found requested table at: ..\\results\\tables\\task3.2.4_centered_ratings.csv\n",
                        "Calculating Full Sparse Matrix (N x N)...\n",
                        "Preparing for Full Sparse Covariance Calculation...\n",
                        "Dimensions: 96345 Users x 1000 Items\n",
                        "Constructing User-Item Sparse Matrix...\n",
                        "Computing X.T @ X ... (This may take a moment)\n",
                        "Dividing by N-1...\n",
                        "Matrix Shape: (1000, 1000)\n",
                        "Saving Sparse Matrix to d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\\tables\\3.2.6_full_covariance.npz...\n",
                        "    Saved CSV: tables/3.2.6_full_covariance_ids.csv\n",
                        "Writing PARTIAL (60%) Full Matrix to CSV: d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\results\\tables\\3.2.6_full_covariance_partial.csv\n",
                        "Writing in chunks...\n",
                        "Total items: 1000. Writing first 600 items.\n",
                        "  Written rows 0 to 600 / 600 (0.6s)\n",
                        "CSV Write Complete. Time: 0.6s\n",
                        "Task 6 Complete.\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading centered ratings for Full Matrix calculation...\")\n",
                "df_full = load_data(table_name='task3.2.4_centered_ratings.csv')\n",
                "\n",
                "if df_full is not None:\n",
                "    print(\"Calculating Full Sparse Matrix (N x N)...\")\n",
                "    sparse_cov, movie_ids = calculate_full_covariance_sparse(df_full)\n",
                "    \n",
                "    print(f\"Matrix Shape: {sparse_cov.shape}\")\n",
                "    \n",
                "    # Save Matrix (Sparse) - using global results_root from utils for robust path\n",
                "    out_path = os.path.join(results_root, 'tables', '3.2.6_full_covariance.npz')\n",
                "    print(f\"Saving Sparse Matrix to {out_path}...\")\n",
                "    sparse.save_npz(out_path, sparse_cov)\n",
                "    \n",
                "    # Save ID Mapping\n",
                "    pd.DataFrame(movie_ids, columns=['movieId'])\n",
                "    save_csv(pd.DataFrame(movie_ids, columns=['movieId']), '3.2.6_full_covariance_ids.csv')\n",
                "    \n",
                "    # --- MASSIVE CSV WRITE WITH CHUNKING (PARTIAL 60%) ---\n",
                "    # Since save_csv simplifies overwriting, we handle chunking manually but use path from utils if possible\n",
                "    # or just use os.path.join with results_root\n",
                "    full_csv_path = os.path.join(results_root, 'tables', '3.2.6_full_covariance_partial.csv')\n",
                "    print(f\"Writing PARTIAL (60%) Full Matrix to CSV: {full_csv_path}\")\n",
                "    print(\"Writing in chunks...\")\n",
                "    \n",
                "    chunk_size = 1000\n",
                "    num_items = sparse_cov.shape[0]\n",
                "    limit_items = int(num_items * 0.60) # 60% Limit\n",
                "    \n",
                "    print(f\"Total items: {num_items}. Writing first {limit_items} items.\")\n",
                "    \n",
                "    start_time = time.time()\n",
                "    \n",
                "    for i in range(0, limit_items, chunk_size):\n",
                "        end_i = min(i + chunk_size, limit_items)\n",
                "        \n",
                "        # Extract chunk and make dense\n",
                "        chunk_dense = sparse_cov[i:end_i].toarray()\n",
                "        \n",
                "        # Create DF\n",
                "        chunk_index = movie_ids[i:end_i]\n",
                "        chunk_df = pd.DataFrame(chunk_dense, index=chunk_index, columns=movie_ids)\n",
                "        \n",
                "        # Write mode: 'w' for first chunk, 'a' for others\n",
                "        if i == 0:\n",
                "            chunk_df.to_csv(full_csv_path, mode='w', header=True)\n",
                "        else:\n",
                "            chunk_df.to_csv(full_csv_path, mode='a', header=False)\n",
                "            \n",
                "        # Progress log\n",
                "        if i % 3000 == 0:\n",
                "            elapsed = time.time() - start_time\n",
                "            print(f\"  Written rows {i} to {end_i} / {limit_items} ({elapsed:.1f}s)\")\n",
                "    \n",
                "    print(f\"CSV Write Complete. Time: {time.time() - start_time:.1f}s\")\n",
                "    print(\"Task 6 Complete.\")\n",
                "    \n",
                "else:\n",
                "    print(\"Input file not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "252d16fd",
            "metadata": {},
            "source": [
                "## Task 7: Determine Top 5 and Top 10 Peers\n",
                "Identify the items with the highest covariance with each target item (I1 and I2) using the generated full matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "e8f43929",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Full Sparse Matrix...\n",
                        "Loading IDs...\n",
                        "Loading Targets...\n",
                        "Performing Eigen-decomposition...\n",
                        "Number of components to explain 75% variance: 504\n",
                        "Correlation Variance at k=504: 0.7503\n",
                        "\n",
                        "Processing Peers for Target: 1556.0 (Latent Space)\n",
                        "Top 5 Peers for 1556.0:\n",
                        "[2421.0, 1722.0, 256.0, 1831.0, 1370.0]\n",
                        "Top 10 Peers for 1556.0:\n",
                        "[2421.0, 1722.0, 256.0, 1831.0, 1370.0, 2431.0, 2407.0, 2134.0, 1591.0, 79.0]\n",
                        "\n",
                        "Processing Peers for Target: 1499.0 (Latent Space)\n",
                        "Top 5 Peers for 1499.0:\n",
                        "[2722.0, 1347.0, 1591.0, 3826.0, 1438.0]\n",
                        "Top 10 Peers for 1499.0:\n",
                        "[2722.0, 1347.0, 1591.0, 3826.0, 1438.0, 2949.0, 1042.0, 2067.0, 799.0, 379.0]\n",
                        "\n",
                        "Saving Top Peers to ..\\results\\tables\\3.2.7_top_peers.csv...\n",
                        "Task 7 Complete.\n"
                    ]
                }
            ],
            "source": [
                "# Task 7 - Identify Item Peers Using PCA Latent Space\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "from scipy import sparse\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "\n",
                "matrix_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance.npz')\n",
                "id_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance_ids.csv')\n",
                "target_items_path = os.path.join('..', 'results', 'tables', 'lowest_two_rateditems.csv')\n",
                "\n",
                "if os.path.exists(matrix_path) and os.path.exists(id_path) and os.path.exists(target_items_path):\n",
                "    print(\"Loading Full Sparse Matrix...\")\n",
                "    cov_matrix_sparse = sparse.load_npz(matrix_path)\n",
                "    cov_matrix = cov_matrix_sparse.toarray() # PCA needs dense matrix usually\n",
                "    \n",
                "    print(\"Loading IDs...\")\n",
                "    movie_ids_df = pd.read_csv(id_path)\n",
                "    all_movie_ids = movie_ids_df['movieId'].tolist()\n",
                "    \n",
                "    print(\"Loading Targets...\")\n",
                "    targets_df = pd.read_csv(target_items_path)\n",
                "    targets = [targets_df.iloc[0]['movieId'], targets_df.iloc[1]['movieId']]\n",
                "    \n",
                "    # --- PCA Eigen-decomposition ---\n",
                "    print(\"Performing Eigen-decomposition...\")\n",
                "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
                "    \n",
                "    # Sort descending\n",
                "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
                "    eigenvalues = eigenvalues[sorted_indices]\n",
                "    eigenvectors = eigenvectors[:, sorted_indices]\n",
                "    \n",
                "    # Determine k for 75% Variance\n",
                "    total_variance = np.sum(eigenvalues)\n",
                "    explained_variance_ratio = eigenvalues / total_variance\n",
                "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
                "    \n",
                "    k_75 = np.argmax(cumulative_variance >= 0.75) + 1\n",
                "    print(f\"Number of components to explain 75% variance: {k_75}\")\n",
                "    print(f\"Correlation Variance at k={k_75}: {cumulative_variance[k_75-1]:.4f}\")\n",
                "    \n",
                "    # Construct Latent Item Features\n",
                "    latent_item_features = eigenvectors[:, :k_75]\n",
                "    latent_item_df = pd.DataFrame(latent_item_features, index=all_movie_ids)\n",
                "    \n",
                "    results = []\n",
                "    \n",
                "    for tid in targets:\n",
                "        print(f\"\\nProcessing Peers for Target: {tid} (Latent Space)\")\n",
                "        if tid in latent_item_df.index:\n",
                "            target_vec = latent_item_df.loc[tid].values.reshape(1, -1)\n",
                "            \n",
                "            # Compute Cosine Similarity\n",
                "            sim_scores = cosine_similarity(target_vec, latent_item_df.values).flatten()\n",
                "            sim_series = pd.Series(sim_scores, index=latent_item_df.index)\n",
                "            \n",
                "            # Exclude self\n",
                "            sim_series = sim_series.drop(tid)\n",
                "            \n",
                "            # Sort & Top 10\n",
                "            top_10 = sim_series.sort_values(ascending=False).head(10)\n",
                "            \n",
                "            print(f\"Top 5 Peers for {tid}:\")\n",
                "            print(top_10.head(5).index.tolist())\n",
                "\n",
                "            print(f\"Top 10 Peers for {tid}:\")\n",
                "            print(top_10.index.tolist())\n",
                "            \n",
                "            # Store results\n",
                "            rank = 1\n",
                "            for peer_id, sim in top_10.items():\n",
                "                results.append({\n",
                "                    'Target_Item': tid,\n",
                "                    'Rank': rank,\n",
                "                    'Peer_Item': peer_id,\n",
                "                    'Latent_Similarity': sim # Storing similarity weight\n",
                "                })\n",
                "                rank += 1\n",
                "        else:\n",
                "            print(f\"Target {tid} not found in matrix indices.\")\n",
                "            \n",
                "    # Save Results\n",
                "    results_df = pd.DataFrame(results)\n",
                "    out_path = os.path.join('..', 'results', 'tables', '3.2.7_top_peers.csv')\n",
                "    \n",
                "    print(f\"\\nSaving Top Peers to {out_path}...\")\n",
                "    results_df.to_csv(out_path, index=False)\n",
                "    print(\"Task 7 Complete.\")\n",
                "    \n",
                "else:\n",
                "    print(\"Missing required files for Task 7. Run Task 6 first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39adba8a",
            "metadata": {},
            "source": [
                "## Task 8: Determine Reduced Dimensional Space (Top 5 Peers)\n",
                "Create a reduced dimensional space for each user by selecting their centered ratings for the Top 5 peer items identified in Task 7.\n",
                "Missing values are filled with 0 (since ratings are centered)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "edb63cae",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Top Peers and Centered Ratings...\n",
                        "Total Users: 96345\n",
                        "\n",
                        "--- Processing Target 1556.0 ---\n",
                        "Top 5 Peers: [2421.0, 1722.0, 256.0, 1831.0, 1370.0]\n",
                        "Creating Reduced Space Matrix (User Vectors)...\n",
                        "Reduced Matrix Shape: (96345, 5)\n",
                        "Saving User Vectors to 3.2.8_reduced_space_target_1556.csv...\n",
                        "    Saved CSV: tables/3.2.8_reduced_space_target_1556.csv\n",
                        "\n",
                        "--- Processing Target 1499.0 ---\n",
                        "Top 5 Peers: [2722.0, 1347.0, 1591.0, 3826.0, 1438.0]\n",
                        "Creating Reduced Space Matrix (User Vectors)...\n",
                        "Reduced Matrix Shape: (96345, 5)\n",
                        "Saving User Vectors to 3.2.8_reduced_space_target_1499.csv...\n",
                        "    Saved CSV: tables/3.2.8_reduced_space_target_1499.csv\n",
                        "\n",
                        "Task 8 Complete.\n"
                    ]
                }
            ],
            "source": [
                "# Task 8: Reduced Dimensional Space (Save Weights & User Vectors)\n",
                "import pandas as pd\n",
                "import os\n",
                "import utils\n",
                "\n",
                "peers_path = os.path.join('..', 'results', 'tables', '3.2.7_top_peers.csv')\n",
                "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
                "\n",
                "if os.path.exists(peers_path) and os.path.exists(centered_path):\n",
                "    print(\"Loading Top Peers and Centered Ratings...\")\n",
                "    peers_df = pd.read_csv(peers_path)\n",
                "    centered_df = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
                "    \n",
                "    all_users = centered_df['userId'].unique()\n",
                "    print(f\"Total Users: {len(all_users)}\")\n",
                "    \n",
                "    targets = peers_df['Target_Item'].unique()\n",
                "    \n",
                "    for target in targets:\n",
                "        print(f\"\\n--- Processing Target {target} ---\")\n",
                "        \n",
                "        # 1. Identify Top 5 Peers\n",
                "        target_peers = peers_df[peers_df['Target_Item'] == target].sort_values('Rank')\n",
                "        top_5_peers = target_peers.head(5)['Peer_Item'].tolist()\n",
                "        print(f\"Top 5 Peers: {top_5_peers}\")\n",
                "        \n",
                "        # 2. Filter Centered Ratings\n",
                "        subset_df = centered_df[centered_df['movieId'].isin(top_5_peers)]\n",
                "        \n",
                "        # 3. Create User Vectors (For verification/pipeline consistency)\n",
                "        # We pivot to get the user's ratings for the top 5 peers\n",
                "        print(\"Creating Reduced Space Matrix (User Vectors)...\")\n",
                "        reduced_matrix = subset_df.pivot(index='userId', columns='movieId', values='rating_diff')\n",
                "        reduced_matrix = reduced_matrix.fillna(0.0)\n",
                "        reduced_matrix = reduced_matrix.reindex(all_users, fill_value=0.0)\n",
                "        \n",
                "        print(f\"Reduced Matrix Shape: {reduced_matrix.shape}\")\n",
                "        \n",
                "        # 4. Save User Vectors\n",
                "        out_filename = f'3.2.8_reduced_space_target_{int(target)}.csv'\n",
                "        print(f\"Saving User Vectors to {out_filename}...\")\n",
                "        utils.save_csv(reduced_matrix.reset_index(), out_filename)\n",
                "    \n",
                "    print(\"\\nTask 8 Complete.\")\n",
                "else:\n",
                "    print(\"Error: Required files not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "afb784d5",
            "metadata": {},
            "source": [
                "## Task 9: Rating Prediction using Linear Regression (Top 5 Peers)\n",
                "Use the reduced dimensional space (Top 5 Peers) to train a Linear Regression model for each target item.\n",
                "Predict the centered ratings for users who haven't rated the target item, then add the mean rating to get the final prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "5546c106",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading data for Weighted Prediction...\n",
                        "\n",
                        "--- Predicting for Target 1556.0 ---\n",
                        "Using Peers: [2421.0, 1722.0, 256.0, 1831.0, 1370.0]\n",
                        "Saving predictions to 3.2.9_predictions_target_1556.csv...\n",
                        "    Saved CSV: tables/3.2.9_predictions_target_1556.csv\n",
                        "\n",
                        "--- Predicting for Target 1499.0 ---\n",
                        "Using Peers: [2722.0, 1347.0, 1591.0, 3826.0, 1438.0]\n",
                        "Saving predictions to 3.2.9_predictions_target_1499.csv...\n",
                        "    Saved CSV: tables/3.2.9_predictions_target_1499.csv\n",
                        "\n",
                        "Task 9 Complete.\n"
                    ]
                }
            ],
            "source": [
                "# Task 9: Prediction using Weighted Average (Top 5 Peers)\n",
                "import pandas as pd\n",
                "import os\n",
                "import utils\n",
                "\n",
                "stats_path = os.path.join('..', 'results', 'tables', 'task3.2.3_final_item_stats.csv')\n",
                "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
                "peers_path = os.path.join('..', 'results', 'tables', '3.2.7_top_peers.csv') # Contains Weights\n",
                "\n",
                "if os.path.exists(stats_path) and os.path.exists(centered_path) and os.path.exists(peers_path):\n",
                "    print(\"Loading data for Weighted Prediction...\")\n",
                "    item_stats = pd.read_csv(stats_path)\n",
                "    centered_df = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
                "    peers_df = pd.read_csv(peers_path)\n",
                "    \n",
                "    all_users = centered_df['userId'].unique()\n",
                "    targets = peers_df['Target_Item'].unique()\n",
                "    \n",
                "    for target in targets:\n",
                "        print(f\"\\n--- Predicting for Target {target} ---\")\n",
                "        \n",
                "        # 1. Get Top 5 Peers and Weights\n",
                "        target_peers = peers_df[peers_df['Target_Item'] == target].sort_values('Rank').head(5)\n",
                "        peer_ids = target_peers['Peer_Item'].tolist()\n",
                "        weights = dict(zip(target_peers['Peer_Item'], target_peers['Latent_Similarity']))\n",
                "        \n",
                "        print(f\"Using Peers: {peer_ids}\")\n",
                "        \n",
                "        # 2. Get User Ratings for these Peers (Efficiently)\n",
                "        subset_df = centered_df[centered_df['movieId'].isin(peer_ids)].copy()\n",
                "        \n",
                "        # 3. Pivot to User x Peer\n",
                "        user_peer_matrix = subset_df.pivot(index='userId', columns='movieId', values='rating_diff').reindex(all_users).fillna(0.0)\n",
                "        \n",
                "        # 4. Calculate Prediction\n",
                "        # Formula: Pred = Mean + (Sum(W * (R - Mu)) / Sum(|W|))\n",
                "        # Since R is already 'rating_diff' (R - Mu), we use Sum(W * rating_diff) / Sum(|W|)\n",
                "        \n",
                "        predictions = []\n",
                "        \n",
                "        # Vectorized implementation for speed\n",
                "        numerator = pd.Series(0.0, index=user_peer_matrix.index)\n",
                "        denominator = 0.0\n",
                "        \n",
                "        for pid in peer_ids:\n",
                "            w = weights[pid]\n",
                "            if pid in user_peer_matrix.columns:\n",
                "                numerator += user_peer_matrix[pid] * w\n",
                "                denominator += abs(w)\n",
                "                \n",
                "        if denominator == 0:\n",
                "            centered_pred = 0.0\n",
                "        else:\n",
                "            centered_pred = numerator / denominator\n",
                "            \n",
                "        # Add Target Mean\n",
                "        target_mean = item_stats[item_stats['movieId'] == target]['mean_rating'].values[0]\n",
                "        final_pred = centered_pred + target_mean\n",
                "        \n",
                "        # Create Result DF\n",
                "        pred_df = pd.DataFrame({\n",
                "            'userId': user_peer_matrix.index,\n",
                "            'movieId': target,\n",
                "            'predicted_rating_centered': centered_pred,\n",
                "            'predicted_rating_final': final_pred\n",
                "        })\n",
                "        \n",
                "        out_name = f'3.2.9_predictions_target_{int(target)}.csv'\n",
                "        print(f\"Saving predictions to {out_name}...\")\n",
                "        utils.save_csv(pred_df, out_name)\n",
                "        \n",
                "    print(\"\\nTask 9 Complete.\")\n",
                "else:\n",
                "    print(\"Required files missing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "38eaa39a",
            "metadata": {},
            "source": [
                "## Task 10: Determine Reduced Dimensional Space (Top 10 Peers)\n",
                "Create a reduced dimensional space for each user by selecting their centered ratings for the Top 10 peer items identified in Task 7.\n",
                "Missing values are filled with 0 (since ratings are centered)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "090fcbc6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Top Peers and Centered Ratings (Task 10)...\n",
                        "\n",
                        "--- Processing Target 1556.0 (Top 10) ---\n",
                        "Top 10 Peers: [2421.0, 1722.0, 256.0, 1831.0, 1370.0, 2431.0, 2407.0, 2134.0, 1591.0, 79.0]\n",
                        "Creating Reduced Space Matrix...\n",
                        "Reduced Matrix Shape: (96345, 10)\n",
                        "Saving to 3.2.10_reduced_space_target_1556_top10.csv...\n",
                        "    Saved CSV: tables/3.2.10_reduced_space_target_1556_top10.csv\n",
                        "\n",
                        "--- Processing Target 1499.0 (Top 10) ---\n",
                        "Top 10 Peers: [2722.0, 1347.0, 1591.0, 3826.0, 1438.0, 2949.0, 1042.0, 2067.0, 799.0, 379.0]\n",
                        "Creating Reduced Space Matrix...\n",
                        "Reduced Matrix Shape: (96345, 10)\n",
                        "Saving to 3.2.10_reduced_space_target_1499_top10.csv...\n",
                        "    Saved CSV: tables/3.2.10_reduced_space_target_1499_top10.csv\n",
                        "\n",
                        "Task 10 Complete.\n"
                    ]
                }
            ],
            "source": [
                "# Task 10: Reduced Dimensional Space (Top 10 Peers)\n",
                "import pandas as pd\n",
                "import os\n",
                "import utils\n",
                "\n",
                "peers_path = os.path.join('..', 'results', 'tables', '3.2.7_top_peers.csv')\n",
                "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
                "\n",
                "if os.path.exists(peers_path) and os.path.exists(centered_path):\n",
                "    print(\"Loading Top Peers and Centered Ratings (Task 10)...\")\n",
                "    peers_df = pd.read_csv(peers_path)\n",
                "    centered_df = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
                "    all_users = centered_df['userId'].unique()\n",
                "    targets = peers_df['Target_Item'].unique()\n",
                "    \n",
                "    for target in targets:\n",
                "        print(f\"\\n--- Processing Target {target} (Top 10) ---\")\n",
                "        \n",
                "        # Top 10 Peers\n",
                "        target_peers = peers_df[peers_df['Target_Item'] == target].sort_values('Rank')\n",
                "        top_10_peers = target_peers.head(10)['Peer_Item'].tolist()\n",
                "        print(f\"Top 10 Peers: {top_10_peers}\")\n",
                "        \n",
                "        subset_df = centered_df[centered_df['movieId'].isin(top_10_peers)]\n",
                "        \n",
                "        print(\"Creating Reduced Space Matrix...\")\n",
                "        reduced_matrix = subset_df.pivot(index='userId', columns='movieId', values='rating_diff')\n",
                "        reduced_matrix = reduced_matrix.fillna(0.0)\n",
                "        reduced_matrix = reduced_matrix.reindex(all_users, fill_value=0.0)\n",
                "        \n",
                "        print(f\"Reduced Matrix Shape: {reduced_matrix.shape}\")\n",
                "        \n",
                "        out_filename = f'3.2.10_reduced_space_target_{int(target)}_top10.csv'\n",
                "        print(f\"Saving to {out_filename}...\")\n",
                "        utils.save_csv(reduced_matrix.reset_index(), out_filename)\n",
                "        \n",
                "    print(\"\\nTask 10 Complete.\")\n",
                "else:\n",
                "    print(\"Required files missing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b0007a02",
            "metadata": {},
            "source": [
                "# Task 11 "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "42e5b9e1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading data for Weighted Prediction (Top 10)...\n",
                        "\n",
                        "--- Modelling for Target 1556.0 (Top 10) ---\n",
                        "Saving predictions to 3.2.11_predictions_target_1556_top10.csv...\n",
                        "    Saved CSV: tables/3.2.11_predictions_target_1556_top10.csv\n",
                        "\n",
                        "--- Modelling for Target 1499.0 (Top 10) ---\n",
                        "Saving predictions to 3.2.11_predictions_target_1499_top10.csv...\n",
                        "    Saved CSV: tables/3.2.11_predictions_target_1499_top10.csv\n",
                        "\n",
                        "Task 11 Complete.\n"
                    ]
                }
            ],
            "source": [
                "# Task 11: Prediction (Top 10 Weighted Average)\n",
                "import pandas as pd\n",
                "import os\n",
                "import utils\n",
                "\n",
                "stats_path = os.path.join('..', 'results', 'tables', 'task3.2.3_final_item_stats.csv')\n",
                "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
                "peers_path = os.path.join('..', 'results', 'tables', '3.2.7_top_peers.csv')\n",
                "\n",
                "if os.path.exists(stats_path) and os.path.exists(centered_path) and os.path.exists(peers_path):\n",
                "    print(\"Loading data for Weighted Prediction (Top 10)...\")\n",
                "    item_stats = pd.read_csv(stats_path)\n",
                "    centered_df = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
                "    peers_df = pd.read_csv(peers_path)\n",
                "    \n",
                "    all_users = centered_df['userId'].unique()\n",
                "    targets = peers_df['Target_Item'].unique()\n",
                "    \n",
                "    for target in targets:\n",
                "        print(f\"\\n--- Modelling for Target {target} (Top 10) ---\")\n",
                "        \n",
                "        target_peers = peers_df[peers_df['Target_Item'] == target].sort_values('Rank').head(10)\n",
                "        peer_ids = target_peers['Peer_Item'].tolist()\n",
                "        weights = dict(zip(target_peers['Peer_Item'], target_peers['Latent_Similarity']))\n",
                "        \n",
                "        subset_df = centered_df[centered_df['movieId'].isin(peer_ids)].copy()\n",
                "        user_peer_matrix = subset_df.pivot(index='userId', columns='movieId', values='rating_diff').reindex(all_users).fillna(0.0)\n",
                "        \n",
                "        numerator = pd.Series(0.0, index=user_peer_matrix.index)\n",
                "        denominator = 0.0\n",
                "        \n",
                "        for pid in peer_ids:\n",
                "            w = weights[pid]\n",
                "            if pid in user_peer_matrix.columns:\n",
                "                numerator += user_peer_matrix[pid] * w\n",
                "                denominator += abs(w)\n",
                "                \n",
                "        if denominator == 0:\n",
                "            centered_pred = 0.0\n",
                "        else:\n",
                "            centered_pred = numerator / denominator\n",
                "            \n",
                "        target_mean = item_stats[item_stats['movieId'] == target]['mean_rating'].values[0]\n",
                "        final_pred = centered_pred + target_mean\n",
                "        \n",
                "        pred_df = pd.DataFrame({\n",
                "            'userId': user_peer_matrix.index,\n",
                "            'movieId': target,\n",
                "            'predicted_rating_centered': centered_pred,\n",
                "            'predicted_rating_final': final_pred\n",
                "        })\n",
                "        \n",
                "        out_name = f'3.2.11_predictions_target_{int(target)}_top10.csv'\n",
                "        print(f\"Saving predictions to {out_name}...\")\n",
                "        utils.save_csv(pred_df, out_name)\n",
                "        \n",
                "    print(\"\\nTask 11 Complete.\")\n",
                "else:\n",
                "    print(\"Required files missing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "893883e7",
            "metadata": {},
            "source": [
                "# Task 12"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "e09e627f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Comparison for Target 1499 ---\n",
                        "Total Predictions Compared: 96345\n",
                        "Mean Difference (Top10 - Top5): 0.0000\n",
                        "Mean Absolute Difference: 0.0036\n",
                        "Max Difference: 0.4813\n",
                        "\n",
                        "Sample Comparison (First 5 users):\n",
                        " userId  pred_top5  pred_top10  diff  abs_diff\n",
                        "      1   2.059603    2.059603   0.0       0.0\n",
                        "      2   2.059603    2.059603   0.0       0.0\n",
                        "      3   2.059603    2.059603   0.0       0.0\n",
                        "      4   2.059603    2.059603   0.0       0.0\n",
                        "      5   2.059603    2.059603   0.0       0.0\n",
                        "\n",
                        "Conclusion: The predictions are very similar. Adding 5 more peers didn't drastically change the outcome.\n",
                        "\n",
                        "--- Comparison for Target 1556 ---\n",
                        "Total Predictions Compared: 96345\n",
                        "Mean Difference (Top10 - Top5): 0.0000\n",
                        "Mean Absolute Difference: 0.0046\n",
                        "Max Difference: 0.3692\n",
                        "\n",
                        "Sample Comparison (First 5 users):\n",
                        " userId  pred_top5  pred_top10     diff  abs_diff\n",
                        "      1   1.833879    1.872520 0.038641  0.038641\n",
                        "      2   1.919431    1.919431 0.000000  0.000000\n",
                        "      3   1.919431    1.919431 0.000000  0.000000\n",
                        "      4   1.919431    1.919431 0.000000  0.000000\n",
                        "      5   1.919431    1.919431 0.000000  0.000000\n",
                        "\n",
                        "Conclusion: The predictions are very similar. Adding 5 more peers didn't drastically change the outcome.\n"
                    ]
                }
            ],
            "source": [
                "# Task 12: Compare Results\n",
                "results_dir = os.path.join('..', 'results', 'tables')\n",
                "targets = [1499, 1556]\n",
                "\n",
                "for target_id in targets:\n",
                "    file_top5 = os.path.join(results_dir, f'3.2.9_predictions_target_{target_id}.csv')\n",
                "    file_top10 = os.path.join(results_dir, f'3.2.11_predictions_target_{target_id}_top10.csv')\n",
                "    \n",
                "    if os.path.exists(file_top5) and os.path.exists(file_top10):\n",
                "        print(f\"\\n--- Comparison for Target {target_id} ---\")\n",
                "        \n",
                "        df5 = pd.read_csv(file_top5)\n",
                "        df10 = pd.read_csv(file_top10)\n",
                "        \n",
                "        # Rename columns for clear merging\n",
                "        df5 = df5[['userId', 'predicted_rating_final']].rename(columns={'predicted_rating_final': 'pred_top5'})\n",
                "        df10 = df10[['userId', 'predicted_rating_final']].rename(columns={'predicted_rating_final': 'pred_top10'})\n",
                "        \n",
                "        # Merge on userId to align predictions\n",
                "        merged = df5.merge(df10, on='userId', how='inner')\n",
                "        \n",
                "        # Calculate Difference\n",
                "        merged['diff'] = merged['pred_top10'] - merged['pred_top5']\n",
                "        merged['abs_diff'] = merged['diff'].abs()\n",
                "        \n",
                "        # Statistics\n",
                "        print(f\"Total Predictions Compared: {len(merged)}\")\n",
                "        print(f\"Mean Difference (Top10 - Top5): {merged['diff'].mean():.4f}\")\n",
                "        print(f\"Mean Absolute Difference: {merged['abs_diff'].mean():.4f}\")\n",
                "        print(f\"Max Difference: {merged['abs_diff'].max():.4f}\")\n",
                "        \n",
                "        print(\"\\nSample Comparison (First 5 users):\")\n",
                "        print(merged.head(5).to_string(index=False))\n",
                "        \n",
                "        # Explanation logic (dynamic print)\n",
                "        if merged['abs_diff'].mean() < 0.1:\n",
                "            print(\"\\nConclusion: The predictions are very similar. Adding 5 more peers didn't drastically change the outcome.\")\n",
                "        else:\n",
                "            print(\"\\nConclusion: There is a noticeable difference. The additional 5 peers influenced the regression model.\")\n",
                "            \n",
                "    else:\n",
                "        print(f\"Missing files for target {target_id}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "63869512",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Benchmarking PCA Mean Filling (Part 1) ---\n",
                        "Pivoting ratings_df for benchmark...\n",
                        "Results: Time Decomp 3.1502s, RMSE 0.8281\n",
                        "Benchmark saved to d:\\University\\semester 9\\IRS\\AIE425_FinalProject_Group17\\SECTION1_DimensionalityReduction\\code\\../results/tables\\pca_benchmarks.csv\n"
                    ]
                }
            ],
            "source": [
                "# --- DYNAMIC BENCHMARKING: PCA Mean Filling (Part 1) ---\\n\n",
                "method_name = \"PCA Mean Filling (Part 1)\"\n",
                "import time\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "from scipy.linalg import eigh\n",
                "# Ensure utils is loaded if locals missing (fallback)\n",
                "if 'load_data' not in locals():\n",
                "    try:\n",
                "        from utils import load_data, calculate_mle_covariance\n",
                "    except ImportError:\n",
                "        print(\"Warning: utils module not found. Benchmarking might fail.\")\n",
                "\n",
                "print(f\"\\n--- Benchmarking {method_name} ---\")\n",
                "\n",
                "# 1. Setup Results Path\n",
                "results_table_dir = os.path.join(os.getcwd(), '../results/tables')\n",
                "if not os.path.exists(results_table_dir):\n",
                "   os.makedirs(results_table_dir)\n",
                "\n",
                "# 2. Data Preparation (Robust Loading)\n",
                "R_bench = None\n",
                "\n",
                "# Attempt 1: Check for existing pivoted variables\n",
                "if 'user_item_matrix' in locals():\n",
                "    R_bench = user_item_matrix\n",
                "elif 'R_df' in locals():\n",
                "    R_bench = R_df\n",
                "\n",
                "# Attempt 2: Check for raw ratings and pivot\n",
                "if R_bench is None:\n",
                "    if 'ratings_df' in locals():\n",
                "        print(\"Pivoting ratings_df for benchmark...\")\n",
                "        R_bench = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
                "    elif 'df' in locals() and 'rating' in df.columns:\n",
                "        print(\"Pivoting df for benchmark...\")\n",
                "        R_bench = df.pivot(index='userId', columns='movieId', values='rating')\n",
                "\n",
                "# Attempt 3: Load from Disk\n",
                "if R_bench is None and 'load_data' in locals():\n",
                "    print(\"Loading data from disk for benchmark...\")\n",
                "    _df = load_data()\n",
                "    if _df is not None:\n",
                "        R_bench = _df.pivot(index='userId', columns='movieId', values='rating')\n",
                "\n",
                "if R_bench is None:\n",
                "    print(\"Error: Could not obtain User-Item Matrix. Skipping Benchmark.\")\n",
                "else:\n",
                "    # Ensure float\n",
                "    R_bench = R_bench.astype(float)\n",
                "    \n",
                "    # 3. Decomposition & Prediction\n",
                "    t0 = time.time()\n",
                "    \n",
                "    if 'Mean Filling' in method_name:\n",
                "        # Mean Filling Logic\n",
                "        item_means_bench = R_bench.mean()\n",
                "        R_filled_bench = R_bench.fillna(item_means_bench)\n",
                "        R_centered_bench = R_filled_bench - item_means_bench\n",
                "        X = R_centered_bench.values\n",
                "        # Cov = X.T @ X / (N-1)\n",
                "        cov_matrix_bench = np.dot(X.T, X) / (X.shape[0] - 1)\n",
                "        \n",
                "    else:\n",
                "        # MLE Logic\n",
                "        if 'calculate_mle_covariance' in locals():\n",
                "             cov_matrix_bench, _, _ = calculate_mle_covariance(R_bench)\n",
                "             if isinstance(cov_matrix_bench, pd.DataFrame): cov_matrix_bench = cov_matrix_bench.values\n",
                "        else:\n",
                "             print(\"MLE Util not found. Skipping.\")\n",
                "             cov_matrix_bench = None\n",
                "\n",
                "    if cov_matrix_bench is not None:\n",
                "        # Eigh\n",
                "        evals_b, evecs_b = eigh(cov_matrix_bench)\n",
                "        idx_b = np.argsort(evals_b)[::-1]\n",
                "        evecs_b = evecs_b[:, idx_b]\n",
                "        \n",
                "        t1 = time.time()\n",
                "        time_decomp = t1 - t0\n",
                "        \n",
                "        # 4. Prediction Timing\n",
                "        t2 = time.time()\n",
                "        k_limit = 50\n",
                "        V_k = evecs_b[:, :k_limit]\n",
                "        \n",
                "        if 'Mean Filling' in method_name:\n",
                "             # U = X V\n",
                "             U_b = np.dot(X, V_k)\n",
                "             # X_hat = U V.T + Mean\n",
                "             X_hat_b = np.dot(U_b, V_k.T) + item_means_bench.values\n",
                "        else:\n",
                "             means_mle = R_bench.mean()\n",
                "             X_mle = R_bench.fillna(means_mle).values - means_mle.values\n",
                "             U_b = np.dot(X_mle, V_k)\n",
                "             X_hat_b = np.dot(U_b, V_k.T) + means_mle.values\n",
                "             \n",
                "        t3 = time.time()\n",
                "        time_pred = t3 - t2\n",
                "        \n",
                "        # 5. Metrics\n",
                "        mask_b = ~np.isnan(R_bench.values)\n",
                "        diff_b = R_bench.values[mask_b] - X_hat_b[mask_b]\n",
                "        rmse_val = np.sqrt(np.mean(diff_b**2))\n",
                "        mae_val = np.mean(np.abs(diff_b))\n",
                "        mem_mb = (cov_matrix_bench.nbytes + evecs_b.nbytes) / 1024 / 1024\n",
                "        \n",
                "        print(f\"Results: Time Decomp {time_decomp:.4f}s, RMSE {rmse_val:.4f}\")\n",
                "        \n",
                "        # 6. Save\n",
                "        bench_file = os.path.join(results_table_dir, 'pca_benchmarks.csv')\n",
                "        new_data = {\n",
                "            'Method': method_name,\n",
                "            'k': k_limit,\n",
                "            'RMSE': rmse_val,\n",
                "            'MAE': mae_val,\n",
                "            'Time_Decomp(s)': time_decomp,\n",
                "            'Time_Pred(s)': time_pred,\n",
                "            'Memory(MB)': mem_mb\n",
                "        }\n",
                "        \n",
                "        if os.path.exists(bench_file):\n",
                "            df_bench = pd.read_csv(bench_file)\n",
                "            df_bench = df_bench[df_bench['Method'] != method_name]\n",
                "            df_bench = pd.concat([df_bench, pd.DataFrame([new_data])], ignore_index=True)\n",
                "        else:\n",
                "            df_bench = pd.DataFrame([new_data])\n",
                "            \n",
                "        df_bench.to_csv(bench_file, index=False)\n",
                "        print(f\"Benchmark saved to {bench_file}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
