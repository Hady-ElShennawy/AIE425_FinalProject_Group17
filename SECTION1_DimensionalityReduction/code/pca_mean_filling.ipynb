{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092af0a7",
   "metadata": {},
   "source": [
    "# Group 17: \n",
    "# Eyad Medhat 221100279 / Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b1be7",
   "metadata": {},
   "source": [
    "# PCA with Mean Filling\n",
    "Eyad Medhat 221100279/ Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed872307",
   "metadata": {},
   "source": [
    "Task 1: Calculate Average Rating for Target Items I1 and I2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9293850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3a2254df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found requested table at: ..\\results\\tables\\lowest_two_rateditems.csv\n",
      "Target Items Loaded:\n",
      "   movieId  mean_rating_per_movie  rating_count_per_movie\n",
      "0     1556               1.919431                     422\n",
      "1     1499               2.059603                     453\n",
      "Target Item 1 (I1): 1556.0\n",
      "Target Item 2 (I2): 1499.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Target Items (I1, I2) from the previous results\n",
    "target_items_df = load_data(table_name='lowest_two_rateditems.csv')\n",
    "\n",
    "if target_items_df is not None:\n",
    "    print(\"Target Items Loaded:\")\n",
    "    print(target_items_df)\n",
    "    \n",
    "    # Access by position to ensure we get the first two rows regardless of index\n",
    "    I1 = target_items_df.iloc[0]['movieId']\n",
    "    I2 = target_items_df.iloc[1]['movieId']\n",
    "    print(f\"Target Item 1 (I1): {I1}\")\n",
    "    print(f\"Target Item 2 (I2): {I2}\")\n",
    "else:\n",
    "    print(\"ERROR: Target items file not found. Make sure previous steps were run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fbb8145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found cached sample at: ..\\data\\ml-20m\\ratings_cleaned_sampled.csv\n",
      "Ratings loaded. Shape: (1000000, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the Ratings Data\n",
    "ratings_df = load_data()\n",
    "\n",
    "if ratings_df is not None:\n",
    "    print(f\"Ratings loaded. Shape: {ratings_df.shape}\")\n",
    "else:\n",
    "    print(\"FAILED to load ratings data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "63b77b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculated Stats ---\n",
      "Item I1 (ID: 1556.0): Mean Rating = 1.9194, Count = 422\n",
      "Item I2 (ID: 1499.0): Mean Rating = 2.0596, Count = 453\n",
      "\n",
      "--- Verification ---\n",
      "Stored I1 Mean: 1.919431279620853\n",
      "Stored I2 Mean: 2.0596026490066226\n",
      "    Saved CSV: tables/task3.2.1.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Calculate Average Rating for I1 and I2\n",
    "if ratings_df is not None and 'I1' in locals():\n",
    "    # Filter for I1\n",
    "    i1_ratings = ratings_df[ratings_df['movieId'] == I1]\n",
    "    i1_mean = i1_ratings['rating'].mean()\n",
    "    i1_count = len(i1_ratings)\n",
    "    \n",
    "    # Filter for I2\n",
    "    i2_ratings = ratings_df[ratings_df['movieId'] == I2]\n",
    "    i2_mean = i2_ratings['rating'].mean()\n",
    "    i2_count = len(i2_ratings)\n",
    "    \n",
    "    print(f\"\\n--- Calculated Stats ---\")\n",
    "    print(f\"Item I1 (ID: {I1}): Mean Rating = {i1_mean:.4f}, Count = {i1_count}\")\n",
    "    print(f\"Item I2 (ID: {I2}): Mean Rating = {i2_mean:.4f}, Count = {i2_count}\")\n",
    "    \n",
    "    # Verification against loaded values\n",
    "    print(f\"\\n--- Verification ---\")\n",
    "    print(f\"Stored I1 Mean: {target_items_df.iloc[0]['mean_rating_per_movie']}\")\n",
    "    print(f\"Stored I2 Mean: {target_items_df.iloc[1]['mean_rating_per_movie']}\")\n",
    "    \n",
    "    # --- SAVE RESULT ---\n",
    "    task1_data = [\n",
    "        {'movieId': I1, 'mean_rating': i1_mean, 'count': i1_count},\n",
    "        {'movieId': I2, 'mean_rating': i2_mean, 'count': i2_count}\n",
    "    ]\n",
    "    task1_df = pd.DataFrame(task1_data)\n",
    "    save_csv(task1_df, 'task3.2.1.csv')\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot calculate stats: Missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a900d7",
   "metadata": {},
   "source": [
    "## Task 2: Mean Filling and Dataset Augmentation\n",
    "Fill missing ratings for Target Items I1 and I2 with their mean value (1.0) and save the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "15438210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting dataset with filled ratings for I1 and I2...\n",
      "Total unique users: 96345\n",
      "Users missing rating for I1: 95923\n",
      "Users missing rating for I2: 95892\n",
      "Creating dataframe for 191815 new ratings...\n",
      "Appending to original dataset...\n",
      "Original Shape: (1000000, 3)\n",
      "Augmented Shape: (1191815, 3)\n",
      "Final Count I1: 96345 (Should be 96345) \n",
      "Final Count I2: 96345 (Should be 96345) \n",
      "    Saved CSV: tables/ratings_filled_targets.csv\n",
      "Save Complete.\n"
     ]
    }
   ],
   "source": [
    "augmented_df = None # Initialize\n",
    "if ratings_df is not None and 'I1' in locals():\n",
    "    print(\"Augmenting dataset with filled ratings for I1 and I2...\")\n",
    "    \n",
    "    all_users = ratings_df['userId'].unique()\n",
    "    print(f\"Total unique users: {len(all_users)}\")\n",
    "    \n",
    "    new_rows = []\n",
    "    \n",
    "    # --- Process I1 ---\n",
    "    # Find users who rated I1\n",
    "    users_rated_i1 = set(ratings_df[ratings_df['movieId'] == I1]['userId'].unique())\n",
    "    # Find missing\n",
    "    users_missing_i1 = [u for u in all_users if u not in users_rated_i1]\n",
    "    print(f\"Users missing rating for I1: {len(users_missing_i1)}\")\n",
    "    \n",
    "    # Create rows for missing I1\n",
    "    for u in users_missing_i1:\n",
    "        new_rows.append({'userId': u, 'movieId': I1, 'rating': i1_mean})\n",
    "        \n",
    "    # --- Process I2 ---\n",
    "    # Find users who rated I2\n",
    "    users_rated_i2 = set(ratings_df[ratings_df['movieId'] == I2]['userId'].unique())\n",
    "    # Find missing\n",
    "    users_missing_i2 = [u for u in all_users if u not in users_rated_i2]\n",
    "    print(f\"Users missing rating for I2: {len(users_missing_i2)}\")\n",
    "    \n",
    "    # Create rows for missing I2\n",
    "    for u in users_missing_i2:\n",
    "        new_rows.append({'userId': u, 'movieId': I2, 'rating': i2_mean})\n",
    "    \n",
    "    print(f\"Creating dataframe for {len(new_rows)} new ratings...\")\n",
    "    new_ratings_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    # Concatenate\n",
    "    print(\"Appending to original dataset...\")\n",
    "    augmented_df = pd.concat([ratings_df, new_ratings_df], ignore_index=True)\n",
    "    \n",
    "    # Sort for tidiness (optional)\n",
    "    augmented_df.sort_values(by=['userId', 'movieId'], inplace=True)\n",
    "    \n",
    "    print(f\"Original Shape: {ratings_df.shape}\")\n",
    "    print(f\"Augmented Shape: {augmented_df.shape}\")\n",
    "    \n",
    "    # Verify counts\n",
    "    final_i1_count = len(augmented_df[augmented_df['movieId'] == I1])\n",
    "    final_i2_count = len(augmented_df[augmented_df['movieId'] == I2])\n",
    "    print(f\"Final Count I1: {final_i1_count} (Should be {len(all_users)}) \")\n",
    "    print(f\"Final Count I2: {final_i2_count} (Should be {len(all_users)}) \")\n",
    "    \n",
    "    # --- SAVE RESULT ---\n",
    "    save_csv(augmented_df, 'ratings_filled_targets.csv')\n",
    "    print(\"Save Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping augmentation due to missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09100790",
   "metadata": {},
   "source": [
    "## Task 3: Calculate Average Ratings from Augmented Data\n",
    "Calculate the mean rating for each item in the new augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dcb5df75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stats for augmented dataset...\n",
      "Stats calculated.\n",
      "   movieId  mean_rating  rating_count\n",
      "0      1.0     3.915355          3875\n",
      "1      2.0     3.202874          1740\n",
      "2      3.0     3.209073          1014\n",
      "3      5.0     3.061538           975\n",
      "4      6.0     3.770856          1846\n",
      "\n",
      "Final Stats I1:\n",
      "     movieId  mean_rating  rating_count\n",
      "429   1556.0     1.919431         96345\n",
      "Final Stats I2:\n",
      "     movieId  mean_rating  rating_count\n",
      "422   1499.0     2.059603         96345\n",
      "Saving final item stats...\n",
      "    Saved CSV: tables/task3.2.3_final_item_stats.csv\n",
      "Task 3 Complete.\n"
     ]
    }
   ],
   "source": [
    "final_stats = None # Initialize\n",
    "if augmented_df is not None:\n",
    "    print(\"Calculating stats for augmented dataset...\")\n",
    "    \n",
    "    # Calculate stats\n",
    "    final_stats = augmented_df.groupby('movieId')['rating'].agg(['mean', 'count']).reset_index()\n",
    "    final_stats.rename(columns={'mean': 'mean_rating', 'count': 'rating_count'}, inplace=True)\n",
    "    \n",
    "    print(\"Stats calculated.\")\n",
    "    print(final_stats.head())\n",
    "    \n",
    "    # Verification for I1 and I2\n",
    "    if 'I1' in locals():\n",
    "        i1_final = final_stats[final_stats['movieId'] == I1]\n",
    "        print(f\"\\nFinal Stats I1:\\n{i1_final}\")\n",
    "        \n",
    "        i2_final = final_stats[final_stats['movieId'] == I2]\n",
    "        print(f\"Final Stats I2:\\n{i2_final}\")\n",
    "        \n",
    "    # --- SAVE RESULT ---\n",
    "    print(\"Saving final item stats...\")\n",
    "    save_csv(final_stats, 'task3.2.3_final_item_stats.csv')\n",
    "    print(\"Task 3 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Augmented dataframe not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21db64",
   "metadata": {},
   "source": [
    "## Task 4: Calculate Diffrence between Ratings and Mean Rating\n",
    "Load the augmented ratings and final item stats from CSVs, then calculate the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ee299842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files for Task 4...\n",
      " Found requested table at: ..\\results\\tables\\ratings_filled_targets.csv\n",
      " Found requested table at: ..\\results\\tables\\task3.2.3_final_item_stats.csv\n",
      "Loaded Augmented Ratings. Shape: (1191815, 3)\n",
      "Loaded Item Stats. Shape: (1000, 3)\n",
      "Calculating rating differences (Centering)...\n",
      "Difference calculated.\n",
      "   userId  movieId  rating  mean_rating  rating_diff\n",
      "0       1     32.0     4.0     3.881264     0.118736\n",
      "1       1    337.0     4.0     3.755682     0.244318\n",
      "2       1   1193.0     4.0     4.191716    -0.191716\n",
      "3       1   1261.0     4.0     3.848649     0.151351\n",
      "4       1   1370.0     3.0     3.454844    -0.454844\n",
      "\n",
      "Average Global Difference (should be ~0): -0.000000\n",
      "Saving centered ratings...\n",
      "    Saved CSV: tables/task3.2.4_centered_ratings.csv\n",
      "Task 4 Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading files for Task 4...\")\n",
    "\n",
    "task4_ratings = load_data(table_name='ratings_filled_targets.csv')\n",
    "task4_stats = load_data(table_name='task3.2.3_final_item_stats.csv')\n",
    "\n",
    "if task4_ratings is not None and task4_stats is not None:\n",
    "    print(f\"Loaded Augmented Ratings. Shape: {task4_ratings.shape}\")\n",
    "    print(f\"Loaded Item Stats. Shape: {task4_stats.shape}\")\n",
    "    \n",
    "    print(\"Calculating rating differences (Centering)...\")\n",
    "    \n",
    "    # Merge\n",
    "    merged_df = task4_ratings.merge(task4_stats[['movieId', 'mean_rating']], on='movieId', how='left')\n",
    "    \n",
    "    # Calculate difference\n",
    "    merged_df['rating_diff'] = merged_df['rating'] - merged_df['mean_rating']\n",
    "    \n",
    "    print(\"Difference calculated.\")\n",
    "    print(merged_df[['userId', 'movieId', 'rating', 'mean_rating', 'rating_diff']].head())\n",
    "    \n",
    "    # Verify centering (Mean difference should be close to 0)\n",
    "    avg_diff = merged_df['rating_diff'].mean()\n",
    "    print(f\"\\nAverage Global Difference (should be ~0): {avg_diff:.6f}\")\n",
    "    \n",
    "    # --- SAVE RESULT ---\n",
    "    print(\"Saving centered ratings...\")\n",
    "    save_csv(merged_df, 'task3.2.4_centered_ratings.csv')\n",
    "    print(\"Task 4 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot proceed with Task 4 due to missing files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8011c87",
   "metadata": {},
   "source": [
    "## Task 5: Calculate Partial Covariance (Targets vs All)\n",
    "Compute the covariance where we calculate Cov(Target, Item_j) for all Items j.\n",
    "We use the helper function from utils to do this efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "120c063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found requested table at: ..\\results\\tables\\lowest_two_rateditems.csv\n",
      " Target Items IDs found: 1556.0, 1499.0\n",
      " Found requested table at: ..\\results\\tables\\task3.2.4_centered_ratings.csv\n",
      "Calculating partial covariance matrix (2 x All Items)...\n",
      "Starting MANUAL covariance calculation...\n",
      "Total N (Users): 96345\n",
      "Total Items: 1000\n",
      "Users who rated targets: 96345\n",
      "Building efficient lookup dictionary...\n",
      "Lookup built. Calculating sums...\n",
      "Processing Target Item: 1556.0...\n",
      "Processing Target Item: 1499.0...\n",
      "Formatting results...\n",
      "Calculation Complete.\n",
      "Result Shape: (2, 1000)\n",
      "Sample (first 5 cols):\n",
      "             1.0      2.0       3.0       5.0       6.0\n",
      "1556.0  0.000013  0.00007 -0.000016  0.000042  0.000027\n",
      "1499.0  0.000058  0.00010  0.000024  0.000046  0.000014\n",
      "    Saved CSV: tables/3.2.5_target_only_covariances.csv\n",
      "Task 5 Complete.\n"
     ]
    }
   ],
   "source": [
    "target_items_df = load_data(table_name='lowest_two_rateditems.csv')\n",
    "\n",
    "if target_items_df is not None:\n",
    "    I1 = target_items_df.iloc[0]['movieId']\n",
    "    I2 = target_items_df.iloc[1]['movieId']\n",
    "    print(f\" Target Items IDs found: {I1}, {I2}\")\n",
    "\n",
    "    # 2. Load Centered Ratings\n",
    "    # Note: load_data loads full CSV. \n",
    "    df = load_data(table_name='task3.2.4_centered_ratings.csv')\n",
    "    \n",
    "    if df is not None:\n",
    "        # 3. Calculate Target Covariances using Utils\n",
    "        print(\"Calculating partial covariance matrix (2 x All Items)...\")\n",
    "        \n",
    "        targets = [I1, I2]\n",
    "        partial_cov_df = calculate_target_covariance(df, targets)\n",
    "        \n",
    "        print(\"Calculation Complete.\")\n",
    "        print(f\"Result Shape: {partial_cov_df.shape}\")\n",
    "        print(\"Sample (first 5 cols):\")\n",
    "        print(partial_cov_df.iloc[:, :5])\n",
    "        \n",
    "        # Task 5 Save\n",
    "        save_csv(partial_cov_df, '3.2.5_target_only_covariances.csv')\n",
    "        print(\"Task 5 Complete.\")\n",
    "    else:\n",
    "        print(\"Centered ratings data missing.\")\n",
    "else:\n",
    "    print(\"Target items data missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557451f",
   "metadata": {},
   "source": [
    "## Task 6: Reference Full Covariance Matrix Calculation\n",
    "Calculates the FULL N x N covariance matrix for all items using sparse algebra and saves it as a compressed .npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d2e03664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading centered ratings for Full Matrix calculation...\n",
      " Found requested table at: ..\\results\\tables\\task3.2.4_centered_ratings.csv\n",
      "Calculating Full Sparse Matrix (N x N)...\n",
      "Preparing for Full Sparse Covariance Calculation...\n",
      "Dimensions: 96345 Users x 1000 Items\n",
      "Constructing User-Item Sparse Matrix...\n",
      "Computing X.T @ X ... (This may take a moment)\n",
      "Dividing by N-1...\n",
      "Matrix Shape: (1000, 1000)\n",
      "Saving Sparse Matrix to ../results\\tables\\3.2.6_full_covariance.npz...\n",
      "    Saved CSV: tables/3.2.6_full_covariance_ids.csv\n",
      "Writing PARTIAL (60%) Full Matrix to CSV: ../results\\tables\\3.2.6_full_covariance_partial.csv\n",
      "Writing in chunks...\n",
      "Total items: 1000. Writing first 600 items.\n",
      "  Written rows 0 to 600 / 600 (0.7s)\n",
      "CSV Write Complete. Time: 0.7s\n",
      "Task 6 Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading centered ratings for Full Matrix calculation...\")\n",
    "df_full = load_data(table_name='task3.2.4_centered_ratings.csv')\n",
    "\n",
    "if df_full is not None:\n",
    "    print(\"Calculating Full Sparse Matrix (N x N)...\")\n",
    "    sparse_cov, movie_ids = calculate_full_covariance_sparse(df_full)\n",
    "    \n",
    "    print(f\"Matrix Shape: {sparse_cov.shape}\")\n",
    "    \n",
    "    # Save Matrix (Sparse) - using global results_root from utils for robust path\n",
    "    out_path = os.path.join(results_root, 'tables', '3.2.6_full_covariance.npz')\n",
    "    print(f\"Saving Sparse Matrix to {out_path}...\")\n",
    "    sparse.save_npz(out_path, sparse_cov)\n",
    "    \n",
    "    # Save ID Mapping\n",
    "    pd.DataFrame(movie_ids, columns=['movieId'])\n",
    "    save_csv(pd.DataFrame(movie_ids, columns=['movieId']), '3.2.6_full_covariance_ids.csv')\n",
    "    \n",
    "    # --- MASSIVE CSV WRITE WITH CHUNKING (PARTIAL 60%) ---\n",
    "    # Since save_csv simplifies overwriting, we handle chunking manually but use path from utils if possible\n",
    "    # or just use os.path.join with results_root\n",
    "    full_csv_path = os.path.join(results_root, 'tables', '3.2.6_full_covariance_partial.csv')\n",
    "    print(f\"Writing PARTIAL (60%) Full Matrix to CSV: {full_csv_path}\")\n",
    "    print(\"Writing in chunks...\")\n",
    "    \n",
    "    chunk_size = 1000\n",
    "    num_items = sparse_cov.shape[0]\n",
    "    limit_items = int(num_items * 0.60) # 60% Limit\n",
    "    \n",
    "    print(f\"Total items: {num_items}. Writing first {limit_items} items.\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, limit_items, chunk_size):\n",
    "        end_i = min(i + chunk_size, limit_items)\n",
    "        \n",
    "        # Extract chunk and make dense\n",
    "        chunk_dense = sparse_cov[i:end_i].toarray()\n",
    "        \n",
    "        # Create DF\n",
    "        chunk_index = movie_ids[i:end_i]\n",
    "        chunk_df = pd.DataFrame(chunk_dense, index=chunk_index, columns=movie_ids)\n",
    "        \n",
    "        # Write mode: 'w' for first chunk, 'a' for others\n",
    "        if i == 0:\n",
    "            chunk_df.to_csv(full_csv_path, mode='w', header=True)\n",
    "        else:\n",
    "            chunk_df.to_csv(full_csv_path, mode='a', header=False)\n",
    "            \n",
    "        # Progress log\n",
    "        if i % 3000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  Written rows {i} to {end_i} / {limit_items} ({elapsed:.1f}s)\")\n",
    "    \n",
    "    print(f\"CSV Write Complete. Time: {time.time() - start_time:.1f}s\")\n",
    "    print(\"Task 6 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Input file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d16fd",
   "metadata": {},
   "source": [
    "## Task 7: Determine Top 5 and Top 10 Peers\n",
    "Identify the items with the highest covariance with each target item (I1 and I2) using the generated full matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e8f43929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Full Sparse Matrix...\n",
      "Loading IDs...\n",
      " Found requested table at: ..\\results\\tables\\3.2.6_full_covariance_ids.csv\n",
      "Loading Targets...\n",
      " Found requested table at: ..\\results\\tables\\lowest_two_rateditems.csv\n",
      "\n",
      "Processing Peers for Target: 1556.0\n",
      "Top 5 Peers for 1556.0:\n",
      "1370.0    0.000210\n",
      "1580.0    0.000203\n",
      "1722.0    0.000160\n",
      "1591.0    0.000151\n",
      "780.0     0.000136\n",
      "dtype: float64\n",
      "\n",
      "Top 10 Peers for 1556.0:\n",
      "1370.0    0.000210\n",
      "1580.0    0.000203\n",
      "1722.0    0.000160\n",
      "1591.0    0.000151\n",
      "780.0     0.000136\n",
      "1831.0    0.000133\n",
      "1917.0    0.000132\n",
      "595.0     0.000129\n",
      "2407.0    0.000127\n",
      "173.0     0.000126\n",
      "dtype: float64\n",
      "\n",
      "Processing Peers for Target: 1499.0\n",
      "Top 5 Peers for 1499.0:\n",
      "231.0     0.000195\n",
      "2722.0    0.000194\n",
      "1438.0    0.000169\n",
      "379.0     0.000161\n",
      "780.0     0.000154\n",
      "dtype: float64\n",
      "\n",
      "Top 10 Peers for 1499.0:\n",
      "231.0     0.000195\n",
      "2722.0    0.000194\n",
      "1438.0    0.000169\n",
      "379.0     0.000161\n",
      "780.0     0.000154\n",
      "377.0     0.000154\n",
      "1552.0    0.000151\n",
      "434.0     0.000144\n",
      "3623.0    0.000140\n",
      "1573.0    0.000139\n",
      "dtype: float64\n",
      "    Saved CSV: tables/3.2.7_top_peers.csv\n",
      "Task 7 Complete.\n"
     ]
    }
   ],
   "source": [
    "matrix_path = os.path.join(results_root, 'tables', '3.2.6_full_covariance.npz')\n",
    "\n",
    "if os.path.exists(matrix_path):\n",
    "    print(\"Loading Full Sparse Matrix...\")\n",
    "    cov_matrix_sparse = sparse.load_npz(matrix_path)\n",
    "    \n",
    "    print(\"Loading IDs...\")\n",
    "    movie_ids_df = load_data(table_name='3.2.6_full_covariance_ids.csv')\n",
    "    all_movie_ids = movie_ids_df['movieId'].tolist()\n",
    "    # Create index map for fast lookup\n",
    "    id_to_idx = {mid: i for i, mid in enumerate(all_movie_ids)}\n",
    "    \n",
    "    print(\"Loading Targets...\")\n",
    "    targets_df = load_data(table_name='lowest_two_rateditems.csv')\n",
    "    targets = [targets_df.iloc[0]['movieId'], targets_df.iloc[1]['movieId']]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for tid in targets:\n",
    "        print(f\"\\nProcessing Peers for Target: {tid}\")\n",
    "        if tid in id_to_idx:\n",
    "            row_idx = id_to_idx[tid]\n",
    "            \n",
    "            # Extract dense row from sparse matrix\n",
    "            # Get row slice -> to dense array -> flatten to 1D\n",
    "            cov_row = cov_matrix_sparse.getrow(row_idx).toarray().flatten()\n",
    "            \n",
    "            # Create Series for sorting\n",
    "            row_series = pd.Series(cov_row, index=all_movie_ids)\n",
    "            \n",
    "            # Exclude self\n",
    "            if tid in row_series.index:\n",
    "                row_series = row_series.drop(tid)\n",
    "                \n",
    "            # Sort & Top 10\n",
    "            top_10 = row_series.sort_values(ascending=False).head(10)\n",
    "            \n",
    "            print(f\"Top 5 Peers for {tid}:\")\n",
    "            print(top_10.head(5))\n",
    "\n",
    "            print(f\"\\nTop 10 Peers for {tid}:\")\n",
    "            print(top_10)\n",
    "            \n",
    "            # Store results\n",
    "            rank = 1\n",
    "            for peer_id, cv in top_10.items():\n",
    "                results.append({\n",
    "                    'Target_Item': tid,\n",
    "                    'Rank': rank,\n",
    "                    'Peer_Item': peer_id,\n",
    "                    'Covariance': cv\n",
    "                })\n",
    "                rank += 1\n",
    "        else:\n",
    "            print(f\"Target {tid} not found in matrix indices.\")\n",
    "            \n",
    "    # Save Results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    save_csv(results_df, '3.2.7_top_peers.csv')\n",
    "    print(\"Task 7 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Missing required files for Task 7. Run Task 6 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39adba8a",
   "metadata": {},
   "source": [
    "## Task 8: Determine Reduced Dimensional Space (Top 5 Peers)\n",
    "Create a reduced dimensional space for each user by selecting their centered ratings for the Top 5 peer items identified in Task 7.\n",
    "Missing values are filled with 0 (since ratings are centered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "edb63cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Top Peers and Centered Ratings...\n",
      " Found requested table at: ..\\results\\tables\\3.2.7_top_peers.csv\n",
      " Found requested table at: ..\\results\\tables\\task3.2.4_centered_ratings.csv\n",
      "Total Users: 96345\n",
      "\n",
      "--- Processing Target 1556.0 ---\n",
      "Top 5 Peers: [1370.0, 1580.0, 1722.0, 1591.0, 780.0]\n",
      "Creating Reduced Space Matrix (Pivoting)...\n",
      "Reduced Matrix Shape: (96345, 5)\n",
      "movieId  780.0     1370.0  1580.0  1591.0  1722.0\n",
      "userId                                           \n",
      "1           0.0 -0.454844     0.0     0.0     0.0\n",
      "2           0.0  0.000000     0.0     0.0     0.0\n",
      "3           0.0  0.000000     0.0     0.0     0.0\n",
      "4           0.0  0.000000     0.0     0.0     0.0\n",
      "5           0.0  0.000000     0.0     0.0     0.0\n",
      "Saving to 3.2.8_reduced_space_target_1556.csv...\n",
      "    Saved CSV: tables/3.2.8_reduced_space_target_1556.csv\n",
      "\n",
      "--- Processing Target 1499.0 ---\n",
      "Top 5 Peers: [231.0, 2722.0, 1438.0, 379.0, 780.0]\n",
      "Creating Reduced Space Matrix (Pivoting)...\n",
      "Reduced Matrix Shape: (96345, 5)\n",
      "movieId  231.0   379.0   780.0   1438.0  2722.0\n",
      "userId                                         \n",
      "1           0.0     0.0     0.0     0.0     0.0\n",
      "2           0.0     0.0     0.0     0.0     0.0\n",
      "3           0.0     0.0     0.0     0.0     0.0\n",
      "4           0.0     0.0     0.0     0.0     0.0\n",
      "5           0.0     0.0     0.0     0.0     0.0\n",
      "Saving to 3.2.8_reduced_space_target_1499.csv...\n",
      "    Saved CSV: tables/3.2.8_reduced_space_target_1499.csv\n",
      "\n",
      "Task 8 Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Top Peers and Centered Ratings...\")\n",
    "peers_df = load_data(table_name='3.2.7_top_peers.csv')\n",
    "centered_df = load_data(table_name='task3.2.4_centered_ratings.csv')\n",
    "\n",
    "if peers_df is not None and centered_df is not None:\n",
    "    \n",
    "    # Get all unique users to ensure complete index\n",
    "    all_users = centered_df['userId'].unique()\n",
    "    print(f\"Total Users: {len(all_users)}\")\n",
    "    \n",
    "    targets = peers_df['Target_Item'].unique()\n",
    "    \n",
    "    for target in targets:\n",
    "        print(f\"\\n--- Processing Target {target} ---\")\n",
    "        \n",
    "        # 1. Identify Top 5 Peers\n",
    "        # Sort just in case, though file should be sorted\n",
    "        target_peers = peers_df[peers_df['Target_Item'] == target].sort_values('Rank')\n",
    "        top_5_peers = target_peers.head(5)['Peer_Item'].tolist()\n",
    "        print(f\"Top 5 Peers: {top_5_peers}\")\n",
    "        \n",
    "        # 2. Filter Centered Ratings for these items\n",
    "        subset_df = centered_df[centered_df['movieId'].isin(top_5_peers)]\n",
    "        \n",
    "        # 3. Pivot to create User x Peer Matrix\n",
    "        print(\"Creating Reduced Space Matrix (Pivoting)...\")\n",
    "        reduced_matrix = subset_df.pivot(index='userId', columns='movieId', values='rating_diff')\n",
    "        \n",
    "        # 4. Handle Missing Values (Fill with 0 -> Mean)\n",
    "        reduced_matrix = reduced_matrix.fillna(0.0)\n",
    "        \n",
    "        # 5. Ensure all users are represented (Reindex)\n",
    "        # This adds rows for users who rated NONE of the top 5 peers, filling with 0\n",
    "        reduced_matrix = reduced_matrix.reindex(all_users, fill_value=0.0)\n",
    "        \n",
    "        print(f\"Reduced Matrix Shape: {reduced_matrix.shape}\")\n",
    "        print(reduced_matrix.head())\n",
    "        \n",
    "        # 6. Save\n",
    "        out_filename = f'3.2.8_reduced_space_target_{int(target)}.csv'\n",
    "        print(f\"Saving to {out_filename}...\")\n",
    "        # Reset index to include userId in the CSV\n",
    "        save_csv(reduced_matrix.reset_index(), out_filename)\n",
    "        \n",
    "    print(\"\\nTask 8 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: Required files (Top Peers or Centered Ratings) not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb784d5",
   "metadata": {},
   "source": [
    "## Task 9: Rating Prediction using Linear Regression (Top 5 Peers)\n",
    "Use the reduced dimensional space (Top 5 Peers) to train a Linear Regression model for each target item.\n",
    "Predict the centered ratings for users who haven't rated the target item, then add the mean rating to get the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5546c106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stats, centered ratings, and original ratings... (Task 9)\n",
      " Found requested table at: ..\\results\\tables\\task3.2.3_final_item_stats.csv\n",
      " Found requested table at: ..\\results\\tables\\task3.2.4_centered_ratings.csv\n",
      " Found cached sample at: ..\\data\\ml-20m\\ratings_cleaned_sampled.csv\n",
      "\n",
      "--- Modeling for Target Item: 1499 ---\n",
      "Original Raters: 453\n",
      " Found requested table at: ..\\results\\tables\\3.2.8_reduced_space_target_1499.csv\n",
      "Training set size: 453\n",
      "Prediction set size: 95892\n",
      "Model Coefficients: [0.3871221  0.54495451 0.26219894 0.35995851 0.59578871]\n",
      "Model Intercept: -0.005853477668545759\n",
      "Saving predictions to 3.2.9_predictions_target_1499.csv...\n",
      "    Saved CSV: tables/3.2.9_predictions_target_1499.csv\n",
      "\n",
      "--- Modeling for Target Item: 1556 ---\n",
      "Original Raters: 422\n",
      " Found requested table at: ..\\results\\tables\\3.2.8_reduced_space_target_1556.csv\n",
      "Training set size: 422\n",
      "Prediction set size: 95923\n",
      "Model Coefficients: [0.3142321  0.55395172 0.46708939 0.500502   0.36481411]\n",
      "Model Intercept: 0.019076932927554708\n",
      "Saving predictions to 3.2.9_predictions_target_1556.csv...\n",
      "    Saved CSV: tables/3.2.9_predictions_target_1556.csv\n",
      "\n",
      "Task 9 Complete.\n"
     ]
    }
   ],
   "source": [
    "# Task 9: Prediction\n",
    "print(\"Loading stats, centered ratings, and original ratings... (Task 9)\")\n",
    "\n",
    "item_stats = load_data(table_name='task3.2.3_final_item_stats.csv')\n",
    "centered_df = load_data(table_name='task3.2.4_centered_ratings.csv')\n",
    "# Load ORIGINAL ratings specifically using explicit filename for load_data\n",
    "# load_data will search for it\n",
    "original_ratings = load_data()\n",
    "\n",
    "if item_stats is not None and centered_df is not None and original_ratings is not None:\n",
    "    \n",
    "    results_dir = os.path.join(results_root, 'tables')\n",
    "    target_files = [f for f in os.listdir(results_dir) if f.startswith('3.2.8_reduced_space_target_')]\n",
    "    \n",
    "    for t_file in target_files:\n",
    "        try:\n",
    "            target_id = int(t_file.split('_')[-1].replace('.csv', ''))\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- Modeling for Target Item: {target_id} ---\")\n",
    "        \n",
    "        # 1. Identify ORIGINAL Raters (Train Set)\n",
    "        real_raters = set(original_ratings[original_ratings['movieId'] == target_id]['userId'].unique())\n",
    "        print(f\"Original Raters: {len(real_raters)}\")\n",
    "        \n",
    "        # 2. Load Reduced Space (Features X)\n",
    "        reduced_df = load_data(table_name=t_file)\n",
    "        \n",
    "        # 3. Prepare Target Variable (y)\n",
    "        target_centered = centered_df[centered_df['movieId'] == target_id][['userId', 'rating_diff']]\n",
    "        \n",
    "        # 4. Merge\n",
    "        data = reduced_df.merge(target_centered, on='userId', how='left')\n",
    "        \n",
    "        feature_cols = [c for c in reduced_df.columns if c != 'userId']\n",
    "        \n",
    "        # 5. Split using Real Raters\n",
    "        train_data = data[data['userId'].isin(real_raters)]\n",
    "        predict_data = data[~data['userId'].isin(real_raters)]\n",
    "        \n",
    "        # Filter NaNs in Train if any\n",
    "        train_data = train_data[train_data['rating_diff'].notna()]\n",
    "        \n",
    "        print(f\"Training set size: {len(train_data)}\")\n",
    "        print(f\"Prediction set size: {len(predict_data)}\")\n",
    "        \n",
    "        if len(train_data) > 0:\n",
    "            # Train\n",
    "            X_train = train_data[feature_cols]\n",
    "            y_train = train_data['rating_diff']\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            print(f\"Model Coefficients: {model.coef_}\")\n",
    "            print(f\"Model Intercept: {model.intercept_}\")\n",
    "            \n",
    "            # Predict\n",
    "            if len(predict_data) > 0:\n",
    "                X_predict = predict_data[feature_cols]\n",
    "                y_pred_centered = model.predict(X_predict)\n",
    "                \n",
    "                target_mean = item_stats[item_stats['movieId'] == target_id]['mean_rating'].values[0]\n",
    "                y_pred_final = y_pred_centered + target_mean\n",
    "                \n",
    "                predictions_df = pd.DataFrame({\n",
    "                    'userId': predict_data['userId'],\n",
    "                    'movieId': target_id,\n",
    "                    'predicted_rating_centered': y_pred_centered,\n",
    "                    'predicted_rating_final': y_pred_final\n",
    "                })\n",
    "                \n",
    "                out_name = f'3.2.9_predictions_target_{target_id}.csv'\n",
    "                print(f\"Saving predictions to {out_name}...\")\n",
    "                save_csv(predictions_df, out_name)\n",
    "            else:\n",
    "                print(\"No users to predict for.\")\n",
    "        else:\n",
    "            print(\"No training data available.\")\n",
    "            \n",
    "    print(\"\\nTask 9 Complete.\")\n",
    "else:\n",
    "    print(\"Required files for Task 9 missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eaa39a",
   "metadata": {},
   "source": [
    "## Task 10: Determine Reduced Dimensional Space (Top 10 Peers)\n",
    "Create a reduced dimensional space for each user by selecting their centered ratings for the Top 10 peer items identified in Task 7.\n",
    "Missing values are filled with 0 (since ratings are centered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "090fcbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Top Peers and Centered Ratings (Task 10)...\n",
      "Total Users: 96345\n",
      "\n",
      "--- Processing Target 1556.0 (Top 10) ---\n",
      "Top 10 Peers: [1370.0, 1580.0, 1722.0, 1591.0, 780.0, 1831.0, 1917.0, 595.0, 2407.0, 173.0]\n",
      "Creating Reduced Space Matrix...\n",
      "Reduced Matrix Shape: (96345, 10)\n",
      "Saving to 3.2.10_reduced_space_target_1556_top10.csv...\n",
      "    Saved CSV: tables/3.2.10_reduced_space_target_1556_top10.csv\n",
      "\n",
      "--- Processing Target 1499.0 (Top 10) ---\n",
      "Top 10 Peers: [231.0, 2722.0, 1438.0, 379.0, 780.0, 377.0, 1552.0, 434.0, 3623.0, 1573.0]\n",
      "Creating Reduced Space Matrix...\n",
      "Reduced Matrix Shape: (96345, 10)\n",
      "Saving to 3.2.10_reduced_space_target_1499_top10.csv...\n",
      "    Saved CSV: tables/3.2.10_reduced_space_target_1499_top10.csv\n",
      "\n",
      "Task 10 Complete.\n"
     ]
    }
   ],
   "source": [
    "peers_path = os.path.join('..', 'results', 'tables', '3.2.7_top_peers.csv')\n",
    "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
    "\n",
    "if os.path.exists(peers_path) and os.path.exists(centered_path):\n",
    "    print(\"Loading Top Peers and Centered Ratings (Task 10)...\")\n",
    "    peers_df = pd.read_csv(peers_path)\n",
    "    centered_df = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
    "    \n",
    "    all_users = centered_df['userId'].unique()\n",
    "    print(f\"Total Users: {len(all_users)}\")\n",
    "    \n",
    "    targets = peers_df['Target_Item'].unique()\n",
    "    \n",
    "    for target in targets:\n",
    "        print(f\"\\n--- Processing Target {target} (Top 10) ---\")\n",
    "        \n",
    "        # 1. Identify Top 10 Peers (Take first 10, assuming sorted by Rank)\n",
    "        target_peers = peers_df[peers_df['Target_Item'] == target].sort_values('Rank')\n",
    "        top_10_peers = target_peers.head(10)['Peer_Item'].tolist()\n",
    "        print(f\"Top 10 Peers: {top_10_peers}\")\n",
    "        \n",
    "        # 2. Filter Centered Ratings\n",
    "        subset_df = centered_df[centered_df['movieId'].isin(top_10_peers)]\n",
    "        \n",
    "        # 3. Pivot\n",
    "        print(\"Creating Reduced Space Matrix...\")\n",
    "        reduced_matrix = subset_df.pivot(index='userId', columns='movieId', values='rating_diff')\n",
    "        \n",
    "        # 4. Fill NaNs with 0\n",
    "        reduced_matrix = reduced_matrix.fillna(0.0)\n",
    "        \n",
    "        # 5. Reindex for all users\n",
    "        reduced_matrix = reduced_matrix.reindex(all_users, fill_value=0.0)\n",
    "        \n",
    "        print(f\"Reduced Matrix Shape: {reduced_matrix.shape}\")\n",
    "        \n",
    "        # 6. Save\n",
    "        out_filename = f'3.2.10_reduced_space_target_{int(target)}_top10.csv'\n",
    "        print(f\"Saving to {out_filename}...\")\n",
    "        save_csv(reduced_matrix.reset_index(), out_filename)\n",
    "        \n",
    "    print(\"\\nTask 10 Complete.\")\n",
    "else:\n",
    "    print(\"Required files missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0007a02",
   "metadata": {},
   "source": [
    "# Task 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "42e5b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files (Task 11)...\n",
      "\n",
      "--- Modeling for Target Item: 1499 (Top 10) ---\n",
      "Features count: 10\n",
      "Training set size: 453\n",
      "Prediction set size: 95892\n",
      "Saving predictions to 3.2.11_predictions_target_1499_top10.csv...\n",
      "    Saved CSV: tables/3.2.11_predictions_target_1499_top10.csv\n",
      "\n",
      "--- Modeling for Target Item: 1556 (Top 10) ---\n",
      "Features count: 10\n",
      "Training set size: 422\n",
      "Prediction set size: 95923\n",
      "Saving predictions to 3.2.11_predictions_target_1556_top10.csv...\n",
      "    Saved CSV: tables/3.2.11_predictions_target_1556_top10.csv\n",
      "\n",
      "Task 11 Complete.\n"
     ]
    }
   ],
   "source": [
    "stats_path = os.path.join('..', 'results', 'tables', 'task3.2.3_final_item_stats.csv')\n",
    "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
    "ratings_path = os.path.join('ml-20m', 'ratings_cleaned_sampled.csv') \n",
    "\n",
    "# Flexible path lookup in case working directory differs\n",
    "if not os.path.exists(ratings_path):\n",
    "    ratings_path = os.path.join('..', 'data', 'ml-20m', 'ratings_cleaned_sampled.csv')\n",
    "\n",
    "if os.path.exists(stats_path) and os.path.exists(centered_path) and os.path.exists(ratings_path):\n",
    "    print(\"Loading files (Task 11)...\")\n",
    "    item_stats = pd.read_csv(stats_path)\n",
    "    centered_df = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
    "    original_ratings = pd.read_csv(ratings_path, usecols=['userId', 'movieId'])\n",
    "    \n",
    "    results_dir = os.path.join('..', 'results', 'tables')\n",
    "    # Look for Top 10 reduced space files\n",
    "    target_files = [f for f in os.listdir(results_dir) if f.startswith('3.2.10_reduced_space_target_')]\n",
    "    \n",
    "    for t_file in target_files:\n",
    "        try:\n",
    "            # Extract target ID: 3.2.10_reduced_space_target_1556_top10.csv -> 1556\n",
    "            # We split by underscores. The format is 3.2.10_reduced_space_target_ID_top10\n",
    "            # Indices: 0=3.2.10, 1=reduced, 2=space, 3=target, 4=ID, 5=top10\n",
    "            parts = t_file.replace('.csv', '').split('_')\n",
    "            target_id = int(parts[4])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {t_file}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- Modeling for Target Item: {target_id} (Top 10) ---\")\n",
    "        \n",
    "        # 1. Identify ORIGINAL Raters (Train Set)\n",
    "        # We only train on users who actually provided a rating in the original dataset\n",
    "        real_raters = set(original_ratings[original_ratings['movieId'] == target_id]['userId'].unique())\n",
    "        \n",
    "        # 2. Load Reduced Space (Features X)\n",
    "        reduced_space_path = os.path.join(results_dir, t_file)\n",
    "        reduced_df = pd.read_csv(reduced_space_path)\n",
    "        \n",
    "        # 3. Prepare Target Variable (y) from Centered Ratings\n",
    "        target_centered = centered_df[centered_df['movieId'] == target_id][['userId', 'rating_diff']]\n",
    "        \n",
    "        # 4. Merge Features and Target\n",
    "        data = reduced_df.merge(target_centered, on='userId', how='left')\n",
    "        \n",
    "        feature_cols = [c for c in reduced_df.columns if c != 'userId']\n",
    "        print(f\"Features count: {len(feature_cols)}\")\n",
    "        \n",
    "        # 5. Split Data\n",
    "        # Train: Users who exist in the original ratings\n",
    "        train_data = data[data['userId'].isin(real_raters)]\n",
    "        # Predict: Users who do NOT exist in the original ratings\n",
    "        predict_data = data[~data['userId'].isin(real_raters)]\n",
    "        \n",
    "        # Ensure no NaNs in training target (just in case)\n",
    "        train_data = train_data[train_data['rating_diff'].notna()]\n",
    "        \n",
    "        print(f\"Training set size: {len(train_data)}\")\n",
    "        print(f\"Prediction set size: {len(predict_data)}\")\n",
    "        \n",
    "        if len(train_data) > 0:\n",
    "            # Train Model\n",
    "            X_train = train_data[feature_cols]\n",
    "            y_train = train_data['rating_diff']\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            if len(predict_data) > 0:\n",
    "                X_predict = predict_data[feature_cols]\n",
    "                y_pred_centered = model.predict(X_predict)\n",
    "                \n",
    "                # Add back the mean rating to get the final prediction\n",
    "                target_mean = item_stats[item_stats['movieId'] == target_id]['mean_rating'].values[0]\n",
    "                y_pred_final = y_pred_centered + target_mean\n",
    "                \n",
    "                predictions_df = pd.DataFrame({\n",
    "                    'userId': predict_data['userId'],\n",
    "                    'movieId': target_id,\n",
    "                    'predicted_rating_centered': y_pred_centered,\n",
    "                    'predicted_rating_final': y_pred_final\n",
    "                })\n",
    "                \n",
    "                out_name = f'3.2.11_predictions_target_{target_id}_top10.csv'\n",
    "                print(f\"Saving predictions to {out_name}...\")\n",
    "                save_csv(predictions_df, out_name)\n",
    "            else:\n",
    "                print(\"No users to predict for.\")\n",
    "        else:\n",
    "            print(\"No training data available.\")\n",
    "            \n",
    "    print(\"\\nTask 11 Complete.\")\n",
    "else:\n",
    "    print(\"Required files missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893883e7",
   "metadata": {},
   "source": [
    "# Task 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e09e627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison for Target 1499 ---\n",
      "Total Predictions Compared: 95892\n",
      "Mean Difference (Top10 - Top5): 0.0102\n",
      "Mean Absolute Difference: 0.0427\n",
      "Max Difference: 2.5238\n",
      "\n",
      "Sample Comparison (First 5 users):\n",
      " userId  pred_top5  pred_top10     diff  abs_diff\n",
      "      1   2.053749    2.063927 0.010178  0.010178\n",
      "      2   2.053749    2.063927 0.010178  0.010178\n",
      "      3   2.053749    2.063927 0.010178  0.010178\n",
      "      4   2.053749    2.063927 0.010178  0.010178\n",
      "      5   2.053749    2.063927 0.010178  0.010178\n",
      "\n",
      "Conclusion: The predictions are very similar. Adding 5 more peers didn't drastically change the outcome.\n",
      "\n",
      "--- Comparison for Target 1556 ---\n",
      "Total Predictions Compared: 95923\n",
      "Mean Difference (Top10 - Top5): 0.0008\n",
      "Mean Absolute Difference: 0.0224\n",
      "Max Difference: 1.8028\n",
      "\n",
      "Sample Comparison (First 5 users):\n",
      " userId  pred_top5  pred_top10      diff  abs_diff\n",
      "      1   1.686547    1.723270  0.036724  0.036724\n",
      "      2   1.938508    1.939347  0.000839  0.000839\n",
      "      3   1.938508    1.765728 -0.172780  0.172780\n",
      "      4   1.938508    1.939347  0.000839  0.000839\n",
      "      5   1.938508    1.939347  0.000839  0.000839\n",
      "\n",
      "Conclusion: The predictions are very similar. Adding 5 more peers didn't drastically change the outcome.\n"
     ]
    }
   ],
   "source": [
    "# Task 12: Compare Results\n",
    "results_dir = os.path.join('..', 'results', 'tables')\n",
    "targets = [1499, 1556]\n",
    "\n",
    "for target_id in targets:\n",
    "    file_top5 = os.path.join(results_dir, f'3.2.9_predictions_target_{target_id}.csv')\n",
    "    file_top10 = os.path.join(results_dir, f'3.2.11_predictions_target_{target_id}_top10.csv')\n",
    "    \n",
    "    if os.path.exists(file_top5) and os.path.exists(file_top10):\n",
    "        print(f\"\\n--- Comparison for Target {target_id} ---\")\n",
    "        \n",
    "        df5 = pd.read_csv(file_top5)\n",
    "        df10 = pd.read_csv(file_top10)\n",
    "        \n",
    "        # Rename columns for clear merging\n",
    "        df5 = df5[['userId', 'predicted_rating_final']].rename(columns={'predicted_rating_final': 'pred_top5'})\n",
    "        df10 = df10[['userId', 'predicted_rating_final']].rename(columns={'predicted_rating_final': 'pred_top10'})\n",
    "        \n",
    "        # Merge on userId to align predictions\n",
    "        merged = df5.merge(df10, on='userId', how='inner')\n",
    "        \n",
    "        # Calculate Difference\n",
    "        merged['diff'] = merged['pred_top10'] - merged['pred_top5']\n",
    "        merged['abs_diff'] = merged['diff'].abs()\n",
    "        \n",
    "        # Statistics\n",
    "        print(f\"Total Predictions Compared: {len(merged)}\")\n",
    "        print(f\"Mean Difference (Top10 - Top5): {merged['diff'].mean():.4f}\")\n",
    "        print(f\"Mean Absolute Difference: {merged['abs_diff'].mean():.4f}\")\n",
    "        print(f\"Max Difference: {merged['abs_diff'].max():.4f}\")\n",
    "        \n",
    "        print(\"\\nSample Comparison (First 5 users):\")\n",
    "        print(merged.head(5).to_string(index=False))\n",
    "        \n",
    "        # Explanation logic (dynamic print)\n",
    "        if merged['abs_diff'].mean() < 0.1:\n",
    "            print(\"\\nConclusion: The predictions are very similar. Adding 5 more peers didn't drastically change the outcome.\")\n",
    "        else:\n",
    "            print(\"\\nConclusion: There is a noticeable difference. The additional 5 peers influenced the regression model.\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Missing files for target {target_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "63869512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmarking PCA Mean Filling (Part 1) ---\n",
      "Pivoting ratings_df for benchmark...\n",
      "Results: Time Decomp 5.3796s, RMSE 0.8281\n",
      "Benchmark saved to c:\\Users\\moham\\Desktop\\IRS GIT\\SECTION1_DimensionalityReduction\\code\\../results/tables\\pca_benchmarks.csv\n"
     ]
    }
   ],
   "source": [
    "# --- DYNAMIC BENCHMARKING: PCA Mean Filling (Part 1) ---\\n\n",
    "method_name = \"PCA Mean Filling (Part 1)\"\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.linalg import eigh\n",
    "# Ensure utils is loaded if locals missing (fallback)\n",
    "if 'load_data' not in locals():\n",
    "    try:\n",
    "        from utils import load_data, calculate_mle_covariance\n",
    "    except ImportError:\n",
    "        print(\"Warning: utils module not found. Benchmarking might fail.\")\n",
    "\n",
    "print(f\"\\n--- Benchmarking {method_name} ---\")\n",
    "\n",
    "# 1. Setup Results Path\n",
    "results_table_dir = os.path.join(os.getcwd(), '../results/tables')\n",
    "if not os.path.exists(results_table_dir):\n",
    "   os.makedirs(results_table_dir)\n",
    "\n",
    "# 2. Data Preparation (Robust Loading)\n",
    "R_bench = None\n",
    "\n",
    "# Attempt 1: Check for existing pivoted variables\n",
    "if 'user_item_matrix' in locals():\n",
    "    R_bench = user_item_matrix\n",
    "elif 'R_df' in locals():\n",
    "    R_bench = R_df\n",
    "\n",
    "# Attempt 2: Check for raw ratings and pivot\n",
    "if R_bench is None:\n",
    "    if 'ratings_df' in locals():\n",
    "        print(\"Pivoting ratings_df for benchmark...\")\n",
    "        R_bench = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
    "    elif 'df' in locals() and 'rating' in df.columns:\n",
    "        print(\"Pivoting df for benchmark...\")\n",
    "        R_bench = df.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# Attempt 3: Load from Disk\n",
    "if R_bench is None and 'load_data' in locals():\n",
    "    print(\"Loading data from disk for benchmark...\")\n",
    "    _df = load_data()\n",
    "    if _df is not None:\n",
    "        R_bench = _df.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "if R_bench is None:\n",
    "    print(\"Error: Could not obtain User-Item Matrix. Skipping Benchmark.\")\n",
    "else:\n",
    "    # Ensure float\n",
    "    R_bench = R_bench.astype(float)\n",
    "    \n",
    "    # 3. Decomposition & Prediction\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if 'Mean Filling' in method_name:\n",
    "        # Mean Filling Logic\n",
    "        item_means_bench = R_bench.mean()\n",
    "        R_filled_bench = R_bench.fillna(item_means_bench)\n",
    "        R_centered_bench = R_filled_bench - item_means_bench\n",
    "        X = R_centered_bench.values\n",
    "        # Cov = X.T @ X / (N-1)\n",
    "        cov_matrix_bench = np.dot(X.T, X) / (X.shape[0] - 1)\n",
    "        \n",
    "    else:\n",
    "        # MLE Logic\n",
    "        if 'calculate_mle_covariance' in locals():\n",
    "             cov_matrix_bench, _, _ = calculate_mle_covariance(R_bench)\n",
    "             if isinstance(cov_matrix_bench, pd.DataFrame): cov_matrix_bench = cov_matrix_bench.values\n",
    "        else:\n",
    "             print(\"MLE Util not found. Skipping.\")\n",
    "             cov_matrix_bench = None\n",
    "\n",
    "    if cov_matrix_bench is not None:\n",
    "        # Eigh\n",
    "        evals_b, evecs_b = eigh(cov_matrix_bench)\n",
    "        idx_b = np.argsort(evals_b)[::-1]\n",
    "        evecs_b = evecs_b[:, idx_b]\n",
    "        \n",
    "        t1 = time.time()\n",
    "        time_decomp = t1 - t0\n",
    "        \n",
    "        # 4. Prediction Timing\n",
    "        t2 = time.time()\n",
    "        k_limit = 50\n",
    "        V_k = evecs_b[:, :k_limit]\n",
    "        \n",
    "        if 'Mean Filling' in method_name:\n",
    "             # U = X V\n",
    "             U_b = np.dot(X, V_k)\n",
    "             # X_hat = U V.T + Mean\n",
    "             X_hat_b = np.dot(U_b, V_k.T) + item_means_bench.values\n",
    "        else:\n",
    "             means_mle = R_bench.mean()\n",
    "             X_mle = R_bench.fillna(means_mle).values - means_mle.values\n",
    "             U_b = np.dot(X_mle, V_k)\n",
    "             X_hat_b = np.dot(U_b, V_k.T) + means_mle.values\n",
    "             \n",
    "        t3 = time.time()\n",
    "        time_pred = t3 - t2\n",
    "        \n",
    "        # 5. Metrics\n",
    "        mask_b = ~np.isnan(R_bench.values)\n",
    "        diff_b = R_bench.values[mask_b] - X_hat_b[mask_b]\n",
    "        rmse_val = np.sqrt(np.mean(diff_b**2))\n",
    "        mae_val = np.mean(np.abs(diff_b))\n",
    "        mem_mb = (cov_matrix_bench.nbytes + evecs_b.nbytes) / 1024 / 1024\n",
    "        \n",
    "        print(f\"Results: Time Decomp {time_decomp:.4f}s, RMSE {rmse_val:.4f}\")\n",
    "        \n",
    "        # 6. Save\n",
    "        bench_file = os.path.join(results_table_dir, 'pca_benchmarks.csv')\n",
    "        new_data = {\n",
    "            'Method': method_name,\n",
    "            'k': k_limit,\n",
    "            'RMSE': rmse_val,\n",
    "            'MAE': mae_val,\n",
    "            'Time_Decomp(s)': time_decomp,\n",
    "            'Time_Pred(s)': time_pred,\n",
    "            'Memory(MB)': mem_mb\n",
    "        }\n",
    "        \n",
    "        if os.path.exists(bench_file):\n",
    "            df_bench = pd.read_csv(bench_file)\n",
    "            df_bench = df_bench[df_bench['Method'] != method_name]\n",
    "            df_bench = pd.concat([df_bench, pd.DataFrame([new_data])], ignore_index=True)\n",
    "        else:\n",
    "            df_bench = pd.DataFrame([new_data])\n",
    "            \n",
    "        df_bench.to_csv(bench_file, index=False)\n",
    "        print(f\"Benchmark saved to {bench_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
