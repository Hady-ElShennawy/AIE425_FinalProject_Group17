{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2b1be7",
   "metadata": {},
   "source": [
    "# PCA with Mean Filling\n",
    "Eyad Medhat 221100279/ Hady Aly 221101190 / Mohamed Mahfouz 221101743 / Omar Mady 221100745"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed872307",
   "metadata": {},
   "source": [
    "Task 1: Calculate Average Rating for Target Items I1 and I2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9293850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2254df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading target items from: ..\\results\\tables\\lowest_two_rateditems.csv\n",
      "Target Items Loaded:\n",
      "   movieId  mean_rating_per_movie  rating_count_per_movie\n",
      "0     1556               1.919431                     422\n",
      "1     1499               2.059603                     453\n",
      "Target Item 1 (I1): 1556.0\n",
      "Target Item 2 (I2): 1499.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Target Items (I1, I2) from the previous results\n",
    "target_items_path = os.path.join('..', 'results', 'tables', 'lowest_two_rateditems.csv')\n",
    "print(f\"Loading target items from: {target_items_path}\")\n",
    "\n",
    "if os.path.exists(target_items_path):\n",
    "    target_items_df = pd.read_csv(target_items_path)\n",
    "    print(\"Target Items Loaded:\")\n",
    "    print(target_items_df)\n",
    "    \n",
    "    # Access by position to ensure we get the first two rows regardless of index\n",
    "    I1 = target_items_df.iloc[0]['movieId']\n",
    "    I2 = target_items_df.iloc[1]['movieId']\n",
    "    print(f\"Target Item 1 (I1): {I1}\")\n",
    "    print(f\"Target Item 2 (I2): {I2}\")\n",
    "else:\n",
    "    print(\"ERROR: Target items file not found. Make sure previous steps were run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb8145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found cached sample at: ..\\data\\ml-20m\\ratings_cleaned_sampled.csv\n",
      "Ratings loaded. Shape: (1000000, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the Ratings Data\n",
    "ratings_df = load_data()\n",
    "\n",
    "if ratings_df is None:\n",
    "    print(\"Trying raw ratings...\")\n",
    "    ratings_df = utils.load_data(os.path.join('ml-20m', 'ratings.csv'))\n",
    "\n",
    "if ratings_df is not None:\n",
    "    print(f\"Ratings loaded. Shape: {ratings_df.shape}\")\n",
    "else:\n",
    "    print(\"FAILED to load ratings data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b77b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculated Stats ---\n",
      "Item I1 (ID: 1556.0): Mean Rating = 1.9194, Count = 422\n",
      "Item I2 (ID: 1499.0): Mean Rating = 2.0596, Count = 453\n",
      "\n",
      "--- Verification ---\n",
      "Stored I1 Mean: 1.919431279620853\n",
      "Stored I2 Mean: 2.0596026490066226\n",
      "    Saved CSV: tables/task3.2.1.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Calculate Average Rating for I1 and I2\n",
    "if ratings_df is not None and 'I1' in locals():\n",
    "    # Filter for I1\n",
    "    i1_ratings = ratings_df[ratings_df['movieId'] == I1]\n",
    "    i1_mean = i1_ratings['rating'].mean()\n",
    "    i1_count = len(i1_ratings)\n",
    "    \n",
    "    # Filter for I2\n",
    "    i2_ratings = ratings_df[ratings_df['movieId'] == I2]\n",
    "    i2_mean = i2_ratings['rating'].mean()\n",
    "    i2_count = len(i2_ratings)\n",
    "    \n",
    "    print(f\"\\n--- Calculated Stats ---\")\n",
    "    print(f\"Item I1 (ID: {I1}): Mean Rating = {i1_mean:.4f}, Count = {i1_count}\")\n",
    "    print(f\"Item I2 (ID: {I2}): Mean Rating = {i2_mean:.4f}, Count = {i2_count}\")\n",
    "    \n",
    "    # Verification against loaded values\n",
    "    print(f\"\\n--- Verification ---\")\n",
    "    print(f\"Stored I1 Mean: {target_items_df.iloc[0]['mean_rating_per_movie']}\")\n",
    "    print(f\"Stored I2 Mean: {target_items_df.iloc[1]['mean_rating_per_movie']}\")\n",
    "    \n",
    "    # --- SAVE RESULT ---\n",
    "    task1_data = [\n",
    "        {'movieId': I1, 'mean_rating': i1_mean, 'count': i1_count},\n",
    "        {'movieId': I2, 'mean_rating': i2_mean, 'count': i2_count}\n",
    "    ]\n",
    "    task1_df = pd.DataFrame(task1_data)\n",
    "    save_csv(task1_df, 'task3.2.1.csv')\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot calculate stats: Missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a900d7",
   "metadata": {},
   "source": [
    "## Task 2: Mean Filling and Dataset Augmentation\n",
    "Fill missing ratings for Target Items I1 and I2 with their mean value (1.0) and save the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15438210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting dataset with filled ratings for I1 and I2...\n",
      "Total unique users: 96345\n",
      "Users missing rating for I1: 95923\n",
      "Users missing rating for I2: 95892\n",
      "Creating dataframe for 191815 new ratings...\n",
      "Appending to original dataset...\n",
      "Original Shape: (1000000, 3)\n",
      "Augmented Shape: (1191815, 3)\n",
      "Final Count I1: 96345 (Should be 96345) \n",
      "Final Count I2: 96345 (Should be 96345) \n",
      "Saving augmented dataset to ..\\results\\tables\\ratings_filled_targets.csv...\n",
      "Save Complete.\n"
     ]
    }
   ],
   "source": [
    "augmented_df = None # Initialize\n",
    "if ratings_df is not None and 'I1' in locals():\n",
    "    print(\"Augmenting dataset with filled ratings for I1 and I2...\")\n",
    "    \n",
    "    all_users = ratings_df['userId'].unique()\n",
    "    print(f\"Total unique users: {len(all_users)}\")\n",
    "    \n",
    "    new_rows = []\n",
    "    \n",
    "    # --- Process I1 ---\n",
    "    # Find users who rated I1\n",
    "    users_rated_i1 = set(ratings_df[ratings_df['movieId'] == I1]['userId'].unique())\n",
    "    # Find missing\n",
    "    users_missing_i1 = [u for u in all_users if u not in users_rated_i1]\n",
    "    print(f\"Users missing rating for I1: {len(users_missing_i1)}\")\n",
    "    \n",
    "    # Create rows for missing I1\n",
    "    for u in users_missing_i1:\n",
    "        new_rows.append({'userId': u, 'movieId': I1, 'rating': i1_mean})\n",
    "        \n",
    "    # --- Process I2 ---\n",
    "    # Find users who rated I2\n",
    "    users_rated_i2 = set(ratings_df[ratings_df['movieId'] == I2]['userId'].unique())\n",
    "    # Find missing\n",
    "    users_missing_i2 = [u for u in all_users if u not in users_rated_i2]\n",
    "    print(f\"Users missing rating for I2: {len(users_missing_i2)}\")\n",
    "    \n",
    "    # Create rows for missing I2\n",
    "    for u in users_missing_i2:\n",
    "        new_rows.append({'userId': u, 'movieId': I2, 'rating': i2_mean})\n",
    "    \n",
    "    print(f\"Creating dataframe for {len(new_rows)} new ratings...\")\n",
    "    new_ratings_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    # Concatenate\n",
    "    print(\"Appending to original dataset...\")\n",
    "    augmented_df = pd.concat([ratings_df, new_ratings_df], ignore_index=True)\n",
    "    \n",
    "    # Sort for tidiness (optional)\n",
    "    augmented_df.sort_values(by=['userId', 'movieId'], inplace=True)\n",
    "    \n",
    "    print(f\"Original Shape: {ratings_df.shape}\")\n",
    "    print(f\"Augmented Shape: {augmented_df.shape}\")\n",
    "    \n",
    "    # Verify counts\n",
    "    final_i1_count = len(augmented_df[augmented_df['movieId'] == I1])\n",
    "    final_i2_count = len(augmented_df[augmented_df['movieId'] == I2])\n",
    "    print(f\"Final Count I1: {final_i1_count} (Should be {len(all_users)}) \")\n",
    "    print(f\"Final Count I2: {final_i2_count} (Should be {len(all_users)}) \")\n",
    "    \n",
    "    # --- SAVE RESULT ---\n",
    "    # Saving to Results folder as requested: 'ratings_filled_targets.csv'\n",
    "    output_filename = 'ratings_filled_targets.csv'\n",
    "    output_path = os.path.join('..', 'results', 'tables', output_filename)\n",
    "    \n",
    "    # Ensure dir exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving augmented dataset to {output_path}...\")\n",
    "    augmented_df.to_csv(output_path, index=False)\n",
    "    print(\"Save Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping augmentation due to missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09100790",
   "metadata": {},
   "source": [
    "## Task 3: Calculate Average Ratings from Augmented Data\n",
    "Calculate the mean rating for each item in the new augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb5df75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stats for augmented dataset...\n",
      "Stats calculated.\n",
      "   movieId  mean_rating  rating_count\n",
      "0      1.0     3.915355          3875\n",
      "1      2.0     3.202874          1740\n",
      "2      3.0     3.209073          1014\n",
      "3      5.0     3.061538           975\n",
      "4      6.0     3.770856          1846\n",
      "\n",
      "Final Stats I1:\n",
      "     movieId  mean_rating  rating_count\n",
      "429   1556.0     1.919431         96345\n",
      "Final Stats I2:\n",
      "     movieId  mean_rating  rating_count\n",
      "422   1499.0     2.059603         96345\n",
      "Saving final item stats...\n",
      "    Saved CSV: tables/task3.2.3_final_item_stats.csv\n",
      "Task 3 Complete.\n"
     ]
    }
   ],
   "source": [
    "final_stats = None # Initialize\n",
    "if augmented_df is not None:\n",
    "    print(\"Calculating stats for augmented dataset...\")\n",
    "    \n",
    "    # Calculate stats\n",
    "    final_stats = augmented_df.groupby('movieId')['rating'].agg(['mean', 'count']).reset_index()\n",
    "    final_stats.rename(columns={'mean': 'mean_rating', 'count': 'rating_count'}, inplace=True)\n",
    "    \n",
    "    print(\"Stats calculated.\")\n",
    "    print(final_stats.head())\n",
    "    \n",
    "    # Verification for I1 and I2\n",
    "    if 'I1' in locals():\n",
    "        i1_final = final_stats[final_stats['movieId'] == I1]\n",
    "        print(f\"\\nFinal Stats I1:\\n{i1_final}\")\n",
    "        \n",
    "        i2_final = final_stats[final_stats['movieId'] == I2]\n",
    "        print(f\"Final Stats I2:\\n{i2_final}\")\n",
    "        \n",
    "    # --- SAVE RESULT ---\n",
    "    print(\"Saving final item stats...\")\n",
    "    save_csv(final_stats, 'task3.2.3_final_item_stats.csv')\n",
    "    print(\"Task 3 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Augmented dataframe not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21db64",
   "metadata": {},
   "source": [
    "## Task 4: Calculate Diffrence between Ratings and Mean Rating\n",
    "Load the augmented ratings and final item stats from CSVs, then calculate the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee299842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files for Task 4...\n",
      "Loaded Augmented Ratings. Shape: (1191815, 3)\n",
      "Loaded Item Stats. Shape: (1000, 3)\n",
      "Calculating rating differences (Centering)...\n",
      "Difference calculated.\n",
      "   userId  movieId  rating  mean_rating  rating_diff\n",
      "0       1     32.0     4.0     3.881264     0.118736\n",
      "1       1    337.0     4.0     3.755682     0.244318\n",
      "2       1   1193.0     4.0     4.191716    -0.191716\n",
      "3       1   1261.0     4.0     3.848649     0.151351\n",
      "4       1   1370.0     3.0     3.454844    -0.454844\n",
      "\n",
      "Average Global Difference (should be ~0): -0.000000\n",
      "Saving centered ratings...\n",
      "    Saved CSV: tables/task3.2.4_centered_ratings.csv\n",
      "Task 4 Complete.\n"
     ]
    }
   ],
   "source": [
    "# Load data from results folder\n",
    "augmented_csv_path = os.path.join('..', 'results', 'tables', 'ratings_filled_targets.csv')\n",
    "stats_csv_path = os.path.join('..', 'results', 'tables', 'task3.2.3_final_item_stats.csv')\n",
    "\n",
    "print(\"Loading files for Task 4...\")\n",
    "\n",
    "has_data_task4 = True\n",
    "if os.path.exists(augmented_csv_path):\n",
    "    # Use float32 for ratings to save memory if dataset is large\n",
    "    # Reading only necessary cols if we were pipelining, but here we read all\n",
    "    task4_ratings = pd.read_csv(augmented_csv_path)\n",
    "    print(f\"Loaded Augmented Ratings. Shape: {task4_ratings.shape}\")\n",
    "else:\n",
    "    print(f\"Error: {augmented_csv_path} not found.\")\n",
    "    has_data_task4 = False\n",
    "    \n",
    "if os.path.exists(stats_csv_path):\n",
    "    task4_stats = pd.read_csv(stats_csv_path)\n",
    "    print(f\"Loaded Item Stats. Shape: {task4_stats.shape}\")\n",
    "else:\n",
    "    print(f\"Error: {stats_csv_path} not found.\")\n",
    "    has_data_task4 = False\n",
    "\n",
    "if has_data_task4:\n",
    "    print(\"Calculating rating differences (Centering)...\")\n",
    "    \n",
    "    # Merge\n",
    "    # We want merged_df to contain 'mean_rating' from task4_stats matching 'movieId'\n",
    "    merged_df = task4_ratings.merge(task4_stats[['movieId', 'mean_rating']], on='movieId', how='left')\n",
    "    \n",
    "    # Calculate difference\n",
    "    merged_df['rating_diff'] = merged_df['rating'] - merged_df['mean_rating']\n",
    "    \n",
    "    print(\"Difference calculated.\")\n",
    "    print(merged_df[['userId', 'movieId', 'rating', 'mean_rating', 'rating_diff']].head())\n",
    "    \n",
    "    # Verify centering (Mean difference should be close to 0)\n",
    "    avg_diff = merged_df['rating_diff'].mean()\n",
    "    print(f\"\\nAverage Global Difference (should be ~0): {avg_diff:.6f}\")\n",
    "    \n",
    "    # --- SAVE RESULT ---\n",
    "    print(\"Saving centered ratings...\")\n",
    "    save_csv(merged_df, 'task3.2.4_centered_ratings.csv')\n",
    "    print(\"Task 4 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot proceed with Task 4 due to missing files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8011c87",
   "metadata": {},
   "source": [
    "## Task 5: Calculate Partial Covariance (Targets vs All)\n",
    "Compute the covariance where we calculate Cov(Target, Item_j) for all Items j.\n",
    "We use the helper function from utils to do this efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120c063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Items IDs: 1556.0, 1499.0\n",
      "Loading dataset...\n",
      "Calculating partial covariance matrix (2 x All Items)...\n",
      "Starting MANUAL covariance calculation...\n",
      "Total N (Users): 96345\n",
      "Total Items: 1000\n",
      "Users who rated targets: 96345\n",
      "Building efficient lookup dictionary...\n",
      "Lookup built. Calculating sums...\n",
      "Processing Target Item: 1556.0...\n",
      "Processing Target Item: 1499.0...\n",
      "Formatting results...\n",
      "Calculation Complete.\n",
      "Result Shape: (2, 1000)\n",
      "Sample (first 5 cols):\n",
      "             1.0      2.0       3.0       5.0       6.0\n",
      "1556.0  0.000013  0.00007 -0.000016  0.000042  0.000027\n",
      "1499.0  0.000058  0.00010  0.000024  0.000046  0.000014\n",
      "Saving intermediate partial covariance to ..\\results\\tables\\3.2.5_target_only_covariances.csv...\n",
      "Task 5 Complete.\n"
     ]
    }
   ],
   "source": [
    "centered_ratings_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
    "target_items_path = os.path.join('..', 'results', 'tables', 'lowest_two_rateditems.csv')\n",
    "\n",
    "if os.path.exists(centered_ratings_path) and os.path.exists(target_items_path):\n",
    "    # 1. Load Target Items to identify I1 and I2\n",
    "    target_items_df = pd.read_csv(target_items_path)\n",
    "    I1 = target_items_df.iloc[0]['movieId']\n",
    "    I2 = target_items_df.iloc[1]['movieId']\n",
    "    print(f\"Target Items IDs: {I1}, {I2}\")\n",
    "\n",
    "    # 2. Load Centered Ratings\n",
    "    print(\"Loading dataset...\")\n",
    "    # Need all data to calculate correlation against everything\n",
    "    df = pd.read_csv(centered_ratings_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
    "    \n",
    "    # 3. Calculate Target Covariances using Utils\n",
    "    print(\"Calculating partial covariance matrix (2 x All Items)...\")\n",
    "    \n",
    "    targets = [I1, I2]\n",
    "    partial_cov_df = calculate_target_covariance(df, targets)\n",
    "    \n",
    "    print(\"Calculation Complete.\")\n",
    "    print(f\"Result Shape: {partial_cov_df.shape}\")\n",
    "    print(\"Sample (first 5 cols):\")\n",
    "    print(partial_cov_df.iloc[:, :5])\n",
    "    \n",
    "    # Task 5 & 6 effectively merge here because Task 6 was just \"Generate Matrix\"\n",
    "    # But to separate them as requested:\n",
    "    \n",
    "    # Task 5 Save: We can save this intermediate result\n",
    "    output_filename = '3.2.5_target_only_covariances.csv'\n",
    "    output_path = os.path.join('..', 'results', 'tables', output_filename)\n",
    "    print(f\"Saving intermediate partial covariance to {output_path}...\")\n",
    "    partial_cov_df.to_csv(output_path)\n",
    "    print(\"Task 5 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Required files not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557451f",
   "metadata": {},
   "source": [
    "## Task 6: Reference Full Covariance Matrix Calculation\n",
    "Calculates the FULL N x N covariance matrix for all items using sparse algebra and saves it as a compressed .npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e03664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading centered ratings for Full Matrix calculation...\n",
      "Calculating Full Sparse Matrix (N x N)...\n",
      "Preparing for Full Sparse Covariance Calculation...\n",
      "Dimensions: 96345 Users x 1000 Items\n",
      "Constructing User-Item Sparse Matrix...\n",
      "Computing X.T @ X ... (This may take a moment)\n",
      "Dividing by N-1...\n",
      "Matrix Shape: (1000, 1000)\n",
      "Saving Sparse Matrix to ..\\results\\tables\\3.2.6_full_covariance.npz...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m out_path = os.path.join(\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtables\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m3.2.6_full_covariance.npz\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaving Sparse Matrix to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43msparse\u001b[49m.save_npz(out_path, sparse_cov)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Save ID Mapping\u001b[39;00m\n\u001b[32m     18\u001b[39m id_map_path = os.path.join(\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtables\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m3.2.6_full_covariance_ids.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sparse' is not defined"
     ]
    }
   ],
   "source": [
    "centered_path = os.path.join('..', 'results', 'tables', 'task3.2.4_centered_ratings.csv')\n",
    "\n",
    "if os.path.exists(centered_path):\n",
    "    print(\"Loading centered ratings for Full Matrix calculation...\")\n",
    "    df_full = pd.read_csv(centered_path, usecols=['userId', 'movieId', 'rating_diff'])\n",
    "    \n",
    "    print(\"Calculating Full Sparse Matrix (N x N)...\")\n",
    "    sparse_cov, movie_ids = utils.calculate_full_covariance_sparse(df_full)\n",
    "    \n",
    "    print(f\"Matrix Shape: {sparse_cov.shape}\")\n",
    "    \n",
    "    # Save Matrix (Sparse)\n",
    "    out_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance.npz')\n",
    "    print(f\"Saving Sparse Matrix to {out_path}...\")\n",
    "    sparse.save_npz(out_path, sparse_cov)\n",
    "    \n",
    "    # Save ID Mapping\n",
    "    id_map_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance_ids.csv')\n",
    "    pd.DataFrame(movie_ids, columns=['movieId']).to_csv(id_map_path, index=False)\n",
    "    \n",
    "    # --- MASSIVE CSV WRITE WITH CHUNKING (PARTIAL 60%) ---\n",
    "    full_csv_path = os.path.join('..', 'results', 'tables', '3.2.6_full_covariance_partial.csv')\n",
    "    print(f\"Writing PARTIAL (60%) Full Matrix to CSV: {full_csv_path}\")\n",
    "    print(\"Writing in chunks...\")\n",
    "    \n",
    "    chunk_size = 1000\n",
    "    num_items = sparse_cov.shape[0]\n",
    "    limit_items = int(num_items * 0.60) # 60% Limit\n",
    "    \n",
    "    print(f\"Total items: {num_items}. Writing first {limit_items} items.\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, limit_items, chunk_size):\n",
    "        end_i = min(i + chunk_size, limit_items)\n",
    "        \n",
    "        # Extract chunk and make dense\n",
    "        chunk_dense = sparse_cov[i:end_i].toarray()\n",
    "        \n",
    "        # Create DF\n",
    "        # Index is subset of IDs, Columns is ALL IDs\n",
    "        chunk_index = movie_ids[i:end_i]\n",
    "        chunk_df = pd.DataFrame(chunk_dense, index=chunk_index, columns=movie_ids)\n",
    "        \n",
    "        # Write mode: 'w' for first chunk, 'a' for others\n",
    "        # Header: True for first chunk, False for others\n",
    "        if i == 0:\n",
    "            chunk_df.to_csv(full_csv_path, mode='w', header=True)\n",
    "        else:\n",
    "            chunk_df.to_csv(full_csv_path, mode='a', header=False)\n",
    "            \n",
    "        # Progress log\n",
    "        if i % 3000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  Written rows {i} to {end_i} / {limit_items} ({elapsed:.1f}s)\")\n",
    "    \n",
    "    print(f\"CSV Write Complete. Time: {time.time() - start_time:.1f}s\")\n",
    "    print(\"Task 6 Complete.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Input file not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
